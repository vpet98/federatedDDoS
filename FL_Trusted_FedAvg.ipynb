{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ej63Q9F5VDWy"
      },
      "source": [
        "Federated Learning using PyTorch and PySyft with Trusted FedAvg based on https://arxiv.org/pdf/2104.07853.pdf"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "dNwIEBajagKA"
      },
      "source": [
        "## Install PySyft and Torchvision"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "p9Z-tl_jWgeE"
      },
      "outputs": [],
      "source": [
        "!pip install syft==0.2.9"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "FQBYDiJbdQHG"
      },
      "outputs": [],
      "source": [
        "!pip install torchvision==0.2.0"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "AKNKzhn9YVE0"
      },
      "source": [
        "## FL on MNIST dataset"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "eVP2vWh8pzsS"
      },
      "outputs": [],
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "import torch.optim as optim\n",
        "from torchvision import datasets, transforms\n",
        "import logging\n",
        "\n",
        "from numpy.linalg import norm\n",
        "from statistics import median\n",
        "\n",
        "# import Pysyft to help us to simulate federated learning\n",
        "import syft as sy\n",
        "\n",
        "# hook PyTorch to PySyft, i.e. add extra functionalities to support Federated Learning and other private AI tools\n",
        "hook = sy.TorchHook(torch) \n",
        "\n",
        "# create clients\n",
        "clients = []\n",
        "clients.append(sy.VirtualWorker(hook, id=\"bob\"))\n",
        "clients.append(sy.VirtualWorker(hook, id=\"alice\"))\n",
        "clients.append(sy.VirtualWorker(hook, id=\"untrustful\"))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "fd5-982uY-6L"
      },
      "outputs": [],
      "source": [
        "# define the args\n",
        "args = {\n",
        "    'use_cuda' : True,\n",
        "    'batch_size' : 64,\n",
        "    'test_batch_size' : 1000,\n",
        "    'lr' : 0.01,\n",
        "    'log_interval' : 10,\n",
        "    # with federated learning convergence is faster and less epochs are needed\n",
        "    'epochs' : 7\n",
        "}\n",
        "\n",
        "# check to use GPU or not\n",
        "use_cuda = args['use_cuda'] and torch.cuda.is_available()\n",
        "device = torch.device(\"cuda\" if use_cuda else \"cpu\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "VShbMriNZC6I"
      },
      "outputs": [],
      "source": [
        "# create a simple CNN net\n",
        "class Net(nn.Module):\n",
        "    \n",
        "    def __init__(self):\n",
        "        super(Net, self).__init__()\n",
        "        \n",
        "        self.conv = nn.Sequential(\n",
        "            nn.Conv2d(in_channels = 1, out_channels = 32, kernel_size = 3, stride = 1),\n",
        "            nn.ReLU(),\n",
        "            nn.Conv2d(in_channels=32,out_channels = 64, kernel_size = 3, stride = 1),\n",
        "            nn.ReLU()\n",
        "        )\n",
        "        \n",
        "        self.fc = nn.Sequential(\n",
        "            nn.Linear(in_features=64*12*12, out_features=128),\n",
        "            nn.ReLU(),\n",
        "            nn.Linear(in_features=128, out_features=10),\n",
        "        )\n",
        "\n",
        "        self.dropout = nn.Dropout2d(0.25)\n",
        "    \n",
        "    def forward(self, x):\n",
        "        x = self.conv(x)\n",
        "        x = F.max_pool2d(x,2)\n",
        "        x = x.view(-1, 64*12*12)\n",
        "        x = self.fc(x)\n",
        "        x = F.log_softmax(x, dim=1)\n",
        "        return x"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "_McSsgBEZq1f"
      },
      "outputs": [],
      "source": [
        "# prepare and distribute the data across workers\n",
        "# normally there is no need to distribute data, since it is already at the clients\n",
        "# this is more of a simulation of federated learning\n",
        "federated_dataset = datasets.MNIST('../data', train=True, download=True,\n",
        "                   transform=transforms.Compose([\n",
        "                       transforms.ToTensor(),\n",
        "                       transforms.Normalize((0.1307,), (0.3081,))\n",
        "                   ])).federate(tuple(clients))\n",
        "\n",
        "federated_train_loader = sy.FederatedDataLoader(federated_dataset, batch_size=args['batch_size'], shuffle=True)\n",
        "\n",
        "# test data remains at the central entity\n",
        "test_loader = torch.utils.data.DataLoader(\n",
        "        datasets.MNIST('../data', train=False, transform=transforms.Compose([\n",
        "                           transforms.ToTensor(),\n",
        "                           transforms.Normalize((0.1307,), (0.3081,))\n",
        "                       ])),\n",
        "        batch_size=args['test_batch_size'], shuffle=True)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "bP_p7GrAlXgB"
      },
      "outputs": [],
      "source": [
        "# classic torch code for training except for the federated part\n",
        "def train_locally(args, models, device, train_loader, optimizers, epoch):\n",
        "    for c, m in models.items():\n",
        "        m.train()\n",
        "        # send models to workers\n",
        "        m.send(c)\n",
        "\n",
        "    # iterate over federated data\n",
        "    for batch_idx, (data, target) in enumerate(train_loader):\n",
        "        data, target = data.to(device), target.to(device)\n",
        "\n",
        "        optimizers[data.location].zero_grad()\n",
        "        output = models[data.location](data)\n",
        "        # loss is a ptr to the tensor loss at the remote location\n",
        "        loss = F.nll_loss(output, target)\n",
        "        # call backward() on the loss ptr, that will send the command to call\n",
        "        # backward on the actual loss tensor present on the remote machine\n",
        "        loss.backward()\n",
        "        optimizers[data.location].step()\n",
        "\n",
        "        if batch_idx % args['log_interval'] == 0:\n",
        "\n",
        "            # get back loss, that was created at remote worker\n",
        "            loss = loss.get()\n",
        "\n",
        "            print('Train Epoch: {} [{}/{} ({:.0f}%)]\\tLoss: {:.6f}\\tWorker: {}'.format(\n",
        "                    epoch, \n",
        "                    batch_idx * args['batch_size'], # no of images done\n",
        "                    len(train_loader) * args['batch_size'], # total images left\n",
        "                    100. * batch_idx / len(train_loader),\n",
        "                    loss,\n",
        "                    data.location.id\n",
        "                )\n",
        "            )\n",
        "    \n",
        "    # get back models for aggregation\n",
        "    for m in models.values():\n",
        "        m = m.get()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "cHZpA0PtaKtW"
      },
      "outputs": [],
      "source": [
        "# classic torch code for testing\n",
        "def test(model, device, test_loader):\n",
        "    model.eval()\n",
        "    test_loss = 0\n",
        "    correct = 0\n",
        "    with torch.no_grad():\n",
        "        for data, target in test_loader:\n",
        "            data, target = data.to(device), target.to(device)\n",
        "            output = model(data)\n",
        "\n",
        "            # add losses together\n",
        "            test_loss += F.nll_loss(output, target, reduction='sum').item() \n",
        "\n",
        "            # get the index of the max probability class\n",
        "            pred = output.argmax(dim=1, keepdim=True)  \n",
        "            correct += pred.eq(target.view_as(pred)).sum().item()\n",
        "\n",
        "    test_loss /= len(test_loader.dataset)\n",
        "\n",
        "    print('\\nTest set: Average loss: {:.4f}, Accuracy: {}/{} ({:.0f}%)\\n'.format(\n",
        "        test_loss, correct, len(test_loader.dataset),\n",
        "        100. * correct / len(test_loader.dataset)))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Wmz8eKE3bYrt"
      },
      "outputs": [],
      "source": [
        "def aggregate(central_model, models, weights, trust):\n",
        "    # firstly compute new weight values\n",
        "    with torch.no_grad():\n",
        "        dataXtrust = 0\n",
        "        for c in models:\n",
        "            weights['conv0_mean_weight'] += models[c].conv[0].weight.data.clone()*len(federated_dataset.__getitem__(c.id))*trust[c]\n",
        "            weights['conv0_mean_bias'] += models[c].conv[0].bias.data.clone()*len(federated_dataset.__getitem__(c.id))*trust[c]\n",
        "            weights['conv2_mean_weight'] += models[c].conv[2].weight.data.clone()*len(federated_dataset.__getitem__(c.id))*trust[c]\n",
        "            weights['conv2_mean_bias'] += models[c].conv[2].bias.data.clone()*len(federated_dataset.__getitem__(c.id))*trust[c]\n",
        "            weights['fc0_mean_weight'] += models[c].fc[0].weight.data.clone()*len(federated_dataset.__getitem__(c.id))*trust[c]\n",
        "            weights['fc0_mean_bias'] += models[c].fc[0].bias.data.clone()*len(federated_dataset.__getitem__(c.id))*trust[c]\n",
        "            weights['fc2_mean_weight'] += models[c].fc[2].weight.data.clone()*len(federated_dataset.__getitem__(c.id))*trust[c]\n",
        "            weights['fc2_mean_bias'] += models[c].fc[2].bias.data.clone()*len(federated_dataset.__getitem__(c.id))*trust[c]\n",
        "\n",
        "            dataXtrust += len(federated_dataset.__getitem__(c.id))*trust[c]\n",
        "\n",
        "        weights['conv0_mean_weight'] = weights['conv0_mean_weight']/dataXtrust\n",
        "        weights['conv0_mean_bias'] = weights['conv0_mean_bias']/dataXtrust\n",
        "        weights['conv2_mean_weight'] = weights['conv2_mean_weight']/dataXtrust\n",
        "        weights['conv2_mean_bias'] = weights['conv2_mean_bias']/dataXtrust\n",
        "        weights['fc0_mean_weight'] = weights['fc0_mean_weight']/dataXtrust\n",
        "        weights['fc0_mean_bias'] = weights['fc0_mean_bias']/dataXtrust\n",
        "        weights['fc2_mean_weight'] = weights['fc2_mean_weight']/dataXtrust\n",
        "        weights['fc2_mean_bias'] = weights['fc2_mean_bias']/dataXtrust\n",
        "\n",
        "        # then copy them to the local models\n",
        "        for m in models.values():\n",
        "            m.conv[0].weight.data = weights['conv0_mean_weight'].data.clone()\n",
        "            m.conv[0].bias.data = weights['conv0_mean_bias'].data.clone()\n",
        "            m.conv[2].weight.data = weights['conv2_mean_weight'].data.clone()\n",
        "            m.conv[2].bias.data = weights['conv2_mean_bias'].data.clone()\n",
        "            m.fc[0].weight.data = weights['fc0_mean_weight'].data.clone()\n",
        "            m.fc[0].bias.data = weights['fc0_mean_bias'].data.clone()\n",
        "            m.fc[2].weight.data = weights['fc2_mean_weight'].data.clone()\n",
        "            m.fc[2].bias.data = weights['fc2_mean_bias'].data.clone()\n",
        "\n",
        "        # and to the central model for the test set\n",
        "        central_model.conv[0].weight.data = weights['conv0_mean_weight'].data.clone()\n",
        "        central_model.conv[0].bias.data = weights['conv0_mean_bias'].data.clone()\n",
        "        central_model.conv[2].weight.data = weights['conv2_mean_weight'].data.clone()\n",
        "        central_model.conv[2].bias.data = weights['conv2_mean_bias'].data.clone()\n",
        "        central_model.fc[0].weight.data = weights['fc0_mean_weight'].data.clone()\n",
        "        central_model.fc[0].bias.data = weights['fc0_mean_bias'].data.clone()\n",
        "        central_model.fc[2].weight.data = weights['fc2_mean_weight'].data.clone()\n",
        "        central_model.fc[2].bias.data = weights['fc2_mean_bias'].data.clone()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "VKXEFqRzdSvj"
      },
      "outputs": [],
      "source": [
        "def computeTrust(models, trust, r, s):\n",
        "    # dev[i] shows how the weights of model of client i differ from the models of all other clients\n",
        "    dev = [0 for i in clients]\n",
        "    for n, i in enumerate(clients):\n",
        "        for j in clients:\n",
        "            dev[n] += norm(models[j].conv[0].weight.data.cpu()-models[i].conv[0].weight.data.cpu())**2\n",
        "            dev[n] += norm(models[j].conv[2].weight.data.cpu()-models[i].conv[2].weight.data.cpu())**2\n",
        "            dev[n] += norm(models[j].fc[0].weight.data.cpu()-models[i].fc[0].weight.data.cpu())**2\n",
        "            dev[n] += norm(models[j].fc[2].weight.data.cpu()-models[i].fc[2].weight.data.cpu())**2\n",
        "        dev[n] /= len(clients)\n",
        "\n",
        "    # I[i] = 1 if client i acts normally and 0 if malicious or malfunctions\n",
        "    I = [1 if d <= 1.3 * median(sorted(dev)) else 0 for d in dev]\n",
        "    #print(\"dev: \",dev) # testing\n",
        "    #print(\"median*1.3: \", 1.3*median(dev)) # testing\n",
        "    #print(\"I: \", I) # testing\n",
        "\n",
        "    for i in range(len(clients)):\n",
        "        r[i] = 0.8*r[i] + I[i]\n",
        "        s[i] = 3.0*s[i] + 1 - I[i]\n",
        "\n",
        "    for i, c in enumerate(clients):\n",
        "        trust[c] = (r[i]+1)/(r[i]+s[i]+2)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "B_jH4dE8FVs1",
        "outputId": "2beb9799-0d43-404b-f427-87551739dcc2"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Train Epoch: 1 [0/60032 (0%)]\tLoss: 2.313279\tWorker: bob\n",
            "Train Epoch: 1 [640/60032 (1%)]\tLoss: 2.202435\tWorker: bob\n",
            "Train Epoch: 1 [1280/60032 (2%)]\tLoss: 2.088405\tWorker: bob\n",
            "Train Epoch: 1 [1920/60032 (3%)]\tLoss: 1.774398\tWorker: bob\n",
            "Train Epoch: 1 [2560/60032 (4%)]\tLoss: 1.459744\tWorker: bob\n",
            "Train Epoch: 1 [3200/60032 (5%)]\tLoss: 1.063543\tWorker: bob\n",
            "Train Epoch: 1 [3840/60032 (6%)]\tLoss: 0.717759\tWorker: bob\n",
            "Train Epoch: 1 [4480/60032 (7%)]\tLoss: 0.649427\tWorker: bob\n",
            "Train Epoch: 1 [5120/60032 (9%)]\tLoss: 0.635050\tWorker: bob\n",
            "Train Epoch: 1 [5760/60032 (10%)]\tLoss: 0.571844\tWorker: bob\n",
            "Train Epoch: 1 [6400/60032 (11%)]\tLoss: 0.415664\tWorker: bob\n",
            "Train Epoch: 1 [7040/60032 (12%)]\tLoss: 0.454812\tWorker: bob\n",
            "Train Epoch: 1 [7680/60032 (13%)]\tLoss: 0.593601\tWorker: bob\n",
            "Train Epoch: 1 [8320/60032 (14%)]\tLoss: 0.470521\tWorker: bob\n",
            "Train Epoch: 1 [8960/60032 (15%)]\tLoss: 0.392465\tWorker: bob\n",
            "Train Epoch: 1 [9600/60032 (16%)]\tLoss: 0.397079\tWorker: bob\n",
            "Train Epoch: 1 [10240/60032 (17%)]\tLoss: 0.505615\tWorker: bob\n",
            "Train Epoch: 1 [10880/60032 (18%)]\tLoss: 0.288859\tWorker: bob\n",
            "Train Epoch: 1 [11520/60032 (19%)]\tLoss: 0.419233\tWorker: bob\n",
            "Train Epoch: 1 [12160/60032 (20%)]\tLoss: 0.374685\tWorker: bob\n",
            "Train Epoch: 1 [12800/60032 (21%)]\tLoss: 0.406983\tWorker: bob\n",
            "Train Epoch: 1 [13440/60032 (22%)]\tLoss: 0.322513\tWorker: bob\n",
            "Train Epoch: 1 [14080/60032 (23%)]\tLoss: 0.439811\tWorker: bob\n",
            "Train Epoch: 1 [14720/60032 (25%)]\tLoss: 0.330118\tWorker: bob\n",
            "Train Epoch: 1 [15360/60032 (26%)]\tLoss: 0.398437\tWorker: bob\n",
            "Train Epoch: 1 [16000/60032 (27%)]\tLoss: 0.284693\tWorker: bob\n",
            "Train Epoch: 1 [16640/60032 (28%)]\tLoss: 0.384313\tWorker: bob\n",
            "Train Epoch: 1 [17280/60032 (29%)]\tLoss: 0.316033\tWorker: bob\n",
            "Train Epoch: 1 [17920/60032 (30%)]\tLoss: 0.250673\tWorker: bob\n",
            "Train Epoch: 1 [18560/60032 (31%)]\tLoss: 0.423816\tWorker: bob\n",
            "Train Epoch: 1 [19200/60032 (32%)]\tLoss: 0.526092\tWorker: bob\n",
            "Train Epoch: 1 [19840/60032 (33%)]\tLoss: 0.375321\tWorker: bob\n",
            "Train Epoch: 1 [20480/60032 (34%)]\tLoss: 2.246072\tWorker: alice\n",
            "Train Epoch: 1 [21120/60032 (35%)]\tLoss: 2.144461\tWorker: alice\n",
            "Train Epoch: 1 [21760/60032 (36%)]\tLoss: 1.992191\tWorker: alice\n",
            "Train Epoch: 1 [22400/60032 (37%)]\tLoss: 1.757952\tWorker: alice\n",
            "Train Epoch: 1 [23040/60032 (38%)]\tLoss: 1.410917\tWorker: alice\n",
            "Train Epoch: 1 [23680/60032 (39%)]\tLoss: 0.948050\tWorker: alice\n",
            "Train Epoch: 1 [24320/60032 (41%)]\tLoss: 0.830535\tWorker: alice\n",
            "Train Epoch: 1 [24960/60032 (42%)]\tLoss: 0.764718\tWorker: alice\n",
            "Train Epoch: 1 [25600/60032 (43%)]\tLoss: 0.712616\tWorker: alice\n",
            "Train Epoch: 1 [26240/60032 (44%)]\tLoss: 0.806587\tWorker: alice\n",
            "Train Epoch: 1 [26880/60032 (45%)]\tLoss: 0.515561\tWorker: alice\n",
            "Train Epoch: 1 [27520/60032 (46%)]\tLoss: 0.361370\tWorker: alice\n",
            "Train Epoch: 1 [28160/60032 (47%)]\tLoss: 0.375845\tWorker: alice\n",
            "Train Epoch: 1 [28800/60032 (48%)]\tLoss: 0.572170\tWorker: alice\n",
            "Train Epoch: 1 [29440/60032 (49%)]\tLoss: 0.461203\tWorker: alice\n",
            "Train Epoch: 1 [30080/60032 (50%)]\tLoss: 0.524243\tWorker: alice\n",
            "Train Epoch: 1 [30720/60032 (51%)]\tLoss: 0.421558\tWorker: alice\n",
            "Train Epoch: 1 [31360/60032 (52%)]\tLoss: 0.475210\tWorker: alice\n",
            "Train Epoch: 1 [32000/60032 (53%)]\tLoss: 0.633905\tWorker: alice\n",
            "Train Epoch: 1 [32640/60032 (54%)]\tLoss: 0.497826\tWorker: alice\n",
            "Train Epoch: 1 [33280/60032 (55%)]\tLoss: 0.357284\tWorker: alice\n",
            "Train Epoch: 1 [33920/60032 (57%)]\tLoss: 0.314106\tWorker: alice\n",
            "Train Epoch: 1 [34560/60032 (58%)]\tLoss: 0.389738\tWorker: alice\n",
            "Train Epoch: 1 [35200/60032 (59%)]\tLoss: 0.623141\tWorker: alice\n",
            "Train Epoch: 1 [35840/60032 (60%)]\tLoss: 0.440868\tWorker: alice\n",
            "Train Epoch: 1 [36480/60032 (61%)]\tLoss: 0.249926\tWorker: alice\n",
            "Train Epoch: 1 [37120/60032 (62%)]\tLoss: 0.285447\tWorker: alice\n",
            "Train Epoch: 1 [37760/60032 (63%)]\tLoss: 0.337477\tWorker: alice\n",
            "Train Epoch: 1 [38400/60032 (64%)]\tLoss: 0.394750\tWorker: alice\n",
            "Train Epoch: 1 [39040/60032 (65%)]\tLoss: 0.327093\tWorker: alice\n",
            "Train Epoch: 1 [39680/60032 (66%)]\tLoss: 0.380189\tWorker: alice\n",
            "Train Epoch: 1 [40320/60032 (67%)]\tLoss: 2.257521\tWorker: untrustful\n",
            "Train Epoch: 1 [40960/60032 (68%)]\tLoss: 2.167383\tWorker: untrustful\n",
            "Train Epoch: 1 [41600/60032 (69%)]\tLoss: 2.015565\tWorker: untrustful\n",
            "Train Epoch: 1 [42240/60032 (70%)]\tLoss: 1.745838\tWorker: untrustful\n",
            "Train Epoch: 1 [42880/60032 (71%)]\tLoss: 1.352034\tWorker: untrustful\n",
            "Train Epoch: 1 [43520/60032 (72%)]\tLoss: 0.947120\tWorker: untrustful\n",
            "Train Epoch: 1 [44160/60032 (74%)]\tLoss: 0.757646\tWorker: untrustful\n",
            "Train Epoch: 1 [44800/60032 (75%)]\tLoss: 0.660616\tWorker: untrustful\n",
            "Train Epoch: 1 [45440/60032 (76%)]\tLoss: 0.470079\tWorker: untrustful\n",
            "Train Epoch: 1 [46080/60032 (77%)]\tLoss: 0.515319\tWorker: untrustful\n",
            "Train Epoch: 1 [46720/60032 (78%)]\tLoss: 0.461008\tWorker: untrustful\n",
            "Train Epoch: 1 [47360/60032 (79%)]\tLoss: 0.619931\tWorker: untrustful\n",
            "Train Epoch: 1 [48000/60032 (80%)]\tLoss: 0.273160\tWorker: untrustful\n",
            "Train Epoch: 1 [48640/60032 (81%)]\tLoss: 0.499607\tWorker: untrustful\n",
            "Train Epoch: 1 [49280/60032 (82%)]\tLoss: 0.561284\tWorker: untrustful\n",
            "Train Epoch: 1 [49920/60032 (83%)]\tLoss: 0.470607\tWorker: untrustful\n",
            "Train Epoch: 1 [50560/60032 (84%)]\tLoss: 0.386213\tWorker: untrustful\n",
            "Train Epoch: 1 [51200/60032 (85%)]\tLoss: 0.525770\tWorker: untrustful\n",
            "Train Epoch: 1 [51840/60032 (86%)]\tLoss: 0.379949\tWorker: untrustful\n",
            "Train Epoch: 1 [52480/60032 (87%)]\tLoss: 0.369460\tWorker: untrustful\n",
            "Train Epoch: 1 [53120/60032 (88%)]\tLoss: 0.279090\tWorker: untrustful\n",
            "Train Epoch: 1 [53760/60032 (90%)]\tLoss: 0.247280\tWorker: untrustful\n",
            "Train Epoch: 1 [54400/60032 (91%)]\tLoss: 0.450397\tWorker: untrustful\n",
            "Train Epoch: 1 [55040/60032 (92%)]\tLoss: 0.178523\tWorker: untrustful\n",
            "Train Epoch: 1 [55680/60032 (93%)]\tLoss: 0.240944\tWorker: untrustful\n",
            "Train Epoch: 1 [56320/60032 (94%)]\tLoss: 0.424049\tWorker: untrustful\n",
            "Train Epoch: 1 [56960/60032 (95%)]\tLoss: 0.477589\tWorker: untrustful\n",
            "Train Epoch: 1 [57600/60032 (96%)]\tLoss: 0.296968\tWorker: untrustful\n",
            "Train Epoch: 1 [58240/60032 (97%)]\tLoss: 0.337585\tWorker: untrustful\n",
            "Train Epoch: 1 [58880/60032 (98%)]\tLoss: 0.347752\tWorker: untrustful\n",
            "Train Epoch: 1 [59520/60032 (99%)]\tLoss: 0.325438\tWorker: untrustful\n",
            "dev:  [2899.889078846058, 2893.2208422678123, 5682.65011866401]\n",
            "median*1.3:  3769.8558024998756\n",
            "I:  [1, 1, 0]\n",
            "\n",
            "Test set: Average loss: 3.7255, Accuracy: 8822/10000 (88%)\n",
            "\n",
            "Train Epoch: 2 [0/60032 (0%)]\tLoss: 4.426394\tWorker: bob\n",
            "Train Epoch: 2 [640/60032 (1%)]\tLoss: 0.891669\tWorker: bob\n",
            "Train Epoch: 2 [1280/60032 (2%)]\tLoss: 0.358853\tWorker: bob\n",
            "Train Epoch: 2 [1920/60032 (3%)]\tLoss: 0.474380\tWorker: bob\n",
            "Train Epoch: 2 [2560/60032 (4%)]\tLoss: 0.316109\tWorker: bob\n",
            "Train Epoch: 2 [3200/60032 (5%)]\tLoss: 0.450179\tWorker: bob\n",
            "Train Epoch: 2 [3840/60032 (6%)]\tLoss: 0.450882\tWorker: bob\n",
            "Train Epoch: 2 [4480/60032 (7%)]\tLoss: 0.253017\tWorker: bob\n",
            "Train Epoch: 2 [5120/60032 (9%)]\tLoss: 0.242358\tWorker: bob\n",
            "Train Epoch: 2 [5760/60032 (10%)]\tLoss: 0.255865\tWorker: bob\n",
            "Train Epoch: 2 [6400/60032 (11%)]\tLoss: 0.116302\tWorker: bob\n",
            "Train Epoch: 2 [7040/60032 (12%)]\tLoss: 0.210770\tWorker: bob\n",
            "Train Epoch: 2 [7680/60032 (13%)]\tLoss: 0.363016\tWorker: bob\n",
            "Train Epoch: 2 [8320/60032 (14%)]\tLoss: 0.126691\tWorker: bob\n",
            "Train Epoch: 2 [8960/60032 (15%)]\tLoss: 0.128259\tWorker: bob\n",
            "Train Epoch: 2 [9600/60032 (16%)]\tLoss: 0.225117\tWorker: bob\n",
            "Train Epoch: 2 [10240/60032 (17%)]\tLoss: 0.310168\tWorker: bob\n",
            "Train Epoch: 2 [10880/60032 (18%)]\tLoss: 0.191815\tWorker: bob\n",
            "Train Epoch: 2 [11520/60032 (19%)]\tLoss: 0.181165\tWorker: bob\n",
            "Train Epoch: 2 [12160/60032 (20%)]\tLoss: 0.225330\tWorker: bob\n",
            "Train Epoch: 2 [12800/60032 (21%)]\tLoss: 0.121142\tWorker: bob\n",
            "Train Epoch: 2 [13440/60032 (22%)]\tLoss: 0.226557\tWorker: bob\n",
            "Train Epoch: 2 [14080/60032 (23%)]\tLoss: 0.210160\tWorker: bob\n",
            "Train Epoch: 2 [14720/60032 (25%)]\tLoss: 0.131483\tWorker: bob\n",
            "Train Epoch: 2 [15360/60032 (26%)]\tLoss: 0.160915\tWorker: bob\n",
            "Train Epoch: 2 [16000/60032 (27%)]\tLoss: 0.165443\tWorker: bob\n",
            "Train Epoch: 2 [16640/60032 (28%)]\tLoss: 0.109810\tWorker: bob\n",
            "Train Epoch: 2 [17280/60032 (29%)]\tLoss: 0.164051\tWorker: bob\n",
            "Train Epoch: 2 [17920/60032 (30%)]\tLoss: 0.041750\tWorker: bob\n",
            "Train Epoch: 2 [18560/60032 (31%)]\tLoss: 0.218262\tWorker: bob\n",
            "Train Epoch: 2 [19200/60032 (32%)]\tLoss: 0.056752\tWorker: bob\n",
            "Train Epoch: 2 [19840/60032 (33%)]\tLoss: 0.149444\tWorker: bob\n",
            "Train Epoch: 2 [20480/60032 (34%)]\tLoss: 1.425218\tWorker: alice\n",
            "Train Epoch: 2 [21120/60032 (35%)]\tLoss: 0.788279\tWorker: alice\n",
            "Train Epoch: 2 [21760/60032 (36%)]\tLoss: 0.572868\tWorker: alice\n",
            "Train Epoch: 2 [22400/60032 (37%)]\tLoss: 0.458282\tWorker: alice\n",
            "Train Epoch: 2 [23040/60032 (38%)]\tLoss: 0.501566\tWorker: alice\n",
            "Train Epoch: 2 [23680/60032 (39%)]\tLoss: 0.504027\tWorker: alice\n",
            "Train Epoch: 2 [24320/60032 (41%)]\tLoss: 0.314123\tWorker: alice\n",
            "Train Epoch: 2 [24960/60032 (42%)]\tLoss: 0.342150\tWorker: alice\n",
            "Train Epoch: 2 [25600/60032 (43%)]\tLoss: 0.165803\tWorker: alice\n",
            "Train Epoch: 2 [26240/60032 (44%)]\tLoss: 0.199735\tWorker: alice\n",
            "Train Epoch: 2 [26880/60032 (45%)]\tLoss: 0.193062\tWorker: alice\n",
            "Train Epoch: 2 [27520/60032 (46%)]\tLoss: 0.293863\tWorker: alice\n",
            "Train Epoch: 2 [28160/60032 (47%)]\tLoss: 0.096185\tWorker: alice\n",
            "Train Epoch: 2 [28800/60032 (48%)]\tLoss: 0.209590\tWorker: alice\n",
            "Train Epoch: 2 [29440/60032 (49%)]\tLoss: 0.124810\tWorker: alice\n",
            "Train Epoch: 2 [30080/60032 (50%)]\tLoss: 0.198845\tWorker: alice\n",
            "Train Epoch: 2 [30720/60032 (51%)]\tLoss: 0.230421\tWorker: alice\n",
            "Train Epoch: 2 [31360/60032 (52%)]\tLoss: 0.274719\tWorker: alice\n",
            "Train Epoch: 2 [32000/60032 (53%)]\tLoss: 0.321538\tWorker: alice\n",
            "Train Epoch: 2 [32640/60032 (54%)]\tLoss: 0.195281\tWorker: alice\n",
            "Train Epoch: 2 [33280/60032 (55%)]\tLoss: 0.097050\tWorker: alice\n",
            "Train Epoch: 2 [33920/60032 (57%)]\tLoss: 0.162577\tWorker: alice\n",
            "Train Epoch: 2 [34560/60032 (58%)]\tLoss: 0.160516\tWorker: alice\n",
            "Train Epoch: 2 [35200/60032 (59%)]\tLoss: 0.185616\tWorker: alice\n",
            "Train Epoch: 2 [35840/60032 (60%)]\tLoss: 0.132067\tWorker: alice\n",
            "Train Epoch: 2 [36480/60032 (61%)]\tLoss: 0.353558\tWorker: alice\n",
            "Train Epoch: 2 [37120/60032 (62%)]\tLoss: 0.086897\tWorker: alice\n",
            "Train Epoch: 2 [37760/60032 (63%)]\tLoss: 0.109137\tWorker: alice\n",
            "Train Epoch: 2 [38400/60032 (64%)]\tLoss: 0.253958\tWorker: alice\n",
            "Train Epoch: 2 [39040/60032 (65%)]\tLoss: 0.049055\tWorker: alice\n",
            "Train Epoch: 2 [39680/60032 (66%)]\tLoss: 0.295370\tWorker: alice\n",
            "Train Epoch: 2 [40320/60032 (67%)]\tLoss: 20.961905\tWorker: untrustful\n",
            "Train Epoch: 2 [40960/60032 (68%)]\tLoss: 0.670230\tWorker: untrustful\n",
            "Train Epoch: 2 [41600/60032 (69%)]\tLoss: 0.470181\tWorker: untrustful\n",
            "Train Epoch: 2 [42240/60032 (70%)]\tLoss: 0.447046\tWorker: untrustful\n",
            "Train Epoch: 2 [42880/60032 (71%)]\tLoss: 0.324556\tWorker: untrustful\n",
            "Train Epoch: 2 [43520/60032 (72%)]\tLoss: 0.545221\tWorker: untrustful\n",
            "Train Epoch: 2 [44160/60032 (74%)]\tLoss: 0.346707\tWorker: untrustful\n",
            "Train Epoch: 2 [44800/60032 (75%)]\tLoss: 0.151020\tWorker: untrustful\n",
            "Train Epoch: 2 [45440/60032 (76%)]\tLoss: 0.330995\tWorker: untrustful\n",
            "Train Epoch: 2 [46080/60032 (77%)]\tLoss: 0.359767\tWorker: untrustful\n",
            "Train Epoch: 2 [46720/60032 (78%)]\tLoss: 0.187795\tWorker: untrustful\n",
            "Train Epoch: 2 [47360/60032 (79%)]\tLoss: 0.121075\tWorker: untrustful\n",
            "Train Epoch: 2 [48000/60032 (80%)]\tLoss: 0.332942\tWorker: untrustful\n",
            "Train Epoch: 2 [48640/60032 (81%)]\tLoss: 0.164137\tWorker: untrustful\n",
            "Train Epoch: 2 [49280/60032 (82%)]\tLoss: 0.142460\tWorker: untrustful\n",
            "Train Epoch: 2 [49920/60032 (83%)]\tLoss: 0.146217\tWorker: untrustful\n",
            "Train Epoch: 2 [50560/60032 (84%)]\tLoss: 0.110234\tWorker: untrustful\n",
            "Train Epoch: 2 [51200/60032 (85%)]\tLoss: 0.228015\tWorker: untrustful\n",
            "Train Epoch: 2 [51840/60032 (86%)]\tLoss: 0.183352\tWorker: untrustful\n",
            "Train Epoch: 2 [52480/60032 (87%)]\tLoss: 0.133022\tWorker: untrustful\n",
            "Train Epoch: 2 [53120/60032 (88%)]\tLoss: 0.091696\tWorker: untrustful\n",
            "Train Epoch: 2 [53760/60032 (90%)]\tLoss: 0.090215\tWorker: untrustful\n",
            "Train Epoch: 2 [54400/60032 (91%)]\tLoss: 0.206829\tWorker: untrustful\n",
            "Train Epoch: 2 [55040/60032 (92%)]\tLoss: 0.233380\tWorker: untrustful\n",
            "Train Epoch: 2 [55680/60032 (93%)]\tLoss: 0.135420\tWorker: untrustful\n",
            "Train Epoch: 2 [56320/60032 (94%)]\tLoss: 0.093585\tWorker: untrustful\n",
            "Train Epoch: 2 [56960/60032 (95%)]\tLoss: 0.173895\tWorker: untrustful\n",
            "Train Epoch: 2 [57600/60032 (96%)]\tLoss: 0.096377\tWorker: untrustful\n",
            "Train Epoch: 2 [58240/60032 (97%)]\tLoss: 0.189308\tWorker: untrustful\n",
            "Train Epoch: 2 [58880/60032 (98%)]\tLoss: 0.117103\tWorker: untrustful\n",
            "Train Epoch: 2 [59520/60032 (99%)]\tLoss: 0.065694\tWorker: untrustful\n",
            "dev:  [9966.063062031271, 9983.379606629098, 19926.07419759798]\n",
            "median*1.3:  12978.393488617829\n",
            "I:  [1, 1, 0]\n",
            "\n",
            "Test set: Average loss: 4.6548, Accuracy: 7707/10000 (77%)\n",
            "\n",
            "Train Epoch: 3 [0/60032 (0%)]\tLoss: 2.805220\tWorker: bob\n",
            "Train Epoch: 3 [640/60032 (1%)]\tLoss: 0.673045\tWorker: bob\n",
            "Train Epoch: 3 [1280/60032 (2%)]\tLoss: 0.318580\tWorker: bob\n",
            "Train Epoch: 3 [1920/60032 (3%)]\tLoss: 0.432921\tWorker: bob\n",
            "Train Epoch: 3 [2560/60032 (4%)]\tLoss: 0.576726\tWorker: bob\n",
            "Train Epoch: 3 [3200/60032 (5%)]\tLoss: 0.537496\tWorker: bob\n",
            "Train Epoch: 3 [3840/60032 (6%)]\tLoss: 0.452673\tWorker: bob\n",
            "Train Epoch: 3 [4480/60032 (7%)]\tLoss: 0.234043\tWorker: bob\n",
            "Train Epoch: 3 [5120/60032 (9%)]\tLoss: 0.366163\tWorker: bob\n",
            "Train Epoch: 3 [5760/60032 (10%)]\tLoss: 0.104613\tWorker: bob\n",
            "Train Epoch: 3 [6400/60032 (11%)]\tLoss: 0.263994\tWorker: bob\n",
            "Train Epoch: 3 [7040/60032 (12%)]\tLoss: 0.170964\tWorker: bob\n",
            "Train Epoch: 3 [7680/60032 (13%)]\tLoss: 0.218205\tWorker: bob\n",
            "Train Epoch: 3 [8320/60032 (14%)]\tLoss: 0.096299\tWorker: bob\n",
            "Train Epoch: 3 [8960/60032 (15%)]\tLoss: 0.049952\tWorker: bob\n",
            "Train Epoch: 3 [9600/60032 (16%)]\tLoss: 0.413129\tWorker: bob\n",
            "Train Epoch: 3 [10240/60032 (17%)]\tLoss: 0.476366\tWorker: bob\n",
            "Train Epoch: 3 [10880/60032 (18%)]\tLoss: 0.178965\tWorker: bob\n",
            "Train Epoch: 3 [11520/60032 (19%)]\tLoss: 0.165610\tWorker: bob\n",
            "Train Epoch: 3 [12160/60032 (20%)]\tLoss: 0.257184\tWorker: bob\n",
            "Train Epoch: 3 [12800/60032 (21%)]\tLoss: 0.127289\tWorker: bob\n",
            "Train Epoch: 3 [13440/60032 (22%)]\tLoss: 0.061075\tWorker: bob\n",
            "Train Epoch: 3 [14080/60032 (23%)]\tLoss: 0.064355\tWorker: bob\n",
            "Train Epoch: 3 [14720/60032 (25%)]\tLoss: 0.213344\tWorker: bob\n",
            "Train Epoch: 3 [15360/60032 (26%)]\tLoss: 0.274981\tWorker: bob\n",
            "Train Epoch: 3 [16000/60032 (27%)]\tLoss: 0.014880\tWorker: bob\n",
            "Train Epoch: 3 [16640/60032 (28%)]\tLoss: 0.111000\tWorker: bob\n",
            "Train Epoch: 3 [17280/60032 (29%)]\tLoss: 0.124214\tWorker: bob\n",
            "Train Epoch: 3 [17920/60032 (30%)]\tLoss: 0.043022\tWorker: bob\n",
            "Train Epoch: 3 [18560/60032 (31%)]\tLoss: 0.039653\tWorker: bob\n",
            "Train Epoch: 3 [19200/60032 (32%)]\tLoss: 0.577958\tWorker: bob\n",
            "Train Epoch: 3 [19840/60032 (33%)]\tLoss: 0.126548\tWorker: bob\n",
            "Train Epoch: 3 [20480/60032 (34%)]\tLoss: 1.293414\tWorker: alice\n",
            "Train Epoch: 3 [21120/60032 (35%)]\tLoss: 0.143619\tWorker: alice\n",
            "Train Epoch: 3 [21760/60032 (36%)]\tLoss: 0.598686\tWorker: alice\n",
            "Train Epoch: 3 [22400/60032 (37%)]\tLoss: 0.362586\tWorker: alice\n",
            "Train Epoch: 3 [23040/60032 (38%)]\tLoss: 0.168969\tWorker: alice\n",
            "Train Epoch: 3 [23680/60032 (39%)]\tLoss: 0.366451\tWorker: alice\n",
            "Train Epoch: 3 [24320/60032 (41%)]\tLoss: 0.226231\tWorker: alice\n",
            "Train Epoch: 3 [24960/60032 (42%)]\tLoss: 0.159451\tWorker: alice\n",
            "Train Epoch: 3 [25600/60032 (43%)]\tLoss: 0.392424\tWorker: alice\n",
            "Train Epoch: 3 [26240/60032 (44%)]\tLoss: 0.031718\tWorker: alice\n",
            "Train Epoch: 3 [26880/60032 (45%)]\tLoss: 0.241129\tWorker: alice\n",
            "Train Epoch: 3 [27520/60032 (46%)]\tLoss: 0.208714\tWorker: alice\n",
            "Train Epoch: 3 [28160/60032 (47%)]\tLoss: 0.338823\tWorker: alice\n",
            "Train Epoch: 3 [28800/60032 (48%)]\tLoss: 0.114669\tWorker: alice\n",
            "Train Epoch: 3 [29440/60032 (49%)]\tLoss: 0.587048\tWorker: alice\n",
            "Train Epoch: 3 [30080/60032 (50%)]\tLoss: 0.142633\tWorker: alice\n",
            "Train Epoch: 3 [30720/60032 (51%)]\tLoss: 0.120027\tWorker: alice\n",
            "Train Epoch: 3 [31360/60032 (52%)]\tLoss: 0.143678\tWorker: alice\n",
            "Train Epoch: 3 [32000/60032 (53%)]\tLoss: 0.052685\tWorker: alice\n",
            "Train Epoch: 3 [32640/60032 (54%)]\tLoss: 0.072305\tWorker: alice\n",
            "Train Epoch: 3 [33280/60032 (55%)]\tLoss: 0.179431\tWorker: alice\n",
            "Train Epoch: 3 [33920/60032 (57%)]\tLoss: 0.131417\tWorker: alice\n",
            "Train Epoch: 3 [34560/60032 (58%)]\tLoss: 0.179785\tWorker: alice\n",
            "Train Epoch: 3 [35200/60032 (59%)]\tLoss: 0.093928\tWorker: alice\n",
            "Train Epoch: 3 [35840/60032 (60%)]\tLoss: 0.260362\tWorker: alice\n",
            "Train Epoch: 3 [36480/60032 (61%)]\tLoss: 0.158325\tWorker: alice\n",
            "Train Epoch: 3 [37120/60032 (62%)]\tLoss: 0.183546\tWorker: alice\n",
            "Train Epoch: 3 [37760/60032 (63%)]\tLoss: 0.113437\tWorker: alice\n",
            "Train Epoch: 3 [38400/60032 (64%)]\tLoss: 0.104214\tWorker: alice\n",
            "Train Epoch: 3 [39040/60032 (65%)]\tLoss: 0.009692\tWorker: alice\n",
            "Train Epoch: 3 [39680/60032 (66%)]\tLoss: 0.211424\tWorker: alice\n",
            "Train Epoch: 3 [40320/60032 (67%)]\tLoss: 4.918547\tWorker: untrustful\n",
            "Train Epoch: 3 [40960/60032 (68%)]\tLoss: 0.756998\tWorker: untrustful\n",
            "Train Epoch: 3 [41600/60032 (69%)]\tLoss: 0.441706\tWorker: untrustful\n",
            "Train Epoch: 3 [42240/60032 (70%)]\tLoss: 0.741454\tWorker: untrustful\n",
            "Train Epoch: 3 [42880/60032 (71%)]\tLoss: 0.636300\tWorker: untrustful\n",
            "Train Epoch: 3 [43520/60032 (72%)]\tLoss: 0.208433\tWorker: untrustful\n",
            "Train Epoch: 3 [44160/60032 (74%)]\tLoss: 0.254932\tWorker: untrustful\n",
            "Train Epoch: 3 [44800/60032 (75%)]\tLoss: 0.378076\tWorker: untrustful\n",
            "Train Epoch: 3 [45440/60032 (76%)]\tLoss: 0.070917\tWorker: untrustful\n",
            "Train Epoch: 3 [46080/60032 (77%)]\tLoss: 0.449987\tWorker: untrustful\n",
            "Train Epoch: 3 [46720/60032 (78%)]\tLoss: 0.160250\tWorker: untrustful\n",
            "Train Epoch: 3 [47360/60032 (79%)]\tLoss: 0.013984\tWorker: untrustful\n",
            "Train Epoch: 3 [48000/60032 (80%)]\tLoss: 0.536930\tWorker: untrustful\n",
            "Train Epoch: 3 [48640/60032 (81%)]\tLoss: 0.177187\tWorker: untrustful\n",
            "Train Epoch: 3 [49280/60032 (82%)]\tLoss: 0.215983\tWorker: untrustful\n",
            "Train Epoch: 3 [49920/60032 (83%)]\tLoss: 0.417947\tWorker: untrustful\n",
            "Train Epoch: 3 [50560/60032 (84%)]\tLoss: 0.116157\tWorker: untrustful\n",
            "Train Epoch: 3 [51200/60032 (85%)]\tLoss: 0.074407\tWorker: untrustful\n",
            "Train Epoch: 3 [51840/60032 (86%)]\tLoss: 0.164419\tWorker: untrustful\n",
            "Train Epoch: 3 [52480/60032 (87%)]\tLoss: 0.163622\tWorker: untrustful\n",
            "Train Epoch: 3 [53120/60032 (88%)]\tLoss: 0.365199\tWorker: untrustful\n",
            "Train Epoch: 3 [53760/60032 (90%)]\tLoss: 0.092634\tWorker: untrustful\n",
            "Train Epoch: 3 [54400/60032 (91%)]\tLoss: 0.082198\tWorker: untrustful\n",
            "Train Epoch: 3 [55040/60032 (92%)]\tLoss: 0.252443\tWorker: untrustful\n",
            "Train Epoch: 3 [55680/60032 (93%)]\tLoss: 0.070407\tWorker: untrustful\n",
            "Train Epoch: 3 [56320/60032 (94%)]\tLoss: 0.375668\tWorker: untrustful\n",
            "Train Epoch: 3 [56960/60032 (95%)]\tLoss: 0.231886\tWorker: untrustful\n",
            "Train Epoch: 3 [57600/60032 (96%)]\tLoss: 0.330202\tWorker: untrustful\n",
            "Train Epoch: 3 [58240/60032 (97%)]\tLoss: 0.163435\tWorker: untrustful\n",
            "Train Epoch: 3 [58880/60032 (98%)]\tLoss: 0.138088\tWorker: untrustful\n",
            "Train Epoch: 3 [59520/60032 (99%)]\tLoss: 0.210202\tWorker: untrustful\n",
            "dev:  [35606.0000527384, 35607.06753043213, 71211.82497039308]\n",
            "median*1.3:  46289.187789561765\n",
            "I:  [1, 1, 0]\n",
            "\n",
            "Test set: Average loss: 0.2588, Accuracy: 9666/10000 (97%)\n",
            "\n",
            "Train Epoch: 4 [0/60032 (0%)]\tLoss: 0.302490\tWorker: bob\n",
            "Train Epoch: 4 [640/60032 (1%)]\tLoss: 0.100467\tWorker: bob\n",
            "Train Epoch: 4 [1280/60032 (2%)]\tLoss: 0.622641\tWorker: bob\n",
            "Train Epoch: 4 [1920/60032 (3%)]\tLoss: 0.640575\tWorker: bob\n",
            "Train Epoch: 4 [2560/60032 (4%)]\tLoss: 0.435591\tWorker: bob\n",
            "Train Epoch: 4 [3200/60032 (5%)]\tLoss: 0.191316\tWorker: bob\n",
            "Train Epoch: 4 [3840/60032 (6%)]\tLoss: 0.094832\tWorker: bob\n",
            "Train Epoch: 4 [4480/60032 (7%)]\tLoss: 0.441638\tWorker: bob\n",
            "Train Epoch: 4 [5120/60032 (9%)]\tLoss: 0.319048\tWorker: bob\n",
            "Train Epoch: 4 [5760/60032 (10%)]\tLoss: 0.206432\tWorker: bob\n",
            "Train Epoch: 4 [6400/60032 (11%)]\tLoss: 0.134582\tWorker: bob\n",
            "Train Epoch: 4 [7040/60032 (12%)]\tLoss: 0.116023\tWorker: bob\n",
            "Train Epoch: 4 [7680/60032 (13%)]\tLoss: 0.362376\tWorker: bob\n",
            "Train Epoch: 4 [8320/60032 (14%)]\tLoss: 0.491592\tWorker: bob\n",
            "Train Epoch: 4 [8960/60032 (15%)]\tLoss: 0.005296\tWorker: bob\n",
            "Train Epoch: 4 [9600/60032 (16%)]\tLoss: 0.411557\tWorker: bob\n",
            "Train Epoch: 4 [10240/60032 (17%)]\tLoss: 0.359526\tWorker: bob\n",
            "Train Epoch: 4 [10880/60032 (18%)]\tLoss: 0.355287\tWorker: bob\n",
            "Train Epoch: 4 [11520/60032 (19%)]\tLoss: 0.060196\tWorker: bob\n",
            "Train Epoch: 4 [12160/60032 (20%)]\tLoss: 0.058130\tWorker: bob\n",
            "Train Epoch: 4 [12800/60032 (21%)]\tLoss: 0.201455\tWorker: bob\n",
            "Train Epoch: 4 [13440/60032 (22%)]\tLoss: 0.211197\tWorker: bob\n",
            "Train Epoch: 4 [14080/60032 (23%)]\tLoss: 0.406256\tWorker: bob\n",
            "Train Epoch: 4 [14720/60032 (25%)]\tLoss: 0.165953\tWorker: bob\n",
            "Train Epoch: 4 [15360/60032 (26%)]\tLoss: 0.339829\tWorker: bob\n",
            "Train Epoch: 4 [16000/60032 (27%)]\tLoss: 0.688061\tWorker: bob\n",
            "Train Epoch: 4 [16640/60032 (28%)]\tLoss: 0.093750\tWorker: bob\n",
            "Train Epoch: 4 [17280/60032 (29%)]\tLoss: 0.006486\tWorker: bob\n",
            "Train Epoch: 4 [17920/60032 (30%)]\tLoss: 0.374934\tWorker: bob\n",
            "Train Epoch: 4 [18560/60032 (31%)]\tLoss: 0.046666\tWorker: bob\n",
            "Train Epoch: 4 [19200/60032 (32%)]\tLoss: 0.474992\tWorker: bob\n",
            "Train Epoch: 4 [19840/60032 (33%)]\tLoss: 0.191989\tWorker: bob\n",
            "Train Epoch: 4 [20480/60032 (34%)]\tLoss: 0.291045\tWorker: alice\n",
            "Train Epoch: 4 [21120/60032 (35%)]\tLoss: 0.745898\tWorker: alice\n",
            "Train Epoch: 4 [21760/60032 (36%)]\tLoss: 0.119860\tWorker: alice\n",
            "Train Epoch: 4 [22400/60032 (37%)]\tLoss: 0.539767\tWorker: alice\n",
            "Train Epoch: 4 [23040/60032 (38%)]\tLoss: 0.412798\tWorker: alice\n",
            "Train Epoch: 4 [23680/60032 (39%)]\tLoss: 0.294784\tWorker: alice\n",
            "Train Epoch: 4 [24320/60032 (41%)]\tLoss: 0.205923\tWorker: alice\n",
            "Train Epoch: 4 [24960/60032 (42%)]\tLoss: 0.278921\tWorker: alice\n",
            "Train Epoch: 4 [25600/60032 (43%)]\tLoss: 0.804978\tWorker: alice\n",
            "Train Epoch: 4 [26240/60032 (44%)]\tLoss: 0.155203\tWorker: alice\n",
            "Train Epoch: 4 [26880/60032 (45%)]\tLoss: 0.531401\tWorker: alice\n",
            "Train Epoch: 4 [27520/60032 (46%)]\tLoss: 0.054894\tWorker: alice\n",
            "Train Epoch: 4 [28160/60032 (47%)]\tLoss: 0.071598\tWorker: alice\n",
            "Train Epoch: 4 [28800/60032 (48%)]\tLoss: 0.178282\tWorker: alice\n",
            "Train Epoch: 4 [29440/60032 (49%)]\tLoss: 0.825294\tWorker: alice\n",
            "Train Epoch: 4 [30080/60032 (50%)]\tLoss: 0.495457\tWorker: alice\n",
            "Train Epoch: 4 [30720/60032 (51%)]\tLoss: 0.394605\tWorker: alice\n",
            "Train Epoch: 4 [31360/60032 (52%)]\tLoss: 0.260810\tWorker: alice\n",
            "Train Epoch: 4 [32000/60032 (53%)]\tLoss: 0.494044\tWorker: alice\n",
            "Train Epoch: 4 [32640/60032 (54%)]\tLoss: 0.447914\tWorker: alice\n",
            "Train Epoch: 4 [33280/60032 (55%)]\tLoss: 1.462726\tWorker: alice\n",
            "Train Epoch: 4 [33920/60032 (57%)]\tLoss: 0.047555\tWorker: alice\n",
            "Train Epoch: 4 [34560/60032 (58%)]\tLoss: 0.010780\tWorker: alice\n",
            "Train Epoch: 4 [35200/60032 (59%)]\tLoss: 0.357279\tWorker: alice\n",
            "Train Epoch: 4 [35840/60032 (60%)]\tLoss: 0.473903\tWorker: alice\n",
            "Train Epoch: 4 [36480/60032 (61%)]\tLoss: 0.148895\tWorker: alice\n",
            "Train Epoch: 4 [37120/60032 (62%)]\tLoss: 0.056107\tWorker: alice\n",
            "Train Epoch: 4 [37760/60032 (63%)]\tLoss: 0.186274\tWorker: alice\n",
            "Train Epoch: 4 [38400/60032 (64%)]\tLoss: 0.101941\tWorker: alice\n",
            "Train Epoch: 4 [39040/60032 (65%)]\tLoss: 0.024115\tWorker: alice\n",
            "Train Epoch: 4 [39680/60032 (66%)]\tLoss: 0.118982\tWorker: alice\n",
            "Train Epoch: 4 [40320/60032 (67%)]\tLoss: 0.000043\tWorker: untrustful\n",
            "Train Epoch: 4 [40960/60032 (68%)]\tLoss: 0.898607\tWorker: untrustful\n",
            "Train Epoch: 4 [41600/60032 (69%)]\tLoss: 1.584135\tWorker: untrustful\n",
            "Train Epoch: 4 [42240/60032 (70%)]\tLoss: 0.526347\tWorker: untrustful\n",
            "Train Epoch: 4 [42880/60032 (71%)]\tLoss: 0.307275\tWorker: untrustful\n",
            "Train Epoch: 4 [43520/60032 (72%)]\tLoss: 0.000430\tWorker: untrustful\n",
            "Train Epoch: 4 [44160/60032 (74%)]\tLoss: 0.410158\tWorker: untrustful\n",
            "Train Epoch: 4 [44800/60032 (75%)]\tLoss: 0.215836\tWorker: untrustful\n",
            "Train Epoch: 4 [45440/60032 (76%)]\tLoss: 0.039736\tWorker: untrustful\n",
            "Train Epoch: 4 [46080/60032 (77%)]\tLoss: 0.884929\tWorker: untrustful\n",
            "Train Epoch: 4 [46720/60032 (78%)]\tLoss: 0.101227\tWorker: untrustful\n",
            "Train Epoch: 4 [47360/60032 (79%)]\tLoss: 0.404927\tWorker: untrustful\n",
            "Train Epoch: 4 [48000/60032 (80%)]\tLoss: 0.085647\tWorker: untrustful\n",
            "Train Epoch: 4 [48640/60032 (81%)]\tLoss: 0.863270\tWorker: untrustful\n",
            "Train Epoch: 4 [49280/60032 (82%)]\tLoss: 0.235164\tWorker: untrustful\n",
            "Train Epoch: 4 [49920/60032 (83%)]\tLoss: 0.559718\tWorker: untrustful\n",
            "Train Epoch: 4 [50560/60032 (84%)]\tLoss: 0.049439\tWorker: untrustful\n",
            "Train Epoch: 4 [51200/60032 (85%)]\tLoss: 0.342883\tWorker: untrustful\n",
            "Train Epoch: 4 [51840/60032 (86%)]\tLoss: 0.165359\tWorker: untrustful\n",
            "Train Epoch: 4 [52480/60032 (87%)]\tLoss: 0.108500\tWorker: untrustful\n",
            "Train Epoch: 4 [53120/60032 (88%)]\tLoss: 0.143037\tWorker: untrustful\n",
            "Train Epoch: 4 [53760/60032 (90%)]\tLoss: 0.428021\tWorker: untrustful\n",
            "Train Epoch: 4 [54400/60032 (91%)]\tLoss: 0.460771\tWorker: untrustful\n",
            "Train Epoch: 4 [55040/60032 (92%)]\tLoss: 0.133298\tWorker: untrustful\n",
            "Train Epoch: 4 [55680/60032 (93%)]\tLoss: 0.159854\tWorker: untrustful\n",
            "Train Epoch: 4 [56320/60032 (94%)]\tLoss: 0.349294\tWorker: untrustful\n",
            "Train Epoch: 4 [56960/60032 (95%)]\tLoss: 0.241931\tWorker: untrustful\n",
            "Train Epoch: 4 [57600/60032 (96%)]\tLoss: 0.281781\tWorker: untrustful\n",
            "Train Epoch: 4 [58240/60032 (97%)]\tLoss: 0.567656\tWorker: untrustful\n",
            "Train Epoch: 4 [58880/60032 (98%)]\tLoss: 0.199085\tWorker: untrustful\n",
            "Train Epoch: 4 [59520/60032 (99%)]\tLoss: 0.054240\tWorker: untrustful\n",
            "dev:  [66838.5048063695, 66837.6717995276, 133674.43118866102]\n",
            "median*1.3:  86890.05624828035\n",
            "I:  [1, 1, 0]\n",
            "\n",
            "Test set: Average loss: 0.2113, Accuracy: 9660/10000 (97%)\n",
            "\n",
            "Train Epoch: 5 [0/60032 (0%)]\tLoss: 0.072885\tWorker: bob\n",
            "Train Epoch: 5 [640/60032 (1%)]\tLoss: 0.059657\tWorker: bob\n",
            "Train Epoch: 5 [1280/60032 (2%)]\tLoss: 0.297778\tWorker: bob\n",
            "Train Epoch: 5 [1920/60032 (3%)]\tLoss: 0.408535\tWorker: bob\n",
            "Train Epoch: 5 [2560/60032 (4%)]\tLoss: 0.107289\tWorker: bob\n",
            "Train Epoch: 5 [3200/60032 (5%)]\tLoss: 0.062627\tWorker: bob\n",
            "Train Epoch: 5 [3840/60032 (6%)]\tLoss: 0.103446\tWorker: bob\n",
            "Train Epoch: 5 [4480/60032 (7%)]\tLoss: 0.264493\tWorker: bob\n",
            "Train Epoch: 5 [5120/60032 (9%)]\tLoss: 0.104856\tWorker: bob\n",
            "Train Epoch: 5 [5760/60032 (10%)]\tLoss: 0.214203\tWorker: bob\n",
            "Train Epoch: 5 [6400/60032 (11%)]\tLoss: 0.237420\tWorker: bob\n",
            "Train Epoch: 5 [7040/60032 (12%)]\tLoss: 0.017917\tWorker: bob\n",
            "Train Epoch: 5 [7680/60032 (13%)]\tLoss: 0.450972\tWorker: bob\n",
            "Train Epoch: 5 [8320/60032 (14%)]\tLoss: 0.096012\tWorker: bob\n",
            "Train Epoch: 5 [8960/60032 (15%)]\tLoss: 0.223084\tWorker: bob\n",
            "Train Epoch: 5 [9600/60032 (16%)]\tLoss: 0.201713\tWorker: bob\n",
            "Train Epoch: 5 [10240/60032 (17%)]\tLoss: 0.011774\tWorker: bob\n",
            "Train Epoch: 5 [10880/60032 (18%)]\tLoss: 0.010206\tWorker: bob\n",
            "Train Epoch: 5 [11520/60032 (19%)]\tLoss: 0.044913\tWorker: bob\n",
            "Train Epoch: 5 [12160/60032 (20%)]\tLoss: 0.136917\tWorker: bob\n",
            "Train Epoch: 5 [12800/60032 (21%)]\tLoss: 0.054575\tWorker: bob\n",
            "Train Epoch: 5 [13440/60032 (22%)]\tLoss: 0.400252\tWorker: bob\n",
            "Train Epoch: 5 [14080/60032 (23%)]\tLoss: 0.340406\tWorker: bob\n",
            "Train Epoch: 5 [14720/60032 (25%)]\tLoss: 0.128145\tWorker: bob\n",
            "Train Epoch: 5 [15360/60032 (26%)]\tLoss: 0.119181\tWorker: bob\n",
            "Train Epoch: 5 [16000/60032 (27%)]\tLoss: 0.044213\tWorker: bob\n",
            "Train Epoch: 5 [16640/60032 (28%)]\tLoss: 0.028488\tWorker: bob\n",
            "Train Epoch: 5 [17280/60032 (29%)]\tLoss: 0.200555\tWorker: bob\n",
            "Train Epoch: 5 [17920/60032 (30%)]\tLoss: 0.074486\tWorker: bob\n",
            "Train Epoch: 5 [18560/60032 (31%)]\tLoss: 0.165773\tWorker: bob\n",
            "Train Epoch: 5 [19200/60032 (32%)]\tLoss: 0.249904\tWorker: bob\n",
            "Train Epoch: 5 [19840/60032 (33%)]\tLoss: 0.013931\tWorker: bob\n",
            "Train Epoch: 5 [20480/60032 (34%)]\tLoss: 0.105358\tWorker: alice\n",
            "Train Epoch: 5 [21120/60032 (35%)]\tLoss: 0.038803\tWorker: alice\n",
            "Train Epoch: 5 [21760/60032 (36%)]\tLoss: 0.352194\tWorker: alice\n",
            "Train Epoch: 5 [22400/60032 (37%)]\tLoss: 0.399009\tWorker: alice\n",
            "Train Epoch: 5 [23040/60032 (38%)]\tLoss: 0.051212\tWorker: alice\n",
            "Train Epoch: 5 [23680/60032 (39%)]\tLoss: 0.026831\tWorker: alice\n",
            "Train Epoch: 5 [24320/60032 (41%)]\tLoss: 0.050586\tWorker: alice\n",
            "Train Epoch: 5 [24960/60032 (42%)]\tLoss: 0.102587\tWorker: alice\n",
            "Train Epoch: 5 [25600/60032 (43%)]\tLoss: 0.163471\tWorker: alice\n",
            "Train Epoch: 5 [26240/60032 (44%)]\tLoss: 0.121032\tWorker: alice\n",
            "Train Epoch: 5 [26880/60032 (45%)]\tLoss: 0.417243\tWorker: alice\n",
            "Train Epoch: 5 [27520/60032 (46%)]\tLoss: 0.211188\tWorker: alice\n",
            "Train Epoch: 5 [28160/60032 (47%)]\tLoss: 0.469940\tWorker: alice\n",
            "Train Epoch: 5 [28800/60032 (48%)]\tLoss: 0.176587\tWorker: alice\n",
            "Train Epoch: 5 [29440/60032 (49%)]\tLoss: 0.258443\tWorker: alice\n",
            "Train Epoch: 5 [30080/60032 (50%)]\tLoss: 0.047664\tWorker: alice\n",
            "Train Epoch: 5 [30720/60032 (51%)]\tLoss: 0.171375\tWorker: alice\n",
            "Train Epoch: 5 [31360/60032 (52%)]\tLoss: 0.385210\tWorker: alice\n",
            "Train Epoch: 5 [32000/60032 (53%)]\tLoss: 0.123124\tWorker: alice\n",
            "Train Epoch: 5 [32640/60032 (54%)]\tLoss: 0.132785\tWorker: alice\n",
            "Train Epoch: 5 [33280/60032 (55%)]\tLoss: 0.058569\tWorker: alice\n",
            "Train Epoch: 5 [33920/60032 (57%)]\tLoss: 0.163527\tWorker: alice\n",
            "Train Epoch: 5 [34560/60032 (58%)]\tLoss: 0.036154\tWorker: alice\n",
            "Train Epoch: 5 [35200/60032 (59%)]\tLoss: 0.149729\tWorker: alice\n",
            "Train Epoch: 5 [35840/60032 (60%)]\tLoss: 0.688278\tWorker: alice\n",
            "Train Epoch: 5 [36480/60032 (61%)]\tLoss: 0.064357\tWorker: alice\n",
            "Train Epoch: 5 [37120/60032 (62%)]\tLoss: 0.103368\tWorker: alice\n",
            "Train Epoch: 5 [37760/60032 (63%)]\tLoss: 0.524138\tWorker: alice\n",
            "Train Epoch: 5 [38400/60032 (64%)]\tLoss: 0.016902\tWorker: alice\n",
            "Train Epoch: 5 [39040/60032 (65%)]\tLoss: 0.112954\tWorker: alice\n",
            "Train Epoch: 5 [39680/60032 (66%)]\tLoss: 0.001270\tWorker: alice\n",
            "Train Epoch: 5 [40320/60032 (67%)]\tLoss: 0.238649\tWorker: untrustful\n",
            "Train Epoch: 5 [40960/60032 (68%)]\tLoss: 0.003665\tWorker: untrustful\n",
            "Train Epoch: 5 [41600/60032 (69%)]\tLoss: 0.000046\tWorker: untrustful\n",
            "Train Epoch: 5 [42240/60032 (70%)]\tLoss: 0.264249\tWorker: untrustful\n",
            "Train Epoch: 5 [42880/60032 (71%)]\tLoss: 0.071691\tWorker: untrustful\n",
            "Train Epoch: 5 [43520/60032 (72%)]\tLoss: 0.159760\tWorker: untrustful\n",
            "Train Epoch: 5 [44160/60032 (74%)]\tLoss: 0.103783\tWorker: untrustful\n",
            "Train Epoch: 5 [44800/60032 (75%)]\tLoss: 0.452127\tWorker: untrustful\n",
            "Train Epoch: 5 [45440/60032 (76%)]\tLoss: 0.036865\tWorker: untrustful\n",
            "Train Epoch: 5 [46080/60032 (77%)]\tLoss: 0.056886\tWorker: untrustful\n",
            "Train Epoch: 5 [46720/60032 (78%)]\tLoss: 0.410347\tWorker: untrustful\n",
            "Train Epoch: 5 [47360/60032 (79%)]\tLoss: 0.089935\tWorker: untrustful\n",
            "Train Epoch: 5 [48000/60032 (80%)]\tLoss: 0.557974\tWorker: untrustful\n",
            "Train Epoch: 5 [48640/60032 (81%)]\tLoss: 0.202784\tWorker: untrustful\n",
            "Train Epoch: 5 [49280/60032 (82%)]\tLoss: 0.035179\tWorker: untrustful\n",
            "Train Epoch: 5 [49920/60032 (83%)]\tLoss: 0.190344\tWorker: untrustful\n",
            "Train Epoch: 5 [50560/60032 (84%)]\tLoss: 0.081410\tWorker: untrustful\n",
            "Train Epoch: 5 [51200/60032 (85%)]\tLoss: 0.015115\tWorker: untrustful\n",
            "Train Epoch: 5 [51840/60032 (86%)]\tLoss: 0.224446\tWorker: untrustful\n",
            "Train Epoch: 5 [52480/60032 (87%)]\tLoss: 0.213608\tWorker: untrustful\n",
            "Train Epoch: 5 [53120/60032 (88%)]\tLoss: 0.214537\tWorker: untrustful\n",
            "Train Epoch: 5 [53760/60032 (90%)]\tLoss: 0.218007\tWorker: untrustful\n",
            "Train Epoch: 5 [54400/60032 (91%)]\tLoss: 0.112732\tWorker: untrustful\n",
            "Train Epoch: 5 [55040/60032 (92%)]\tLoss: 0.244485\tWorker: untrustful\n",
            "Train Epoch: 5 [55680/60032 (93%)]\tLoss: 0.015163\tWorker: untrustful\n",
            "Train Epoch: 5 [56320/60032 (94%)]\tLoss: 0.006272\tWorker: untrustful\n",
            "Train Epoch: 5 [56960/60032 (95%)]\tLoss: 0.104117\tWorker: untrustful\n",
            "Train Epoch: 5 [57600/60032 (96%)]\tLoss: 0.027502\tWorker: untrustful\n",
            "Train Epoch: 5 [58240/60032 (97%)]\tLoss: 0.332593\tWorker: untrustful\n",
            "Train Epoch: 5 [58880/60032 (98%)]\tLoss: 0.439970\tWorker: untrustful\n",
            "Train Epoch: 5 [59520/60032 (99%)]\tLoss: 0.204532\tWorker: untrustful\n",
            "dev:  [85608.86409429606, 85607.93632033584, 171215.33748877284]\n",
            "median*1.3:  111291.52332258488\n",
            "I:  [1, 1, 0]\n",
            "\n",
            "Test set: Average loss: 0.1908, Accuracy: 9699/10000 (97%)\n",
            "\n",
            "Train Epoch: 6 [0/60032 (0%)]\tLoss: 0.032208\tWorker: bob\n",
            "Train Epoch: 6 [640/60032 (1%)]\tLoss: 0.019365\tWorker: bob\n",
            "Train Epoch: 6 [1280/60032 (2%)]\tLoss: 0.372930\tWorker: bob\n",
            "Train Epoch: 6 [1920/60032 (3%)]\tLoss: 0.244734\tWorker: bob\n",
            "Train Epoch: 6 [2560/60032 (4%)]\tLoss: 0.278204\tWorker: bob\n",
            "Train Epoch: 6 [3200/60032 (5%)]\tLoss: 0.005607\tWorker: bob\n",
            "Train Epoch: 6 [3840/60032 (6%)]\tLoss: 0.199135\tWorker: bob\n",
            "Train Epoch: 6 [4480/60032 (7%)]\tLoss: 0.259949\tWorker: bob\n",
            "Train Epoch: 6 [5120/60032 (9%)]\tLoss: 0.486935\tWorker: bob\n",
            "Train Epoch: 6 [5760/60032 (10%)]\tLoss: 0.068954\tWorker: bob\n",
            "Train Epoch: 6 [6400/60032 (11%)]\tLoss: 0.409229\tWorker: bob\n",
            "Train Epoch: 6 [7040/60032 (12%)]\tLoss: 0.112789\tWorker: bob\n",
            "Train Epoch: 6 [7680/60032 (13%)]\tLoss: 0.046389\tWorker: bob\n",
            "Train Epoch: 6 [8320/60032 (14%)]\tLoss: 0.000977\tWorker: bob\n",
            "Train Epoch: 6 [8960/60032 (15%)]\tLoss: 0.028232\tWorker: bob\n",
            "Train Epoch: 6 [9600/60032 (16%)]\tLoss: 0.065947\tWorker: bob\n",
            "Train Epoch: 6 [10240/60032 (17%)]\tLoss: 0.007372\tWorker: bob\n",
            "Train Epoch: 6 [10880/60032 (18%)]\tLoss: 0.057463\tWorker: bob\n",
            "Train Epoch: 6 [11520/60032 (19%)]\tLoss: 0.102807\tWorker: bob\n",
            "Train Epoch: 6 [12160/60032 (20%)]\tLoss: 0.258408\tWorker: bob\n",
            "Train Epoch: 6 [12800/60032 (21%)]\tLoss: 0.150500\tWorker: bob\n",
            "Train Epoch: 6 [13440/60032 (22%)]\tLoss: 0.002292\tWorker: bob\n",
            "Train Epoch: 6 [14080/60032 (23%)]\tLoss: 0.070220\tWorker: bob\n",
            "Train Epoch: 6 [14720/60032 (25%)]\tLoss: 0.023607\tWorker: bob\n",
            "Train Epoch: 6 [15360/60032 (26%)]\tLoss: 0.066368\tWorker: bob\n",
            "Train Epoch: 6 [16000/60032 (27%)]\tLoss: 0.093876\tWorker: bob\n",
            "Train Epoch: 6 [16640/60032 (28%)]\tLoss: 0.186312\tWorker: bob\n",
            "Train Epoch: 6 [17280/60032 (29%)]\tLoss: 0.028416\tWorker: bob\n",
            "Train Epoch: 6 [17920/60032 (30%)]\tLoss: 0.017020\tWorker: bob\n",
            "Train Epoch: 6 [18560/60032 (31%)]\tLoss: 0.134308\tWorker: bob\n",
            "Train Epoch: 6 [19200/60032 (32%)]\tLoss: 0.210313\tWorker: bob\n",
            "Train Epoch: 6 [19840/60032 (33%)]\tLoss: 0.208758\tWorker: bob\n",
            "Train Epoch: 6 [20480/60032 (34%)]\tLoss: 0.191577\tWorker: alice\n",
            "Train Epoch: 6 [21120/60032 (35%)]\tLoss: 0.102118\tWorker: alice\n",
            "Train Epoch: 6 [21760/60032 (36%)]\tLoss: 0.163528\tWorker: alice\n",
            "Train Epoch: 6 [22400/60032 (37%)]\tLoss: 0.094509\tWorker: alice\n",
            "Train Epoch: 6 [23040/60032 (38%)]\tLoss: 0.615196\tWorker: alice\n",
            "Train Epoch: 6 [23680/60032 (39%)]\tLoss: 0.049638\tWorker: alice\n",
            "Train Epoch: 6 [24320/60032 (41%)]\tLoss: 0.022300\tWorker: alice\n",
            "Train Epoch: 6 [24960/60032 (42%)]\tLoss: 0.147246\tWorker: alice\n",
            "Train Epoch: 6 [25600/60032 (43%)]\tLoss: 0.182899\tWorker: alice\n",
            "Train Epoch: 6 [26240/60032 (44%)]\tLoss: 0.000009\tWorker: alice\n",
            "Train Epoch: 6 [26880/60032 (45%)]\tLoss: 0.138352\tWorker: alice\n",
            "Train Epoch: 6 [27520/60032 (46%)]\tLoss: 0.171613\tWorker: alice\n",
            "Train Epoch: 6 [28160/60032 (47%)]\tLoss: 0.441791\tWorker: alice\n",
            "Train Epoch: 6 [28800/60032 (48%)]\tLoss: 0.017254\tWorker: alice\n",
            "Train Epoch: 6 [29440/60032 (49%)]\tLoss: 0.064198\tWorker: alice\n",
            "Train Epoch: 6 [30080/60032 (50%)]\tLoss: 0.356037\tWorker: alice\n",
            "Train Epoch: 6 [30720/60032 (51%)]\tLoss: 0.005856\tWorker: alice\n",
            "Train Epoch: 6 [31360/60032 (52%)]\tLoss: 0.005054\tWorker: alice\n",
            "Train Epoch: 6 [32000/60032 (53%)]\tLoss: 0.110817\tWorker: alice\n",
            "Train Epoch: 6 [32640/60032 (54%)]\tLoss: 0.044380\tWorker: alice\n",
            "Train Epoch: 6 [33280/60032 (55%)]\tLoss: 0.268282\tWorker: alice\n",
            "Train Epoch: 6 [33920/60032 (57%)]\tLoss: 0.000485\tWorker: alice\n",
            "Train Epoch: 6 [34560/60032 (58%)]\tLoss: 0.151872\tWorker: alice\n",
            "Train Epoch: 6 [35200/60032 (59%)]\tLoss: 0.005212\tWorker: alice\n",
            "Train Epoch: 6 [35840/60032 (60%)]\tLoss: 0.005213\tWorker: alice\n",
            "Train Epoch: 6 [36480/60032 (61%)]\tLoss: 0.285115\tWorker: alice\n",
            "Train Epoch: 6 [37120/60032 (62%)]\tLoss: 0.344489\tWorker: alice\n",
            "Train Epoch: 6 [37760/60032 (63%)]\tLoss: 0.054155\tWorker: alice\n",
            "Train Epoch: 6 [38400/60032 (64%)]\tLoss: 0.057010\tWorker: alice\n",
            "Train Epoch: 6 [39040/60032 (65%)]\tLoss: 0.321856\tWorker: alice\n",
            "Train Epoch: 6 [39680/60032 (66%)]\tLoss: 0.015591\tWorker: alice\n",
            "Train Epoch: 6 [40320/60032 (67%)]\tLoss: 0.006436\tWorker: untrustful\n",
            "Train Epoch: 6 [40960/60032 (68%)]\tLoss: 0.063580\tWorker: untrustful\n",
            "Train Epoch: 6 [41600/60032 (69%)]\tLoss: 0.004304\tWorker: untrustful\n",
            "Train Epoch: 6 [42240/60032 (70%)]\tLoss: 0.504134\tWorker: untrustful\n",
            "Train Epoch: 6 [42880/60032 (71%)]\tLoss: 0.322531\tWorker: untrustful\n",
            "Train Epoch: 6 [43520/60032 (72%)]\tLoss: 0.016806\tWorker: untrustful\n",
            "Train Epoch: 6 [44160/60032 (74%)]\tLoss: 0.148071\tWorker: untrustful\n",
            "Train Epoch: 6 [44800/60032 (75%)]\tLoss: 0.529026\tWorker: untrustful\n",
            "Train Epoch: 6 [45440/60032 (76%)]\tLoss: 0.154078\tWorker: untrustful\n",
            "Train Epoch: 6 [46080/60032 (77%)]\tLoss: 0.049113\tWorker: untrustful\n",
            "Train Epoch: 6 [46720/60032 (78%)]\tLoss: 1.022461\tWorker: untrustful\n",
            "Train Epoch: 6 [47360/60032 (79%)]\tLoss: 0.160333\tWorker: untrustful\n",
            "Train Epoch: 6 [48000/60032 (80%)]\tLoss: 0.140173\tWorker: untrustful\n",
            "Train Epoch: 6 [48640/60032 (81%)]\tLoss: 0.508719\tWorker: untrustful\n",
            "Train Epoch: 6 [49280/60032 (82%)]\tLoss: 0.109895\tWorker: untrustful\n",
            "Train Epoch: 6 [49920/60032 (83%)]\tLoss: 0.160516\tWorker: untrustful\n",
            "Train Epoch: 6 [50560/60032 (84%)]\tLoss: 0.190060\tWorker: untrustful\n",
            "Train Epoch: 6 [51200/60032 (85%)]\tLoss: 0.162461\tWorker: untrustful\n",
            "Train Epoch: 6 [51840/60032 (86%)]\tLoss: 0.298124\tWorker: untrustful\n",
            "Train Epoch: 6 [52480/60032 (87%)]\tLoss: 0.049139\tWorker: untrustful\n",
            "Train Epoch: 6 [53120/60032 (88%)]\tLoss: 0.119555\tWorker: untrustful\n",
            "Train Epoch: 6 [53760/60032 (90%)]\tLoss: 0.228489\tWorker: untrustful\n",
            "Train Epoch: 6 [54400/60032 (91%)]\tLoss: 0.007996\tWorker: untrustful\n",
            "Train Epoch: 6 [55040/60032 (92%)]\tLoss: 0.076874\tWorker: untrustful\n",
            "Train Epoch: 6 [55680/60032 (93%)]\tLoss: 0.189284\tWorker: untrustful\n",
            "Train Epoch: 6 [56320/60032 (94%)]\tLoss: 0.277635\tWorker: untrustful\n",
            "Train Epoch: 6 [56960/60032 (95%)]\tLoss: 0.002313\tWorker: untrustful\n",
            "Train Epoch: 6 [57600/60032 (96%)]\tLoss: 0.264039\tWorker: untrustful\n",
            "Train Epoch: 6 [58240/60032 (97%)]\tLoss: 0.157781\tWorker: untrustful\n",
            "Train Epoch: 6 [58880/60032 (98%)]\tLoss: 0.194455\tWorker: untrustful\n",
            "Train Epoch: 6 [59520/60032 (99%)]\tLoss: 0.299274\tWorker: untrustful\n",
            "dev:  [93388.55901956298, 93388.37131283048, 186775.7578905425]\n",
            "median*1.3:  121405.12672543188\n",
            "I:  [1, 1, 0]\n",
            "\n",
            "Test set: Average loss: 0.1726, Accuracy: 9695/10000 (97%)\n",
            "\n",
            "Train Epoch: 7 [0/60032 (0%)]\tLoss: 0.030702\tWorker: bob\n",
            "Train Epoch: 7 [640/60032 (1%)]\tLoss: 0.073080\tWorker: bob\n",
            "Train Epoch: 7 [1280/60032 (2%)]\tLoss: 0.066272\tWorker: bob\n",
            "Train Epoch: 7 [1920/60032 (3%)]\tLoss: 0.110346\tWorker: bob\n",
            "Train Epoch: 7 [2560/60032 (4%)]\tLoss: 0.044340\tWorker: bob\n",
            "Train Epoch: 7 [3200/60032 (5%)]\tLoss: 0.221457\tWorker: bob\n",
            "Train Epoch: 7 [3840/60032 (6%)]\tLoss: 0.005698\tWorker: bob\n",
            "Train Epoch: 7 [4480/60032 (7%)]\tLoss: 0.009198\tWorker: bob\n",
            "Train Epoch: 7 [5120/60032 (9%)]\tLoss: 0.000370\tWorker: bob\n",
            "Train Epoch: 7 [5760/60032 (10%)]\tLoss: 0.079091\tWorker: bob\n",
            "Train Epoch: 7 [6400/60032 (11%)]\tLoss: 0.220956\tWorker: bob\n",
            "Train Epoch: 7 [7040/60032 (12%)]\tLoss: 0.125222\tWorker: bob\n",
            "Train Epoch: 7 [7680/60032 (13%)]\tLoss: 0.002048\tWorker: bob\n",
            "Train Epoch: 7 [8320/60032 (14%)]\tLoss: 0.027721\tWorker: bob\n",
            "Train Epoch: 7 [8960/60032 (15%)]\tLoss: 0.048798\tWorker: bob\n",
            "Train Epoch: 7 [9600/60032 (16%)]\tLoss: 0.001441\tWorker: bob\n",
            "Train Epoch: 7 [10240/60032 (17%)]\tLoss: 0.003444\tWorker: bob\n",
            "Train Epoch: 7 [10880/60032 (18%)]\tLoss: 0.011897\tWorker: bob\n",
            "Train Epoch: 7 [11520/60032 (19%)]\tLoss: 0.000046\tWorker: bob\n",
            "Train Epoch: 7 [12160/60032 (20%)]\tLoss: 0.131173\tWorker: bob\n",
            "Train Epoch: 7 [12800/60032 (21%)]\tLoss: 0.192725\tWorker: bob\n",
            "Train Epoch: 7 [13440/60032 (22%)]\tLoss: 0.108792\tWorker: bob\n",
            "Train Epoch: 7 [14080/60032 (23%)]\tLoss: 0.117818\tWorker: bob\n",
            "Train Epoch: 7 [14720/60032 (25%)]\tLoss: 0.013708\tWorker: bob\n",
            "Train Epoch: 7 [15360/60032 (26%)]\tLoss: 0.058653\tWorker: bob\n",
            "Train Epoch: 7 [16000/60032 (27%)]\tLoss: 0.015416\tWorker: bob\n",
            "Train Epoch: 7 [16640/60032 (28%)]\tLoss: 0.014667\tWorker: bob\n",
            "Train Epoch: 7 [17280/60032 (29%)]\tLoss: 0.111931\tWorker: bob\n",
            "Train Epoch: 7 [17920/60032 (30%)]\tLoss: 0.273889\tWorker: bob\n",
            "Train Epoch: 7 [18560/60032 (31%)]\tLoss: 0.111999\tWorker: bob\n",
            "Train Epoch: 7 [19200/60032 (32%)]\tLoss: 0.014597\tWorker: bob\n",
            "Train Epoch: 7 [19840/60032 (33%)]\tLoss: 0.013768\tWorker: bob\n",
            "Train Epoch: 7 [20480/60032 (34%)]\tLoss: 0.109389\tWorker: alice\n",
            "Train Epoch: 7 [21120/60032 (35%)]\tLoss: 0.040777\tWorker: alice\n",
            "Train Epoch: 7 [21760/60032 (36%)]\tLoss: 0.004440\tWorker: alice\n",
            "Train Epoch: 7 [22400/60032 (37%)]\tLoss: 0.002686\tWorker: alice\n",
            "Train Epoch: 7 [23040/60032 (38%)]\tLoss: 0.037576\tWorker: alice\n",
            "Train Epoch: 7 [23680/60032 (39%)]\tLoss: 0.058640\tWorker: alice\n",
            "Train Epoch: 7 [24320/60032 (41%)]\tLoss: 0.158757\tWorker: alice\n",
            "Train Epoch: 7 [24960/60032 (42%)]\tLoss: 0.001957\tWorker: alice\n",
            "Train Epoch: 7 [25600/60032 (43%)]\tLoss: 0.084533\tWorker: alice\n",
            "Train Epoch: 7 [26240/60032 (44%)]\tLoss: 0.297865\tWorker: alice\n",
            "Train Epoch: 7 [26880/60032 (45%)]\tLoss: 0.005470\tWorker: alice\n",
            "Train Epoch: 7 [27520/60032 (46%)]\tLoss: 0.168042\tWorker: alice\n",
            "Train Epoch: 7 [28160/60032 (47%)]\tLoss: 0.005374\tWorker: alice\n",
            "Train Epoch: 7 [28800/60032 (48%)]\tLoss: 0.008904\tWorker: alice\n",
            "Train Epoch: 7 [29440/60032 (49%)]\tLoss: 0.002169\tWorker: alice\n",
            "Train Epoch: 7 [30080/60032 (50%)]\tLoss: 0.120617\tWorker: alice\n",
            "Train Epoch: 7 [30720/60032 (51%)]\tLoss: 0.092591\tWorker: alice\n",
            "Train Epoch: 7 [31360/60032 (52%)]\tLoss: 0.017190\tWorker: alice\n",
            "Train Epoch: 7 [32000/60032 (53%)]\tLoss: 0.001531\tWorker: alice\n",
            "Train Epoch: 7 [32640/60032 (54%)]\tLoss: 0.116443\tWorker: alice\n",
            "Train Epoch: 7 [33280/60032 (55%)]\tLoss: 0.007087\tWorker: alice\n",
            "Train Epoch: 7 [33920/60032 (57%)]\tLoss: 0.028353\tWorker: alice\n",
            "Train Epoch: 7 [34560/60032 (58%)]\tLoss: 0.094860\tWorker: alice\n",
            "Train Epoch: 7 [35200/60032 (59%)]\tLoss: 0.097103\tWorker: alice\n",
            "Train Epoch: 7 [35840/60032 (60%)]\tLoss: 0.233098\tWorker: alice\n",
            "Train Epoch: 7 [36480/60032 (61%)]\tLoss: 0.034312\tWorker: alice\n",
            "Train Epoch: 7 [37120/60032 (62%)]\tLoss: 0.260915\tWorker: alice\n",
            "Train Epoch: 7 [37760/60032 (63%)]\tLoss: 0.032588\tWorker: alice\n",
            "Train Epoch: 7 [38400/60032 (64%)]\tLoss: 0.061982\tWorker: alice\n",
            "Train Epoch: 7 [39040/60032 (65%)]\tLoss: 0.252892\tWorker: alice\n",
            "Train Epoch: 7 [39680/60032 (66%)]\tLoss: 0.321244\tWorker: alice\n",
            "Train Epoch: 7 [40320/60032 (67%)]\tLoss: 0.073655\tWorker: untrustful\n",
            "Train Epoch: 7 [40960/60032 (68%)]\tLoss: 0.258003\tWorker: untrustful\n",
            "Train Epoch: 7 [41600/60032 (69%)]\tLoss: 0.181083\tWorker: untrustful\n",
            "Train Epoch: 7 [42240/60032 (70%)]\tLoss: 0.163927\tWorker: untrustful\n",
            "Train Epoch: 7 [42880/60032 (71%)]\tLoss: 0.001592\tWorker: untrustful\n",
            "Train Epoch: 7 [43520/60032 (72%)]\tLoss: 0.355166\tWorker: untrustful\n",
            "Train Epoch: 7 [44160/60032 (74%)]\tLoss: 0.107458\tWorker: untrustful\n",
            "Train Epoch: 7 [44800/60032 (75%)]\tLoss: 0.291587\tWorker: untrustful\n",
            "Train Epoch: 7 [45440/60032 (76%)]\tLoss: 0.296769\tWorker: untrustful\n",
            "Train Epoch: 7 [46080/60032 (77%)]\tLoss: 0.310766\tWorker: untrustful\n",
            "Train Epoch: 7 [46720/60032 (78%)]\tLoss: 0.090427\tWorker: untrustful\n",
            "Train Epoch: 7 [47360/60032 (79%)]\tLoss: 0.427347\tWorker: untrustful\n",
            "Train Epoch: 7 [48000/60032 (80%)]\tLoss: 0.110726\tWorker: untrustful\n",
            "Train Epoch: 7 [48640/60032 (81%)]\tLoss: 0.211982\tWorker: untrustful\n",
            "Train Epoch: 7 [49280/60032 (82%)]\tLoss: 0.102836\tWorker: untrustful\n",
            "Train Epoch: 7 [49920/60032 (83%)]\tLoss: 0.146315\tWorker: untrustful\n",
            "Train Epoch: 7 [50560/60032 (84%)]\tLoss: 0.056407\tWorker: untrustful\n",
            "Train Epoch: 7 [51200/60032 (85%)]\tLoss: 0.330792\tWorker: untrustful\n",
            "Train Epoch: 7 [51840/60032 (86%)]\tLoss: 0.005043\tWorker: untrustful\n",
            "Train Epoch: 7 [52480/60032 (87%)]\tLoss: 0.070917\tWorker: untrustful\n",
            "Train Epoch: 7 [53120/60032 (88%)]\tLoss: 0.029419\tWorker: untrustful\n",
            "Train Epoch: 7 [53760/60032 (90%)]\tLoss: 0.220230\tWorker: untrustful\n",
            "Train Epoch: 7 [54400/60032 (91%)]\tLoss: 0.004710\tWorker: untrustful\n",
            "Train Epoch: 7 [55040/60032 (92%)]\tLoss: 0.109339\tWorker: untrustful\n",
            "Train Epoch: 7 [55680/60032 (93%)]\tLoss: 0.091255\tWorker: untrustful\n",
            "Train Epoch: 7 [56320/60032 (94%)]\tLoss: 0.001110\tWorker: untrustful\n",
            "Train Epoch: 7 [56960/60032 (95%)]\tLoss: 0.129531\tWorker: untrustful\n",
            "Train Epoch: 7 [57600/60032 (96%)]\tLoss: 0.157909\tWorker: untrustful\n",
            "Train Epoch: 7 [58240/60032 (97%)]\tLoss: 0.078770\tWorker: untrustful\n",
            "Train Epoch: 7 [58880/60032 (98%)]\tLoss: 0.049658\tWorker: untrustful\n",
            "Train Epoch: 7 [59520/60032 (99%)]\tLoss: 0.295431\tWorker: untrustful\n",
            "dev:  [96162.62598853187, 96162.86357801665, 192324.75888833366]\n",
            "median*1.3:  125011.72265142165\n",
            "I:  [1, 1, 0]\n",
            "\n",
            "Test set: Average loss: 0.1599, Accuracy: 9745/10000 (97%)\n",
            "\n"
          ]
        }
      ],
      "source": [
        "# central model\n",
        "central_model = Net().to(device)\n",
        "# optimizer for central model not needed if model is not trained\n",
        "#optimizer = optim.SGD(central_model.parameters(), lr=args['lr'])\n",
        "# clients models and optimizers\n",
        "models = {i:Net().to(device) for i in clients}\n",
        "optimizers = {i:optim.SGD(models[i].parameters(), lr=args['lr']) for i in clients}\n",
        "\n",
        "# initialization of dictionary for model aggregation\n",
        "weights = {'conv0_mean_weight' : torch.zeros(size=central_model.conv[0].weight.shape).to(device),\n",
        "           'conv0_mean_bias' : torch.zeros(size=central_model.conv[0].bias.shape).to(device),\n",
        "           'conv2_mean_weight' : torch.zeros(size=central_model.conv[2].weight.shape).to(device),\n",
        "           'conv2_mean_bias' : torch.zeros(size=central_model.conv[2].bias.shape).to(device),\n",
        "           'fc0_mean_weight' : torch.zeros(size=central_model.fc[0].weight.shape).to(device),\n",
        "           'fc0_mean_bias' : torch.zeros(size=central_model.fc[0].bias.shape).to(device),\n",
        "           'fc2_mean_weight' : torch.zeros(size=central_model.fc[2].weight.shape).to(device),\n",
        "           'fc2_mean_bias' : torch.zeros(size=central_model.fc[2].bias.shape).to(device)}\n",
        "\n",
        "# trust values\n",
        "trust = {i:0 for i in clients}\n",
        "r = [0 for i in clients]\n",
        "s = [0 for i in clients]\n",
        "\n",
        "logging.info(\"Starting training!\")\n",
        "\n",
        "for epoch in range(1, args['epochs'] + 1):\n",
        "    train_locally(args, models, device, federated_train_loader, optimizers, epoch)\n",
        "    # we intentionally change the weights of a client's model to make that client behave as untrustful\n",
        "    models[clients[2]].conv[0].weight.data = models[clients[2]].conv[0].weight.data*10\n",
        "    models[clients[2]].conv[2].weight.data = models[clients[2]].conv[2].weight.data*10\n",
        "    models[clients[2]].fc[0].weight.data = models[clients[2]].fc[0].weight.data*10\n",
        "    models[clients[2]].fc[2].weight.data = models[clients[2]].fc[2].weight.data*10\n",
        "    computeTrust(models, trust, r, s)\n",
        "    aggregate(central_model, models, weights, trust)\n",
        "    test(central_model, device, test_loader)"
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "collapsed_sections": [],
      "name": "FL_Trusted_FedAvg.ipynb",
      "toc_visible": true,
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
