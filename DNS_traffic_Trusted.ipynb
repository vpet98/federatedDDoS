{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ej63Q9F5VDWy"
      },
      "source": [
        "Federated Learning using PyTorch and PySyft with Trusted FedAvg on DNS traffic datasets."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "bgSLI4kdwT_r"
      },
      "source": [
        "## Add libraries, define FL clients"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "p9Z-tl_jWgeE"
      },
      "outputs": [],
      "source": [
        "!pip install syft==0.2.9"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "eVP2vWh8pzsS"
      },
      "outputs": [],
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.optim as optim\n",
        "import torch.optim.lr_scheduler as sched\n",
        "from torch.nn import BCELoss\n",
        "import torch.utils.data as tud\n",
        "import pandas as pd\n",
        "from numpy.linalg import norm\n",
        "from statistics import median\n",
        "from sklearn.utils import shuffle\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "import syft as sy\n",
        "\n",
        "# hook PyTorch to PySyft, i.e. add extra functionalities to support Federated Learning and other private AI tools\n",
        "hook = sy.TorchHook(torch)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "R515eomFzXMl"
      },
      "outputs": [],
      "source": [
        "# create clients\n",
        "clients = []\n",
        "clients.append(sy.VirtualWorker(hook, id=\"bob\"))\n",
        "clients.append(sy.VirtualWorker(hook, id=\"alice\"))\n",
        "clients.append(sy.VirtualWorker(hook, id=\"untrustful\"))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "i7GnUhGXwYM3"
      },
      "source": [
        "## Load and preprocess data"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "qHlQttzvcOiI"
      },
      "outputs": [],
      "source": [
        "# load datasets\n",
        "# there is no problem with unbalanced training data, as long as we have many samples of both classes\n",
        "# however, the test set is better to be relatively balanced\n",
        "df1 = pd.read_csv(\"booter1.csv\", nrows=160710)\n",
        "df2 = pd.read_csv(\"booter2.csv\", nrows=115340)\n",
        "df3 = pd.read_csv(\"booter3.csv\", nrows=130200)\n",
        "df4 = pd.read_csv(\"booter4.csv\", nrows=195070)\n",
        "df5 = pd.read_csv(\"booter5.csv\", nrows=30150)\n",
        "df6 = pd.read_csv(\"booter6.csv\", nrows=53050)\n",
        "df7 = pd.read_csv(\"booter7.csv\", nrows=90730)\n",
        "dfg = pd.read_csv(\"wideg.csv\", nrows=247220)\n",
        "dff = pd.read_csv(\"widef.csv\", nrows=278370)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "MvJMt_qF1IzF"
      },
      "outputs": [],
      "source": [
        "# pytorch requires float type values\n",
        "concatenated = pd.concat([df1, df2, df3, df4, df5, df6, df7, dfg, dff], ignore_index=True).astype('float32')\n",
        "concatenated = shuffle(concatenated)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 270
        },
        "id": "B1y6Fp7Etz62",
        "outputId": "ac976a5e-6066-474c-dbe4-4f0b1f10dc28"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "\n",
              "  <div id=\"df-39f3c5e7-acb1-4a83-a390-79bd3f81a750\">\n",
              "    <div class=\"colab-df-container\">\n",
              "      <div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>ip.len</th>\n",
              "      <th>udp.length</th>\n",
              "      <th>dns.flags.authoritative</th>\n",
              "      <th>dns.flags.recdesired</th>\n",
              "      <th>dns.flags.recavail</th>\n",
              "      <th>dns.count.answers</th>\n",
              "      <th>dns.count.add_rr</th>\n",
              "      <th>dns.qry.name</th>\n",
              "      <th>dns.qry.type</th>\n",
              "      <th>target</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>535543</th>\n",
              "      <td>1500.0</td>\n",
              "      <td>4086.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>1.0</td>\n",
              "      <td>1.0</td>\n",
              "      <td>252.0</td>\n",
              "      <td>1.0</td>\n",
              "      <td>70575136.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>1.0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>161225</th>\n",
              "      <td>1054.0</td>\n",
              "      <td>1034.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>1.0</td>\n",
              "      <td>1.0</td>\n",
              "      <td>14.0</td>\n",
              "      <td>23.0</td>\n",
              "      <td>61960000.0</td>\n",
              "      <td>1.0</td>\n",
              "      <td>1.0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>459694</th>\n",
              "      <td>1500.0</td>\n",
              "      <td>4103.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>1.0</td>\n",
              "      <td>1.0</td>\n",
              "      <td>252.0</td>\n",
              "      <td>1.0</td>\n",
              "      <td>70575136.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>1.0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>348837</th>\n",
              "      <td>1500.0</td>\n",
              "      <td>1742.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>1.0</td>\n",
              "      <td>1.0</td>\n",
              "      <td>17.0</td>\n",
              "      <td>23.0</td>\n",
              "      <td>61960000.0</td>\n",
              "      <td>1.0</td>\n",
              "      <td>1.0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1135511</th>\n",
              "      <td>136.0</td>\n",
              "      <td>116.0</td>\n",
              "      <td>1.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>1.0</td>\n",
              "      <td>1.0</td>\n",
              "      <td>55004640.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>\n",
              "      <button class=\"colab-df-convert\" onclick=\"convertToInteractive('df-39f3c5e7-acb1-4a83-a390-79bd3f81a750')\"\n",
              "              title=\"Convert this dataframe to an interactive table.\"\n",
              "              style=\"display:none;\">\n",
              "        \n",
              "  <svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\"viewBox=\"0 0 24 24\"\n",
              "       width=\"24px\">\n",
              "    <path d=\"M0 0h24v24H0V0z\" fill=\"none\"/>\n",
              "    <path d=\"M18.56 5.44l.94 2.06.94-2.06 2.06-.94-2.06-.94-.94-2.06-.94 2.06-2.06.94zm-11 1L8.5 8.5l.94-2.06 2.06-.94-2.06-.94L8.5 2.5l-.94 2.06-2.06.94zm10 10l.94 2.06.94-2.06 2.06-.94-2.06-.94-.94-2.06-.94 2.06-2.06.94z\"/><path d=\"M17.41 7.96l-1.37-1.37c-.4-.4-.92-.59-1.43-.59-.52 0-1.04.2-1.43.59L10.3 9.45l-7.72 7.72c-.78.78-.78 2.05 0 2.83L4 21.41c.39.39.9.59 1.41.59.51 0 1.02-.2 1.41-.59l7.78-7.78 2.81-2.81c.8-.78.8-2.07 0-2.86zM5.41 20L4 18.59l7.72-7.72 1.47 1.35L5.41 20z\"/>\n",
              "  </svg>\n",
              "      </button>\n",
              "      \n",
              "  <style>\n",
              "    .colab-df-container {\n",
              "      display:flex;\n",
              "      flex-wrap:wrap;\n",
              "      gap: 12px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert {\n",
              "      background-color: #E8F0FE;\n",
              "      border: none;\n",
              "      border-radius: 50%;\n",
              "      cursor: pointer;\n",
              "      display: none;\n",
              "      fill: #1967D2;\n",
              "      height: 32px;\n",
              "      padding: 0 0 0 0;\n",
              "      width: 32px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert:hover {\n",
              "      background-color: #E2EBFA;\n",
              "      box-shadow: 0px 1px 2px rgba(60, 64, 67, 0.3), 0px 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
              "      fill: #174EA6;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert {\n",
              "      background-color: #3B4455;\n",
              "      fill: #D2E3FC;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert:hover {\n",
              "      background-color: #434B5C;\n",
              "      box-shadow: 0px 1px 3px 1px rgba(0, 0, 0, 0.15);\n",
              "      filter: drop-shadow(0px 1px 2px rgba(0, 0, 0, 0.3));\n",
              "      fill: #FFFFFF;\n",
              "    }\n",
              "  </style>\n",
              "\n",
              "      <script>\n",
              "        const buttonEl =\n",
              "          document.querySelector('#df-39f3c5e7-acb1-4a83-a390-79bd3f81a750 button.colab-df-convert');\n",
              "        buttonEl.style.display =\n",
              "          google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
              "\n",
              "        async function convertToInteractive(key) {\n",
              "          const element = document.querySelector('#df-39f3c5e7-acb1-4a83-a390-79bd3f81a750');\n",
              "          const dataTable =\n",
              "            await google.colab.kernel.invokeFunction('convertToInteractive',\n",
              "                                                     [key], {});\n",
              "          if (!dataTable) return;\n",
              "\n",
              "          const docLinkHtml = 'Like what you see? Visit the ' +\n",
              "            '<a target=\"_blank\" href=https://colab.research.google.com/notebooks/data_table.ipynb>data table notebook</a>'\n",
              "            + ' to learn more about interactive tables.';\n",
              "          element.innerHTML = '';\n",
              "          dataTable['output_type'] = 'display_data';\n",
              "          await google.colab.output.renderOutput(dataTable, element);\n",
              "          const docLink = document.createElement('div');\n",
              "          docLink.innerHTML = docLinkHtml;\n",
              "          element.appendChild(docLink);\n",
              "        }\n",
              "      </script>\n",
              "    </div>\n",
              "  </div>\n",
              "  "
            ],
            "text/plain": [
              "         ip.len  udp.length  ...  dns.qry.type  target\n",
              "535543   1500.0      4086.0  ...           0.0     1.0\n",
              "161225   1054.0      1034.0  ...           1.0     1.0\n",
              "459694   1500.0      4103.0  ...           0.0     1.0\n",
              "348837   1500.0      1742.0  ...           1.0     1.0\n",
              "1135511   136.0       116.0  ...           0.0     0.0\n",
              "\n",
              "[5 rows x 10 columns]"
            ]
          },
          "metadata": {},
          "execution_count": 4
        }
      ],
      "source": [
        "concatenated.head()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "d3oPA_AP4uxs"
      },
      "outputs": [],
      "source": [
        "trainset = concatenated.iloc[:len(concatenated)*7//10,:]\n",
        "testset = concatenated.iloc[len(concatenated)*7//10:,:]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "e9Y2SVlV0kAu"
      },
      "outputs": [],
      "source": [
        "# we need to normalize data\n",
        "scaler = StandardScaler().fit(trainset.iloc[:,:9])\n",
        "train_scaled = scaler.transform(trainset.iloc[:,:9])\n",
        "# we scale test set using train set distribution,\n",
        "# otherwise we violate rule \"never use test set for training\"\n",
        "test_scaled = scaler.transform(testset.iloc[:,:9])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "hHNAbnLA14Fa"
      },
      "outputs": [],
      "source": [
        "# transform to tensors\n",
        "target_train = torch.tensor(trainset['target'].to_numpy())\n",
        "features_train = torch.tensor(train_scaled)\n",
        "\n",
        "target_test = torch.tensor(testset['target'].to_numpy())\n",
        "features_test = torch.tensor(test_scaled)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "OHmpUsvl22I0"
      },
      "outputs": [],
      "source": [
        "# final datasets for training with pytorch\n",
        "train_dataset = tud.TensorDataset(features_train, target_train)\n",
        "test_dataset = tud.TensorDataset(features_test, target_test)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "wv6y7T3jwbDq"
      },
      "source": [
        "## Define training parameters and model, send data to clients"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "fd5-982uY-6L"
      },
      "outputs": [],
      "source": [
        "# define the args\n",
        "args = {\n",
        "    'use_cuda' : True,\n",
        "    'batch_size' : 128,\n",
        "    'test_batch_size' : 1000,\n",
        "    'lr' : 0.1,\n",
        "    'log_interval' : 200,\n",
        "    'epochs' : 10\n",
        "}\n",
        "\n",
        "# check to use GPU or not\n",
        "use_cuda = args['use_cuda'] and torch.cuda.is_available()\n",
        "device = torch.device(\"cuda\" if use_cuda else \"cpu\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "VShbMriNZC6I"
      },
      "outputs": [],
      "source": [
        "# create a simple feedforward network\n",
        "# n features as input, 2*n hidden layer neurons, 1 output for binary classification\n",
        "class Net(nn.Module):\n",
        "    \n",
        "    def __init__(self):\n",
        "        super(Net, self).__init__()\n",
        "        \n",
        "        self.layers = nn.Sequential(\n",
        "            nn.Linear(in_features=9, out_features=18),\n",
        "            nn.ReLU(),\n",
        "            nn.Linear(in_features=18, out_features=1),\n",
        "            nn.Sigmoid()\n",
        "        )\n",
        "            \n",
        "    def forward(self, x):\n",
        "        return self.layers(x)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "_McSsgBEZq1f"
      },
      "outputs": [],
      "source": [
        "# prepare and distribute the data across workers\n",
        "# normally there is no need to distribute data, since it is already at the clients\n",
        "# this is more of a simulation of federated learning\n",
        "\n",
        "# federate function of PySyft exhausts RAM if we use most part of our datasets \n",
        "federated_dataset = train_dataset.federate(tuple(clients))\n",
        "\n",
        "\"\"\"\n",
        "# below is a memory efficient implementation based on the source code of PySyft's federate function\n",
        "datasets = [sy.BaseDataset(torch.tensor([]).send(c), torch.tensor([]).send(c)) for c in clients]\n",
        "data_loader = tud.DataLoader(train_dataset, batch_size=1024)\n",
        "for dataset_idx, (datas, targetas) in enumerate(data_loader):\n",
        "    worker = clients[dataset_idx % len(clients)]\n",
        "    datas = datas.send(worker)\n",
        "    targetas = targetas.send(worker)\n",
        "    datasets[dataset_idx % len(clients)].data = torch.cat((datasets[dataset_idx % len(clients)].data, datas))\n",
        "    datasets[dataset_idx % len(clients)].targets = torch.cat((datasets[dataset_idx % len(clients)].targets, targetas))\n",
        "\n",
        "federated_dataset = sy.FederatedDataset(datasets)\n",
        "\"\"\"\n",
        "\n",
        "federated_train_loader = sy.FederatedDataLoader(federated_dataset, batch_size=args['batch_size'], shuffle=True)\n",
        "\n",
        "# test data remains at the central entity\n",
        "test_loader = tud.DataLoader(test_dataset, batch_size=args['test_batch_size'], shuffle=True)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "5x-VNSm7PjMa"
      },
      "source": [
        "## Train, test, aggregation, trust computation functions"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "bP_p7GrAlXgB"
      },
      "outputs": [],
      "source": [
        "# classic torch code for training except for the federated part\n",
        "def train_locally(args, models, device, train_loader, optimizers, epoch):\n",
        "    for c, m in models.items():\n",
        "        m.train()\n",
        "        # send models to workers\n",
        "        m.send(c)\n",
        "\n",
        "    # iterate over federated data\n",
        "    for batch_idx, (data, target) in enumerate(train_loader):\n",
        "        data, target = data.to(device), target.to(device)\n",
        "\n",
        "        optimizers[data.location].zero_grad()\n",
        "        output = models[data.location](data)\n",
        "        # we intentionally change the output of a client's model to make that client behave as untrustful\n",
        "        if data.location.id == 'untrustful':\n",
        "            output = torch.ones([len(output), 1], dtype=torch.float32, device=device).send(data.location) - output\n",
        "        # loss is a ptr to the tensor loss at the remote location\n",
        "        loss = BCELoss()(output, torch.reshape(target, [len(target),1]))\n",
        "        # call backward() on the loss ptr, that will send the command to call\n",
        "        # backward on the actual loss tensor present on the remote machine\n",
        "        loss.backward()\n",
        "        optimizers[data.location].step()\n",
        "\n",
        "        if batch_idx % args['log_interval'] == 0:\n",
        "\n",
        "            # get back loss, that was created at remote worker\n",
        "            loss = loss.get()\n",
        "\n",
        "            print('Train Epoch: {} [{}/{} ({:.0f}%)]\\tLoss: {:.6f}\\tWorker: {}'.format(\n",
        "                    epoch, \n",
        "                    batch_idx * args['batch_size'], # number of packets done\n",
        "                    len(train_loader) * args['batch_size'], # total packets left\n",
        "                    100. * batch_idx / len(train_loader),\n",
        "                    loss,\n",
        "                    data.location.id\n",
        "                )\n",
        "            )\n",
        "    \n",
        "    # get back models for aggregation\n",
        "    for m in models.values():\n",
        "        m = m.get()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "cHZpA0PtaKtW"
      },
      "outputs": [],
      "source": [
        "# classic torch code for testing\n",
        "def test(model, device, test_loader):\n",
        "    model.eval()\n",
        "    test_loss = 0\n",
        "    correct = 0\n",
        "    with torch.no_grad():\n",
        "        for data, target in test_loader:\n",
        "            data, target = data.to(device), target.to(device)\n",
        "            output = model(data)\n",
        "\n",
        "            # add losses together\n",
        "            test_loss += BCELoss(reduction='sum')(output, torch.reshape(target, [len(target),1])).item()\n",
        "\n",
        "            # get the index of the max probability class\n",
        "            pred = pred = torch.round(output)\n",
        "            correct += pred.eq(target.view_as(pred)).sum().item()\n",
        "\n",
        "    test_loss /= len(test_loader.dataset)\n",
        "\n",
        "    print('\\nTest set: Average loss: {:.4f}, Accuracy: {}/{} ({:.0f}%)\\n'.format(\n",
        "        test_loss, correct, len(test_loader.dataset),\n",
        "        100. * correct / len(test_loader.dataset)))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Wmz8eKE3bYrt"
      },
      "outputs": [],
      "source": [
        "def aggregate(central_model, models, weights, trust):\n",
        "    with torch.no_grad():\n",
        "        # firstly compute new weight values\n",
        "        dataXtrust = 0\n",
        "        for c in models:\n",
        "            weights['hidden_mean_weight'] += models[c].layers[0].weight.data.clone()*len(federated_dataset.__getitem__(c.id))*trust[c]\n",
        "            weights['hidden_mean_bias'] += models[c].layers[0].bias.data.clone()*len(federated_dataset.__getitem__(c.id))*trust[c]\n",
        "            weights['output_mean_weight'] += models[c].layers[2].weight.data.clone()*len(federated_dataset.__getitem__(c.id))*trust[c]\n",
        "            weights['output_mean_bias'] += models[c].layers[2].bias.data.clone()*len(federated_dataset.__getitem__(c.id))*trust[c]\n",
        "\n",
        "            dataXtrust += len(federated_dataset.__getitem__(c.id))*trust[c]\n",
        "\n",
        "        weights['hidden_mean_weight'] = weights['hidden_mean_weight']/dataXtrust\n",
        "        weights['hidden_mean_bias'] = weights['hidden_mean_bias']/dataXtrust\n",
        "        weights['output_mean_weight'] = weights['output_mean_weight']/dataXtrust\n",
        "        weights['output_mean_bias'] = weights['output_mean_bias']/dataXtrust\n",
        "\n",
        "        # then copy them to the local models\n",
        "        for m in models.values():\n",
        "            m.layers[0].weight.data = weights['hidden_mean_weight'].data.clone()\n",
        "            m.layers[0].bias.data = weights['hidden_mean_bias'].data.clone()\n",
        "            m.layers[2].weight.data = weights['output_mean_weight'].data.clone()\n",
        "            m.layers[2].bias.data = weights['output_mean_bias'].data.clone()\n",
        "\n",
        "        # and to the central model for the test set\n",
        "        central_model.layers[0].weight.data = weights['hidden_mean_weight'].data.clone()\n",
        "        central_model.layers[0].bias.data = weights['hidden_mean_bias'].data.clone()\n",
        "        central_model.layers[2].weight.data = weights['output_mean_weight'].data.clone()\n",
        "        central_model.layers[2].bias.data = weights['output_mean_bias'].data.clone()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "VKXEFqRzdSvj"
      },
      "outputs": [],
      "source": [
        "def computeTrust(models, trust, r, s):\n",
        "    # dev[i] shows how the weights of model of client i differ from the models of all other clients\n",
        "    dev = [0 for i in clients]\n",
        "    for n, i in enumerate(clients):\n",
        "        for j in clients:\n",
        "            dev[n] += norm(models[j].layers[0].weight.data.cpu()-models[i].layers[0].weight.data.cpu())**2\n",
        "            dev[n] += norm(models[j].layers[2].weight.data.cpu()-models[i].layers[2].weight.data.cpu())**2\n",
        "        dev[n] /= len(clients)\n",
        "\n",
        "    # I[i] = 1 if client i acts normally and 0 if malicious or malfunctions\n",
        "    I = [1 if d <= 1.3 * median(sorted(dev)) else 0 for d in dev]\n",
        "    #print(\"dev: \",dev) # testing\n",
        "    #print(\"median*1.3: \", 1.3*median(sorted(dev))) # testing\n",
        "    #print(\"I: \", I) # testing\n",
        " \n",
        "    for i in range(len(clients)):\n",
        "        p1 = 0.5\n",
        "        p2 = lambda x: x/median(sorted(dev)) if x/median(sorted(dev)) > 3 and x > 30 else (x/1000 if x > 1000 else (0.01 if I[i] == 1 and s[i] > 10 else 0.7))\n",
        "        r[i] = p1*r[i] + I[i]\n",
        "        s[i] = p2(dev[i])*s[i] + 1 - I[i]\n",
        "\n",
        "    for i, c in enumerate(clients):\n",
        "        trust[c] = (r[i]+1)/(r[i]+s[i]+2)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "PkbCZpmfLQ7M"
      },
      "source": [
        "## FL training and results"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "B_jH4dE8FVs1",
        "outputId": "168b9dfd-cad4-4da3-f0d0-ba0e7aa7a8c1"
      },
      "outputs": [
        {
          "metadata": {
            "tags": null
          },
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Train Epoch: 1 [0/910592 (0%)]\tLoss: 0.729083\tWorker: bob\n",
            "Train Epoch: 1 [25600/910592 (3%)]\tLoss: 0.043689\tWorker: bob\n",
            "Train Epoch: 1 [51200/910592 (6%)]\tLoss: 0.047886\tWorker: bob\n",
            "Train Epoch: 1 [76800/910592 (8%)]\tLoss: 0.046527\tWorker: bob\n",
            "Train Epoch: 1 [102400/910592 (11%)]\tLoss: 0.041300\tWorker: bob\n",
            "Train Epoch: 1 [128000/910592 (14%)]\tLoss: 0.038757\tWorker: bob\n",
            "Train Epoch: 1 [153600/910592 (17%)]\tLoss: 0.025086\tWorker: bob\n",
            "Train Epoch: 1 [179200/910592 (20%)]\tLoss: 0.052344\tWorker: bob\n",
            "Train Epoch: 1 [204800/910592 (22%)]\tLoss: 0.028032\tWorker: bob\n",
            "Train Epoch: 1 [230400/910592 (25%)]\tLoss: 0.025198\tWorker: bob\n",
            "Train Epoch: 1 [256000/910592 (28%)]\tLoss: 0.029514\tWorker: bob\n",
            "Train Epoch: 1 [281600/910592 (31%)]\tLoss: 0.062800\tWorker: bob\n",
            "Train Epoch: 1 [307200/910592 (34%)]\tLoss: 0.306970\tWorker: alice\n",
            "Train Epoch: 1 [332800/910592 (37%)]\tLoss: 0.093310\tWorker: alice\n",
            "Train Epoch: 1 [358400/910592 (39%)]\tLoss: 0.067395\tWorker: alice\n",
            "Train Epoch: 1 [384000/910592 (42%)]\tLoss: 0.048255\tWorker: alice\n",
            "Train Epoch: 1 [409600/910592 (45%)]\tLoss: 0.014304\tWorker: alice\n",
            "Train Epoch: 1 [435200/910592 (48%)]\tLoss: 0.011107\tWorker: alice\n",
            "Train Epoch: 1 [460800/910592 (51%)]\tLoss: 0.025464\tWorker: alice\n",
            "Train Epoch: 1 [486400/910592 (53%)]\tLoss: 0.031000\tWorker: alice\n",
            "Train Epoch: 1 [512000/910592 (56%)]\tLoss: 0.036592\tWorker: alice\n",
            "Train Epoch: 1 [537600/910592 (59%)]\tLoss: 0.051506\tWorker: alice\n",
            "Train Epoch: 1 [563200/910592 (62%)]\tLoss: 0.034216\tWorker: alice\n",
            "Train Epoch: 1 [588800/910592 (65%)]\tLoss: 0.042454\tWorker: alice\n",
            "Train Epoch: 1 [614400/910592 (67%)]\tLoss: 0.151683\tWorker: untrustful\n",
            "Train Epoch: 1 [640000/910592 (70%)]\tLoss: 0.043402\tWorker: untrustful\n",
            "Train Epoch: 1 [665600/910592 (73%)]\tLoss: 0.037042\tWorker: untrustful\n",
            "Train Epoch: 1 [691200/910592 (76%)]\tLoss: 0.047863\tWorker: untrustful\n",
            "Train Epoch: 1 [716800/910592 (79%)]\tLoss: 0.054981\tWorker: untrustful\n",
            "Train Epoch: 1 [742400/910592 (82%)]\tLoss: 0.026332\tWorker: untrustful\n",
            "Train Epoch: 1 [768000/910592 (84%)]\tLoss: 0.031969\tWorker: untrustful\n",
            "Train Epoch: 1 [793600/910592 (87%)]\tLoss: 0.075453\tWorker: untrustful\n",
            "Train Epoch: 1 [819200/910592 (90%)]\tLoss: 0.036152\tWorker: untrustful\n",
            "Train Epoch: 1 [844800/910592 (93%)]\tLoss: 0.046453\tWorker: untrustful\n",
            "Train Epoch: 1 [870400/910592 (96%)]\tLoss: 0.029432\tWorker: untrustful\n",
            "Train Epoch: 1 [896000/910592 (98%)]\tLoss: 0.067575\tWorker: untrustful\n",
            "\n",
            "Test set: Average loss: 1.2456, Accuracy: 54180/390252 (14%)\n",
            "\n",
            "Train Epoch: 2 [0/910592 (0%)]\tLoss: 1.285507\tWorker: bob\n",
            "Train Epoch: 2 [25600/910592 (3%)]\tLoss: 0.054281\tWorker: bob\n",
            "Train Epoch: 2 [51200/910592 (6%)]\tLoss: 0.062949\tWorker: bob\n",
            "Train Epoch: 2 [76800/910592 (8%)]\tLoss: 0.060538\tWorker: bob\n",
            "Train Epoch: 2 [102400/910592 (11%)]\tLoss: 0.036720\tWorker: bob\n",
            "Train Epoch: 2 [128000/910592 (14%)]\tLoss: 0.047182\tWorker: bob\n",
            "Train Epoch: 2 [153600/910592 (17%)]\tLoss: 0.070408\tWorker: bob\n",
            "Train Epoch: 2 [179200/910592 (20%)]\tLoss: 0.035418\tWorker: bob\n",
            "Train Epoch: 2 [204800/910592 (22%)]\tLoss: 0.014051\tWorker: bob\n",
            "Train Epoch: 2 [230400/910592 (25%)]\tLoss: 0.013677\tWorker: bob\n",
            "Train Epoch: 2 [256000/910592 (28%)]\tLoss: 0.025362\tWorker: bob\n",
            "Train Epoch: 2 [281600/910592 (31%)]\tLoss: 0.043262\tWorker: bob\n",
            "Train Epoch: 2 [307200/910592 (34%)]\tLoss: 0.085747\tWorker: alice\n",
            "Train Epoch: 2 [332800/910592 (37%)]\tLoss: 0.042816\tWorker: alice\n",
            "Train Epoch: 2 [358400/910592 (39%)]\tLoss: 0.072057\tWorker: alice\n",
            "Train Epoch: 2 [384000/910592 (42%)]\tLoss: 0.061038\tWorker: alice\n",
            "Train Epoch: 2 [409600/910592 (45%)]\tLoss: 0.043924\tWorker: alice\n",
            "Train Epoch: 2 [435200/910592 (48%)]\tLoss: 0.049088\tWorker: alice\n",
            "Train Epoch: 2 [460800/910592 (51%)]\tLoss: 0.032481\tWorker: alice\n",
            "Train Epoch: 2 [486400/910592 (53%)]\tLoss: 0.010952\tWorker: alice\n",
            "Train Epoch: 2 [512000/910592 (56%)]\tLoss: 0.024253\tWorker: alice\n",
            "Train Epoch: 2 [537600/910592 (59%)]\tLoss: 0.060806\tWorker: alice\n",
            "Train Epoch: 2 [563200/910592 (62%)]\tLoss: 0.065575\tWorker: alice\n",
            "Train Epoch: 2 [588800/910592 (65%)]\tLoss: 0.054389\tWorker: alice\n",
            "Train Epoch: 2 [614400/910592 (67%)]\tLoss: 0.086857\tWorker: untrustful\n",
            "Train Epoch: 2 [640000/910592 (70%)]\tLoss: 0.043158\tWorker: untrustful\n",
            "Train Epoch: 2 [665600/910592 (73%)]\tLoss: 0.065015\tWorker: untrustful\n",
            "Train Epoch: 2 [691200/910592 (76%)]\tLoss: 0.035248\tWorker: untrustful\n",
            "Train Epoch: 2 [716800/910592 (79%)]\tLoss: 0.036270\tWorker: untrustful\n",
            "Train Epoch: 2 [742400/910592 (82%)]\tLoss: 0.058859\tWorker: untrustful\n",
            "Train Epoch: 2 [768000/910592 (84%)]\tLoss: 0.049709\tWorker: untrustful\n",
            "Train Epoch: 2 [793600/910592 (87%)]\tLoss: 0.064077\tWorker: untrustful\n",
            "Train Epoch: 2 [819200/910592 (90%)]\tLoss: 0.027409\tWorker: untrustful\n",
            "Train Epoch: 2 [844800/910592 (93%)]\tLoss: 0.053438\tWorker: untrustful\n",
            "Train Epoch: 2 [870400/910592 (96%)]\tLoss: 0.038550\tWorker: untrustful\n",
            "Train Epoch: 2 [896000/910592 (98%)]\tLoss: 0.030778\tWorker: untrustful\n",
            "\n",
            "Test set: Average loss: 0.0875, Accuracy: 379981/390252 (97%)\n",
            "\n",
            "Train Epoch: 3 [0/910592 (0%)]\tLoss: 0.146167\tWorker: bob\n",
            "Train Epoch: 3 [25600/910592 (3%)]\tLoss: 0.085129\tWorker: bob\n",
            "Train Epoch: 3 [51200/910592 (6%)]\tLoss: 0.045958\tWorker: bob\n",
            "Train Epoch: 3 [76800/910592 (8%)]\tLoss: 0.016688\tWorker: bob\n",
            "Train Epoch: 3 [102400/910592 (11%)]\tLoss: 0.038430\tWorker: bob\n",
            "Train Epoch: 3 [128000/910592 (14%)]\tLoss: 0.038916\tWorker: bob\n",
            "Train Epoch: 3 [153600/910592 (17%)]\tLoss: 0.044021\tWorker: bob\n",
            "Train Epoch: 3 [179200/910592 (20%)]\tLoss: 0.045136\tWorker: bob\n",
            "Train Epoch: 3 [204800/910592 (22%)]\tLoss: 0.026835\tWorker: bob\n",
            "Train Epoch: 3 [230400/910592 (25%)]\tLoss: 0.035035\tWorker: bob\n",
            "Train Epoch: 3 [256000/910592 (28%)]\tLoss: 0.051273\tWorker: bob\n",
            "Train Epoch: 3 [281600/910592 (31%)]\tLoss: 0.022745\tWorker: bob\n",
            "Train Epoch: 3 [307200/910592 (34%)]\tLoss: 0.055142\tWorker: alice\n",
            "Train Epoch: 3 [332800/910592 (37%)]\tLoss: 0.040020\tWorker: alice\n",
            "Train Epoch: 3 [358400/910592 (39%)]\tLoss: 0.028697\tWorker: alice\n",
            "Train Epoch: 3 [384000/910592 (42%)]\tLoss: 0.046894\tWorker: alice\n",
            "Train Epoch: 3 [409600/910592 (45%)]\tLoss: 0.056447\tWorker: alice\n",
            "Train Epoch: 3 [435200/910592 (48%)]\tLoss: 0.036767\tWorker: alice\n",
            "Train Epoch: 3 [460800/910592 (51%)]\tLoss: 0.033460\tWorker: alice\n",
            "Train Epoch: 3 [486400/910592 (53%)]\tLoss: 0.043649\tWorker: alice\n",
            "Train Epoch: 3 [512000/910592 (56%)]\tLoss: 0.030533\tWorker: alice\n",
            "Train Epoch: 3 [537600/910592 (59%)]\tLoss: 0.052744\tWorker: alice\n",
            "Train Epoch: 3 [563200/910592 (62%)]\tLoss: 0.022344\tWorker: alice\n",
            "Train Epoch: 3 [588800/910592 (65%)]\tLoss: 0.015858\tWorker: alice\n",
            "Train Epoch: 3 [614400/910592 (67%)]\tLoss: 0.065843\tWorker: untrustful\n",
            "Train Epoch: 3 [640000/910592 (70%)]\tLoss: 0.055380\tWorker: untrustful\n",
            "Train Epoch: 3 [665600/910592 (73%)]\tLoss: 0.039467\tWorker: untrustful\n",
            "Train Epoch: 3 [691200/910592 (76%)]\tLoss: 0.046484\tWorker: untrustful\n",
            "Train Epoch: 3 [716800/910592 (79%)]\tLoss: 0.042402\tWorker: untrustful\n",
            "Train Epoch: 3 [742400/910592 (82%)]\tLoss: 0.050667\tWorker: untrustful\n",
            "Train Epoch: 3 [768000/910592 (84%)]\tLoss: 0.051826\tWorker: untrustful\n",
            "Train Epoch: 3 [793600/910592 (87%)]\tLoss: 0.033385\tWorker: untrustful\n",
            "Train Epoch: 3 [819200/910592 (90%)]\tLoss: 0.057906\tWorker: untrustful\n",
            "Train Epoch: 3 [844800/910592 (93%)]\tLoss: 0.022483\tWorker: untrustful\n",
            "Train Epoch: 3 [870400/910592 (96%)]\tLoss: 0.038150\tWorker: untrustful\n",
            "Train Epoch: 3 [896000/910592 (98%)]\tLoss: 0.031674\tWorker: untrustful\n",
            "\n",
            "Test set: Average loss: 0.0602, Accuracy: 380418/390252 (97%)\n",
            "\n",
            "Train Epoch: 4 [0/910592 (0%)]\tLoss: 0.056211\tWorker: bob\n",
            "Train Epoch: 4 [25600/910592 (3%)]\tLoss: 0.019709\tWorker: bob\n",
            "Train Epoch: 4 [51200/910592 (6%)]\tLoss: 0.032527\tWorker: bob\n",
            "Train Epoch: 4 [76800/910592 (8%)]\tLoss: 0.032283\tWorker: bob\n",
            "Train Epoch: 4 [102400/910592 (11%)]\tLoss: 0.022510\tWorker: bob\n",
            "Train Epoch: 4 [128000/910592 (14%)]\tLoss: 0.015577\tWorker: bob\n",
            "Train Epoch: 4 [153600/910592 (17%)]\tLoss: 0.036852\tWorker: bob\n",
            "Train Epoch: 4 [179200/910592 (20%)]\tLoss: 0.029555\tWorker: bob\n",
            "Train Epoch: 4 [204800/910592 (22%)]\tLoss: 0.071352\tWorker: bob\n",
            "Train Epoch: 4 [230400/910592 (25%)]\tLoss: 0.031114\tWorker: bob\n",
            "Train Epoch: 4 [256000/910592 (28%)]\tLoss: 0.019022\tWorker: bob\n",
            "Train Epoch: 4 [281600/910592 (31%)]\tLoss: 0.026054\tWorker: bob\n",
            "Train Epoch: 4 [307200/910592 (34%)]\tLoss: 0.063702\tWorker: alice\n",
            "Train Epoch: 4 [332800/910592 (37%)]\tLoss: 0.031456\tWorker: alice\n",
            "Train Epoch: 4 [358400/910592 (39%)]\tLoss: 0.028049\tWorker: alice\n",
            "Train Epoch: 4 [384000/910592 (42%)]\tLoss: 0.041785\tWorker: alice\n",
            "Train Epoch: 4 [409600/910592 (45%)]\tLoss: 0.030475\tWorker: alice\n",
            "Train Epoch: 4 [435200/910592 (48%)]\tLoss: 0.046121\tWorker: alice\n",
            "Train Epoch: 4 [460800/910592 (51%)]\tLoss: 0.050085\tWorker: alice\n",
            "Train Epoch: 4 [486400/910592 (53%)]\tLoss: 0.041766\tWorker: alice\n",
            "Train Epoch: 4 [512000/910592 (56%)]\tLoss: 0.028181\tWorker: alice\n",
            "Train Epoch: 4 [537600/910592 (59%)]\tLoss: 0.022843\tWorker: alice\n",
            "Train Epoch: 4 [563200/910592 (62%)]\tLoss: 0.027025\tWorker: alice\n",
            "Train Epoch: 4 [588800/910592 (65%)]\tLoss: 0.026872\tWorker: alice\n",
            "Train Epoch: 4 [614400/910592 (67%)]\tLoss: 0.071088\tWorker: untrustful\n",
            "Train Epoch: 4 [640000/910592 (70%)]\tLoss: 0.044673\tWorker: untrustful\n",
            "Train Epoch: 4 [665600/910592 (73%)]\tLoss: 0.034814\tWorker: untrustful\n",
            "Train Epoch: 4 [691200/910592 (76%)]\tLoss: 0.038309\tWorker: untrustful\n",
            "Train Epoch: 4 [716800/910592 (79%)]\tLoss: 0.028409\tWorker: untrustful\n",
            "Train Epoch: 4 [742400/910592 (82%)]\tLoss: 0.025187\tWorker: untrustful\n",
            "Train Epoch: 4 [768000/910592 (84%)]\tLoss: 0.037636\tWorker: untrustful\n",
            "Train Epoch: 4 [793600/910592 (87%)]\tLoss: 0.017486\tWorker: untrustful\n",
            "Train Epoch: 4 [819200/910592 (90%)]\tLoss: 0.089502\tWorker: untrustful\n",
            "Train Epoch: 4 [844800/910592 (93%)]\tLoss: 0.016340\tWorker: untrustful\n",
            "Train Epoch: 4 [870400/910592 (96%)]\tLoss: 0.034292\tWorker: untrustful\n",
            "Train Epoch: 4 [896000/910592 (98%)]\tLoss: 0.073547\tWorker: untrustful\n",
            "\n",
            "Test set: Average loss: 0.0429, Accuracy: 382044/390252 (98%)\n",
            "\n",
            "Train Epoch: 5 [0/910592 (0%)]\tLoss: 0.022647\tWorker: bob\n",
            "Train Epoch: 5 [25600/910592 (3%)]\tLoss: 0.021891\tWorker: bob\n",
            "Train Epoch: 5 [51200/910592 (6%)]\tLoss: 0.050373\tWorker: bob\n",
            "Train Epoch: 5 [76800/910592 (8%)]\tLoss: 0.051146\tWorker: bob\n",
            "Train Epoch: 5 [102400/910592 (11%)]\tLoss: 0.033403\tWorker: bob\n",
            "Train Epoch: 5 [128000/910592 (14%)]\tLoss: 0.013485\tWorker: bob\n",
            "Train Epoch: 5 [153600/910592 (17%)]\tLoss: 0.023654\tWorker: bob\n",
            "Train Epoch: 5 [179200/910592 (20%)]\tLoss: 0.008662\tWorker: bob\n",
            "Train Epoch: 5 [204800/910592 (22%)]\tLoss: 0.036873\tWorker: bob\n",
            "Train Epoch: 5 [230400/910592 (25%)]\tLoss: 0.019884\tWorker: bob\n",
            "Train Epoch: 5 [256000/910592 (28%)]\tLoss: 0.010898\tWorker: bob\n",
            "Train Epoch: 5 [281600/910592 (31%)]\tLoss: 0.026400\tWorker: bob\n",
            "Train Epoch: 5 [307200/910592 (34%)]\tLoss: 0.031071\tWorker: alice\n",
            "Train Epoch: 5 [332800/910592 (37%)]\tLoss: 0.088929\tWorker: alice\n",
            "Train Epoch: 5 [358400/910592 (39%)]\tLoss: 0.026592\tWorker: alice\n",
            "Train Epoch: 5 [384000/910592 (42%)]\tLoss: 0.039682\tWorker: alice\n",
            "Train Epoch: 5 [409600/910592 (45%)]\tLoss: 0.022285\tWorker: alice\n",
            "Train Epoch: 5 [435200/910592 (48%)]\tLoss: 0.011076\tWorker: alice\n",
            "Train Epoch: 5 [460800/910592 (51%)]\tLoss: 0.054376\tWorker: alice\n",
            "Train Epoch: 5 [486400/910592 (53%)]\tLoss: 0.028231\tWorker: alice\n",
            "Train Epoch: 5 [512000/910592 (56%)]\tLoss: 0.040299\tWorker: alice\n",
            "Train Epoch: 5 [537600/910592 (59%)]\tLoss: 0.046154\tWorker: alice\n",
            "Train Epoch: 5 [563200/910592 (62%)]\tLoss: 0.040804\tWorker: alice\n",
            "Train Epoch: 5 [588800/910592 (65%)]\tLoss: 0.041038\tWorker: alice\n",
            "Train Epoch: 5 [614400/910592 (67%)]\tLoss: 0.043421\tWorker: untrustful\n",
            "Train Epoch: 5 [640000/910592 (70%)]\tLoss: 0.054170\tWorker: untrustful\n",
            "Train Epoch: 5 [665600/910592 (73%)]\tLoss: 0.034954\tWorker: untrustful\n",
            "Train Epoch: 5 [691200/910592 (76%)]\tLoss: 0.028549\tWorker: untrustful\n",
            "Train Epoch: 5 [716800/910592 (79%)]\tLoss: 0.026801\tWorker: untrustful\n",
            "Train Epoch: 5 [742400/910592 (82%)]\tLoss: 0.044015\tWorker: untrustful\n",
            "Train Epoch: 5 [768000/910592 (84%)]\tLoss: 0.026600\tWorker: untrustful\n",
            "Train Epoch: 5 [793600/910592 (87%)]\tLoss: 0.030677\tWorker: untrustful\n",
            "Train Epoch: 5 [819200/910592 (90%)]\tLoss: 0.034816\tWorker: untrustful\n",
            "Train Epoch: 5 [844800/910592 (93%)]\tLoss: 0.019602\tWorker: untrustful\n",
            "Train Epoch: 5 [870400/910592 (96%)]\tLoss: 0.020532\tWorker: untrustful\n",
            "Train Epoch: 5 [896000/910592 (98%)]\tLoss: 0.061864\tWorker: untrustful\n",
            "\n",
            "Test set: Average loss: 0.0368, Accuracy: 384119/390252 (98%)\n",
            "\n",
            "Train Epoch: 6 [0/910592 (0%)]\tLoss: 0.042457\tWorker: bob\n",
            "Train Epoch: 6 [25600/910592 (3%)]\tLoss: 0.046507\tWorker: bob\n",
            "Train Epoch: 6 [51200/910592 (6%)]\tLoss: 0.037668\tWorker: bob\n",
            "Train Epoch: 6 [76800/910592 (8%)]\tLoss: 0.024186\tWorker: bob\n",
            "Train Epoch: 6 [102400/910592 (11%)]\tLoss: 0.017154\tWorker: bob\n",
            "Train Epoch: 6 [128000/910592 (14%)]\tLoss: 0.018011\tWorker: bob\n",
            "Train Epoch: 6 [153600/910592 (17%)]\tLoss: 0.012410\tWorker: bob\n",
            "Train Epoch: 6 [179200/910592 (20%)]\tLoss: 0.021120\tWorker: bob\n",
            "Train Epoch: 6 [204800/910592 (22%)]\tLoss: 0.032176\tWorker: bob\n",
            "Train Epoch: 6 [230400/910592 (25%)]\tLoss: 0.021354\tWorker: bob\n",
            "Train Epoch: 6 [256000/910592 (28%)]\tLoss: 0.044838\tWorker: bob\n",
            "Train Epoch: 6 [281600/910592 (31%)]\tLoss: 0.016135\tWorker: bob\n",
            "Train Epoch: 6 [307200/910592 (34%)]\tLoss: 0.034633\tWorker: alice\n",
            "Train Epoch: 6 [332800/910592 (37%)]\tLoss: 0.029010\tWorker: alice\n",
            "Train Epoch: 6 [358400/910592 (39%)]\tLoss: 0.036353\tWorker: alice\n",
            "Train Epoch: 6 [384000/910592 (42%)]\tLoss: 0.035670\tWorker: alice\n",
            "Train Epoch: 6 [409600/910592 (45%)]\tLoss: 0.013514\tWorker: alice\n",
            "Train Epoch: 6 [435200/910592 (48%)]\tLoss: 0.029585\tWorker: alice\n",
            "Train Epoch: 6 [460800/910592 (51%)]\tLoss: 0.011730\tWorker: alice\n",
            "Train Epoch: 6 [486400/910592 (53%)]\tLoss: 0.013430\tWorker: alice\n",
            "Train Epoch: 6 [512000/910592 (56%)]\tLoss: 0.026905\tWorker: alice\n",
            "Train Epoch: 6 [537600/910592 (59%)]\tLoss: 0.022446\tWorker: alice\n",
            "Train Epoch: 6 [563200/910592 (62%)]\tLoss: 0.019700\tWorker: alice\n",
            "Train Epoch: 6 [588800/910592 (65%)]\tLoss: 0.022033\tWorker: alice\n",
            "Train Epoch: 6 [614400/910592 (67%)]\tLoss: 0.060516\tWorker: untrustful\n",
            "Train Epoch: 6 [640000/910592 (70%)]\tLoss: 0.054479\tWorker: untrustful\n",
            "Train Epoch: 6 [665600/910592 (73%)]\tLoss: 0.052786\tWorker: untrustful\n",
            "Train Epoch: 6 [691200/910592 (76%)]\tLoss: 0.013356\tWorker: untrustful\n",
            "Train Epoch: 6 [716800/910592 (79%)]\tLoss: 0.041814\tWorker: untrustful\n",
            "Train Epoch: 6 [742400/910592 (82%)]\tLoss: 0.060386\tWorker: untrustful\n",
            "Train Epoch: 6 [768000/910592 (84%)]\tLoss: 0.003174\tWorker: untrustful\n",
            "Train Epoch: 6 [793600/910592 (87%)]\tLoss: 0.008715\tWorker: untrustful\n",
            "Train Epoch: 6 [819200/910592 (90%)]\tLoss: 0.020499\tWorker: untrustful\n",
            "Train Epoch: 6 [844800/910592 (93%)]\tLoss: 0.035352\tWorker: untrustful\n",
            "Train Epoch: 6 [870400/910592 (96%)]\tLoss: 0.042068\tWorker: untrustful\n",
            "Train Epoch: 6 [896000/910592 (98%)]\tLoss: 0.046433\tWorker: untrustful\n",
            "\n",
            "Test set: Average loss: 0.0315, Accuracy: 386195/390252 (99%)\n",
            "\n",
            "Train Epoch: 7 [0/910592 (0%)]\tLoss: 0.031067\tWorker: bob\n",
            "Train Epoch: 7 [25600/910592 (3%)]\tLoss: 0.021052\tWorker: bob\n",
            "Train Epoch: 7 [51200/910592 (6%)]\tLoss: 0.016016\tWorker: bob\n",
            "Train Epoch: 7 [76800/910592 (8%)]\tLoss: 0.032729\tWorker: bob\n",
            "Train Epoch: 7 [102400/910592 (11%)]\tLoss: 0.025379\tWorker: bob\n",
            "Train Epoch: 7 [128000/910592 (14%)]\tLoss: 0.021051\tWorker: bob\n",
            "Train Epoch: 7 [153600/910592 (17%)]\tLoss: 0.019683\tWorker: bob\n",
            "Train Epoch: 7 [179200/910592 (20%)]\tLoss: 0.031351\tWorker: bob\n",
            "Train Epoch: 7 [204800/910592 (22%)]\tLoss: 0.034681\tWorker: bob\n",
            "Train Epoch: 7 [230400/910592 (25%)]\tLoss: 0.037853\tWorker: bob\n",
            "Train Epoch: 7 [256000/910592 (28%)]\tLoss: 0.033972\tWorker: bob\n",
            "Train Epoch: 7 [281600/910592 (31%)]\tLoss: 0.002125\tWorker: bob\n",
            "Train Epoch: 7 [307200/910592 (34%)]\tLoss: 0.050132\tWorker: alice\n",
            "Train Epoch: 7 [332800/910592 (37%)]\tLoss: 0.018794\tWorker: alice\n",
            "Train Epoch: 7 [358400/910592 (39%)]\tLoss: 0.026035\tWorker: alice\n",
            "Train Epoch: 7 [384000/910592 (42%)]\tLoss: 0.032039\tWorker: alice\n",
            "Train Epoch: 7 [409600/910592 (45%)]\tLoss: 0.039504\tWorker: alice\n",
            "Train Epoch: 7 [435200/910592 (48%)]\tLoss: 0.023330\tWorker: alice\n",
            "Train Epoch: 7 [460800/910592 (51%)]\tLoss: 0.020461\tWorker: alice\n",
            "Train Epoch: 7 [486400/910592 (53%)]\tLoss: 0.006733\tWorker: alice\n",
            "Train Epoch: 7 [512000/910592 (56%)]\tLoss: 0.021919\tWorker: alice\n",
            "Train Epoch: 7 [537600/910592 (59%)]\tLoss: 0.018214\tWorker: alice\n",
            "Train Epoch: 7 [563200/910592 (62%)]\tLoss: 0.026729\tWorker: alice\n",
            "Train Epoch: 7 [588800/910592 (65%)]\tLoss: 0.017616\tWorker: alice\n",
            "Train Epoch: 7 [614400/910592 (67%)]\tLoss: 0.071040\tWorker: untrustful\n",
            "Train Epoch: 7 [640000/910592 (70%)]\tLoss: 0.010847\tWorker: untrustful\n",
            "Train Epoch: 7 [665600/910592 (73%)]\tLoss: 0.038171\tWorker: untrustful\n",
            "Train Epoch: 7 [691200/910592 (76%)]\tLoss: 0.026425\tWorker: untrustful\n",
            "Train Epoch: 7 [716800/910592 (79%)]\tLoss: 0.035551\tWorker: untrustful\n",
            "Train Epoch: 7 [742400/910592 (82%)]\tLoss: 0.022525\tWorker: untrustful\n",
            "Train Epoch: 7 [768000/910592 (84%)]\tLoss: 0.022659\tWorker: untrustful\n",
            "Train Epoch: 7 [793600/910592 (87%)]\tLoss: 0.019593\tWorker: untrustful\n",
            "Train Epoch: 7 [819200/910592 (90%)]\tLoss: 0.025823\tWorker: untrustful\n",
            "Train Epoch: 7 [844800/910592 (93%)]\tLoss: 0.030725\tWorker: untrustful\n",
            "Train Epoch: 7 [870400/910592 (96%)]\tLoss: 0.025385\tWorker: untrustful\n",
            "Train Epoch: 7 [896000/910592 (98%)]\tLoss: 0.039145\tWorker: untrustful\n",
            "\n",
            "Test set: Average loss: 0.0257, Accuracy: 387604/390252 (99%)\n",
            "\n",
            "Train Epoch: 8 [0/910592 (0%)]\tLoss: 0.030748\tWorker: bob\n",
            "Train Epoch: 8 [25600/910592 (3%)]\tLoss: 0.027552\tWorker: bob\n",
            "Train Epoch: 8 [51200/910592 (6%)]\tLoss: 0.022032\tWorker: bob\n",
            "Train Epoch: 8 [76800/910592 (8%)]\tLoss: 0.007761\tWorker: bob\n",
            "Train Epoch: 8 [102400/910592 (11%)]\tLoss: 0.044561\tWorker: bob\n",
            "Train Epoch: 8 [128000/910592 (14%)]\tLoss: 0.017884\tWorker: bob\n",
            "Train Epoch: 8 [153600/910592 (17%)]\tLoss: 0.025046\tWorker: bob\n",
            "Train Epoch: 8 [179200/910592 (20%)]\tLoss: 0.063908\tWorker: bob\n",
            "Train Epoch: 8 [204800/910592 (22%)]\tLoss: 0.016439\tWorker: bob\n",
            "Train Epoch: 8 [230400/910592 (25%)]\tLoss: 0.012215\tWorker: bob\n",
            "Train Epoch: 8 [256000/910592 (28%)]\tLoss: 0.009344\tWorker: bob\n",
            "Train Epoch: 8 [281600/910592 (31%)]\tLoss: 0.031959\tWorker: bob\n",
            "Train Epoch: 8 [307200/910592 (34%)]\tLoss: 0.018435\tWorker: alice\n",
            "Train Epoch: 8 [332800/910592 (37%)]\tLoss: 0.031631\tWorker: alice\n",
            "Train Epoch: 8 [358400/910592 (39%)]\tLoss: 0.023750\tWorker: alice\n",
            "Train Epoch: 8 [384000/910592 (42%)]\tLoss: 0.028201\tWorker: alice\n",
            "Train Epoch: 8 [409600/910592 (45%)]\tLoss: 0.023924\tWorker: alice\n",
            "Train Epoch: 8 [435200/910592 (48%)]\tLoss: 0.016455\tWorker: alice\n",
            "Train Epoch: 8 [460800/910592 (51%)]\tLoss: 0.029567\tWorker: alice\n",
            "Train Epoch: 8 [486400/910592 (53%)]\tLoss: 0.009414\tWorker: alice\n",
            "Train Epoch: 8 [512000/910592 (56%)]\tLoss: 0.018724\tWorker: alice\n",
            "Train Epoch: 8 [537600/910592 (59%)]\tLoss: 0.004249\tWorker: alice\n",
            "Train Epoch: 8 [563200/910592 (62%)]\tLoss: 0.012082\tWorker: alice\n",
            "Train Epoch: 8 [588800/910592 (65%)]\tLoss: 0.014332\tWorker: alice\n",
            "Train Epoch: 8 [614400/910592 (67%)]\tLoss: 0.099048\tWorker: untrustful\n",
            "Train Epoch: 8 [640000/910592 (70%)]\tLoss: 0.033601\tWorker: untrustful\n",
            "Train Epoch: 8 [665600/910592 (73%)]\tLoss: 0.019357\tWorker: untrustful\n",
            "Train Epoch: 8 [691200/910592 (76%)]\tLoss: 0.018577\tWorker: untrustful\n",
            "Train Epoch: 8 [716800/910592 (79%)]\tLoss: 0.066199\tWorker: untrustful\n",
            "Train Epoch: 8 [742400/910592 (82%)]\tLoss: 0.022811\tWorker: untrustful\n",
            "Train Epoch: 8 [768000/910592 (84%)]\tLoss: 0.025121\tWorker: untrustful\n",
            "Train Epoch: 8 [793600/910592 (87%)]\tLoss: 0.013957\tWorker: untrustful\n",
            "Train Epoch: 8 [819200/910592 (90%)]\tLoss: 0.036971\tWorker: untrustful\n",
            "Train Epoch: 8 [844800/910592 (93%)]\tLoss: 0.020149\tWorker: untrustful\n",
            "Train Epoch: 8 [870400/910592 (96%)]\tLoss: 0.032849\tWorker: untrustful\n",
            "Train Epoch: 8 [896000/910592 (98%)]\tLoss: 0.026237\tWorker: untrustful\n",
            "\n",
            "Test set: Average loss: 0.0232, Accuracy: 387564/390252 (99%)\n",
            "\n",
            "Train Epoch: 9 [0/910592 (0%)]\tLoss: 0.005320\tWorker: bob\n",
            "Train Epoch: 9 [25600/910592 (3%)]\tLoss: 0.017880\tWorker: bob\n",
            "Train Epoch: 9 [51200/910592 (6%)]\tLoss: 0.023674\tWorker: bob\n",
            "Train Epoch: 9 [76800/910592 (8%)]\tLoss: 0.014368\tWorker: bob\n",
            "Train Epoch: 9 [102400/910592 (11%)]\tLoss: 0.015007\tWorker: bob\n",
            "Train Epoch: 9 [128000/910592 (14%)]\tLoss: 0.006787\tWorker: bob\n",
            "Train Epoch: 9 [153600/910592 (17%)]\tLoss: 0.013289\tWorker: bob\n",
            "Train Epoch: 9 [179200/910592 (20%)]\tLoss: 0.046123\tWorker: bob\n",
            "Train Epoch: 9 [204800/910592 (22%)]\tLoss: 0.061581\tWorker: bob\n",
            "Train Epoch: 9 [230400/910592 (25%)]\tLoss: 0.011452\tWorker: bob\n",
            "Train Epoch: 9 [256000/910592 (28%)]\tLoss: 0.006882\tWorker: bob\n",
            "Train Epoch: 9 [281600/910592 (31%)]\tLoss: 0.019885\tWorker: bob\n",
            "Train Epoch: 9 [307200/910592 (34%)]\tLoss: 0.009313\tWorker: alice\n",
            "Train Epoch: 9 [332800/910592 (37%)]\tLoss: 0.022463\tWorker: alice\n",
            "Train Epoch: 9 [358400/910592 (39%)]\tLoss: 0.036671\tWorker: alice\n",
            "Train Epoch: 9 [384000/910592 (42%)]\tLoss: 0.043668\tWorker: alice\n",
            "Train Epoch: 9 [409600/910592 (45%)]\tLoss: 0.015485\tWorker: alice\n",
            "Train Epoch: 9 [435200/910592 (48%)]\tLoss: 0.013529\tWorker: alice\n",
            "Train Epoch: 9 [460800/910592 (51%)]\tLoss: 0.026241\tWorker: alice\n",
            "Train Epoch: 9 [486400/910592 (53%)]\tLoss: 0.022212\tWorker: alice\n",
            "Train Epoch: 9 [512000/910592 (56%)]\tLoss: 0.010817\tWorker: alice\n",
            "Train Epoch: 9 [537600/910592 (59%)]\tLoss: 0.016226\tWorker: alice\n",
            "Train Epoch: 9 [563200/910592 (62%)]\tLoss: 0.025831\tWorker: alice\n",
            "Train Epoch: 9 [588800/910592 (65%)]\tLoss: 0.018140\tWorker: alice\n",
            "Train Epoch: 9 [614400/910592 (67%)]\tLoss: 15.758308\tWorker: untrustful\n",
            "Train Epoch: 9 [640000/910592 (70%)]\tLoss: 18.132854\tWorker: untrustful\n",
            "Train Epoch: 9 [665600/910592 (73%)]\tLoss: 15.758308\tWorker: untrustful\n",
            "Train Epoch: 9 [691200/910592 (76%)]\tLoss: 17.916986\tWorker: untrustful\n",
            "Train Epoch: 9 [716800/910592 (79%)]\tLoss: 15.110707\tWorker: untrustful\n",
            "Train Epoch: 9 [742400/910592 (82%)]\tLoss: 18.132854\tWorker: untrustful\n",
            "Train Epoch: 9 [768000/910592 (84%)]\tLoss: 16.621778\tWorker: untrustful\n",
            "Train Epoch: 9 [793600/910592 (87%)]\tLoss: 17.485250\tWorker: untrustful\n",
            "Train Epoch: 9 [819200/910592 (90%)]\tLoss: 17.269382\tWorker: untrustful\n",
            "Train Epoch: 9 [844800/910592 (93%)]\tLoss: 14.247239\tWorker: untrustful\n",
            "Train Epoch: 9 [870400/910592 (96%)]\tLoss: 17.701118\tWorker: untrustful\n",
            "Train Epoch: 9 [896000/910592 (98%)]\tLoss: 18.132854\tWorker: untrustful\n",
            "\n",
            "Test set: Average loss: 1.0139, Accuracy: 315665/390252 (81%)\n",
            "\n",
            "Train Epoch: 10 [0/910592 (0%)]\tLoss: 1.215945\tWorker: bob\n",
            "Train Epoch: 10 [25600/910592 (3%)]\tLoss: 0.018267\tWorker: bob\n",
            "Train Epoch: 10 [51200/910592 (6%)]\tLoss: 0.014846\tWorker: bob\n",
            "Train Epoch: 10 [76800/910592 (8%)]\tLoss: 0.010223\tWorker: bob\n",
            "Train Epoch: 10 [102400/910592 (11%)]\tLoss: 0.015937\tWorker: bob\n",
            "Train Epoch: 10 [128000/910592 (14%)]\tLoss: 0.009667\tWorker: bob\n",
            "Train Epoch: 10 [153600/910592 (17%)]\tLoss: 0.015023\tWorker: bob\n",
            "Train Epoch: 10 [179200/910592 (20%)]\tLoss: 0.010521\tWorker: bob\n",
            "Train Epoch: 10 [204800/910592 (22%)]\tLoss: 0.015231\tWorker: bob\n",
            "Train Epoch: 10 [230400/910592 (25%)]\tLoss: 0.011821\tWorker: bob\n",
            "Train Epoch: 10 [256000/910592 (28%)]\tLoss: 0.006757\tWorker: bob\n",
            "Train Epoch: 10 [281600/910592 (31%)]\tLoss: 0.027430\tWorker: bob\n",
            "Train Epoch: 10 [307200/910592 (34%)]\tLoss: 0.028282\tWorker: alice\n",
            "Train Epoch: 10 [332800/910592 (37%)]\tLoss: 0.015860\tWorker: alice\n",
            "Train Epoch: 10 [358400/910592 (39%)]\tLoss: 0.007410\tWorker: alice\n",
            "Train Epoch: 10 [384000/910592 (42%)]\tLoss: 0.005759\tWorker: alice\n",
            "Train Epoch: 10 [409600/910592 (45%)]\tLoss: 0.008353\tWorker: alice\n",
            "Train Epoch: 10 [435200/910592 (48%)]\tLoss: 0.005144\tWorker: alice\n",
            "Train Epoch: 10 [460800/910592 (51%)]\tLoss: 0.010072\tWorker: alice\n",
            "Train Epoch: 10 [486400/910592 (53%)]\tLoss: 0.018557\tWorker: alice\n",
            "Train Epoch: 10 [512000/910592 (56%)]\tLoss: 0.017335\tWorker: alice\n",
            "Train Epoch: 10 [537600/910592 (59%)]\tLoss: 0.052345\tWorker: alice\n",
            "Train Epoch: 10 [563200/910592 (62%)]\tLoss: 0.027857\tWorker: alice\n",
            "Train Epoch: 10 [588800/910592 (65%)]\tLoss: 0.002882\tWorker: alice\n",
            "Train Epoch: 10 [614400/910592 (67%)]\tLoss: 0.039647\tWorker: untrustful\n",
            "Train Epoch: 10 [640000/910592 (70%)]\tLoss: 0.059324\tWorker: untrustful\n",
            "Train Epoch: 10 [665600/910592 (73%)]\tLoss: 0.049396\tWorker: untrustful\n",
            "Train Epoch: 10 [691200/910592 (76%)]\tLoss: 0.044119\tWorker: untrustful\n",
            "Train Epoch: 10 [716800/910592 (79%)]\tLoss: 0.011229\tWorker: untrustful\n",
            "Train Epoch: 10 [742400/910592 (82%)]\tLoss: 0.048444\tWorker: untrustful\n",
            "Train Epoch: 10 [768000/910592 (84%)]\tLoss: 0.061857\tWorker: untrustful\n",
            "Train Epoch: 10 [793600/910592 (87%)]\tLoss: 0.017352\tWorker: untrustful\n",
            "Train Epoch: 10 [819200/910592 (90%)]\tLoss: 0.028962\tWorker: untrustful\n",
            "Train Epoch: 10 [844800/910592 (93%)]\tLoss: 0.024587\tWorker: untrustful\n",
            "Train Epoch: 10 [870400/910592 (96%)]\tLoss: 0.033799\tWorker: untrustful\n",
            "Train Epoch: 10 [896000/910592 (98%)]\tLoss: 0.014033\tWorker: untrustful\n",
            "\n",
            "Test set: Average loss: 0.0165, Accuracy: 388208/390252 (99%)\n",
            "\n"
          ]
        }
      ],
      "source": [
        "# central model\n",
        "central_model = Net().to(device)\n",
        "# optimizer for central model not needed if model is not trained\n",
        "#optimizer = optim.SGD(central_model.parameters(), lr=args['lr'])\n",
        "\n",
        "# clients' models, optimizers and schedulers for learning rate\n",
        "models = {i:Net().to(device) for i in clients}\n",
        "optimizers = {i:optim.SGD(models[i].parameters(), lr=args['lr']) for i in clients}\n",
        "#lamda = lambda epoch: 10 if epoch < 2 else (1 if epoch < 6 else 0.1)\n",
        "#schedulers = {i:sched.LambdaLR(optimizers[i], lr_lambda=lamda) for i in clients}\n",
        "\n",
        "# initialization of dictionary for models aggregation\n",
        "weights = {'hidden_mean_weight' : torch.zeros(size=central_model.layers[0].weight.shape).to(device),\n",
        "           'hidden_mean_bias' : torch.zeros(size=central_model.layers[0].bias.shape).to(device),\n",
        "           'output_mean_weight' : torch.zeros(size=central_model.layers[2].weight.shape).to(device),\n",
        "           'output_mean_bias' : torch.zeros(size=central_model.layers[2].bias.shape).to(device)}\n",
        "\n",
        "# trust values\n",
        "trust = {i:0 for i in clients}\n",
        "r = [0 for i in clients]\n",
        "s = [0 for i in clients]\n",
        "\n",
        "for epoch in range(1, args['epochs'] + 1):\n",
        "    # below function is modified to simulate untrustful behavior of a client\n",
        "    train_locally(args, models, device, federated_train_loader, optimizers, epoch)\n",
        "    # we also shift the weights of the untrustful client to have more influence on the aggregated model\n",
        "    models[clients[2]].layers[0].weight.data *= 1.5\n",
        "    models[clients[2]].layers[2].weight.data *= 1.5\n",
        "    #for scheduler in schedulers.values():\n",
        "        #scheduler.step()\n",
        "    computeTrust(models, trust, r, s)\n",
        "    aggregate(central_model, models, weights, trust)\n",
        "    test(central_model, device, test_loader)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Lu1jzpb05zXC"
      },
      "source": [
        "## Training without trust"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "53075063-cc9d-463c-c253-3f3a6d8e368c",
        "id": "usjbaDGcYZU7"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Train Epoch: 1 [0/910592 (0%)]\tLoss: 0.707035\tWorker: bob\n",
            "Train Epoch: 1 [25600/910592 (3%)]\tLoss: 0.070420\tWorker: bob\n",
            "Train Epoch: 1 [51200/910592 (6%)]\tLoss: 0.089754\tWorker: bob\n",
            "Train Epoch: 1 [76800/910592 (8%)]\tLoss: 0.049674\tWorker: bob\n",
            "Train Epoch: 1 [102400/910592 (11%)]\tLoss: 0.044907\tWorker: bob\n",
            "Train Epoch: 1 [128000/910592 (14%)]\tLoss: 0.042290\tWorker: bob\n",
            "Train Epoch: 1 [153600/910592 (17%)]\tLoss: 0.074268\tWorker: bob\n",
            "Train Epoch: 1 [179200/910592 (20%)]\tLoss: 0.063465\tWorker: bob\n",
            "Train Epoch: 1 [204800/910592 (22%)]\tLoss: 0.035177\tWorker: bob\n",
            "Train Epoch: 1 [230400/910592 (25%)]\tLoss: 0.022060\tWorker: bob\n",
            "Train Epoch: 1 [256000/910592 (28%)]\tLoss: 0.044067\tWorker: bob\n",
            "Train Epoch: 1 [281600/910592 (31%)]\tLoss: 0.026888\tWorker: bob\n",
            "Train Epoch: 1 [307200/910592 (34%)]\tLoss: 0.302484\tWorker: alice\n",
            "Train Epoch: 1 [332800/910592 (37%)]\tLoss: 0.096393\tWorker: alice\n",
            "Train Epoch: 1 [358400/910592 (39%)]\tLoss: 0.045506\tWorker: alice\n",
            "Train Epoch: 1 [384000/910592 (42%)]\tLoss: 0.041356\tWorker: alice\n",
            "Train Epoch: 1 [409600/910592 (45%)]\tLoss: 0.030927\tWorker: alice\n",
            "Train Epoch: 1 [435200/910592 (48%)]\tLoss: 0.038270\tWorker: alice\n",
            "Train Epoch: 1 [460800/910592 (51%)]\tLoss: 0.078876\tWorker: alice\n",
            "Train Epoch: 1 [486400/910592 (53%)]\tLoss: 0.022233\tWorker: alice\n",
            "Train Epoch: 1 [512000/910592 (56%)]\tLoss: 0.046319\tWorker: alice\n",
            "Train Epoch: 1 [537600/910592 (59%)]\tLoss: 0.028249\tWorker: alice\n",
            "Train Epoch: 1 [563200/910592 (62%)]\tLoss: 0.072067\tWorker: alice\n",
            "Train Epoch: 1 [588800/910592 (65%)]\tLoss: 0.023246\tWorker: alice\n",
            "Train Epoch: 1 [614400/910592 (67%)]\tLoss: 0.154941\tWorker: untrustful\n",
            "Train Epoch: 1 [640000/910592 (70%)]\tLoss: 0.062886\tWorker: untrustful\n",
            "Train Epoch: 1 [665600/910592 (73%)]\tLoss: 0.064940\tWorker: untrustful\n",
            "Train Epoch: 1 [691200/910592 (76%)]\tLoss: 0.043786\tWorker: untrustful\n",
            "Train Epoch: 1 [716800/910592 (79%)]\tLoss: 0.066490\tWorker: untrustful\n",
            "Train Epoch: 1 [742400/910592 (82%)]\tLoss: 0.034747\tWorker: untrustful\n",
            "Train Epoch: 1 [768000/910592 (84%)]\tLoss: 0.046965\tWorker: untrustful\n",
            "Train Epoch: 1 [793600/910592 (87%)]\tLoss: 0.029462\tWorker: untrustful\n",
            "Train Epoch: 1 [819200/910592 (90%)]\tLoss: 0.056408\tWorker: untrustful\n",
            "Train Epoch: 1 [844800/910592 (93%)]\tLoss: 0.084772\tWorker: untrustful\n",
            "Train Epoch: 1 [870400/910592 (96%)]\tLoss: 0.033527\tWorker: untrustful\n",
            "Train Epoch: 1 [896000/910592 (98%)]\tLoss: 0.037543\tWorker: untrustful\n",
            "\n",
            "Test set: Average loss: 0.7162, Accuracy: 191070/390252 (49%)\n",
            "\n",
            "Train Epoch: 2 [0/910592 (0%)]\tLoss: 0.744850\tWorker: bob\n",
            "Train Epoch: 2 [25600/910592 (3%)]\tLoss: 0.071306\tWorker: bob\n",
            "Train Epoch: 2 [51200/910592 (6%)]\tLoss: 0.040158\tWorker: bob\n",
            "Train Epoch: 2 [76800/910592 (8%)]\tLoss: 0.057708\tWorker: bob\n",
            "Train Epoch: 2 [102400/910592 (11%)]\tLoss: 0.043012\tWorker: bob\n",
            "Train Epoch: 2 [128000/910592 (14%)]\tLoss: 0.030836\tWorker: bob\n",
            "Train Epoch: 2 [153600/910592 (17%)]\tLoss: 0.031887\tWorker: bob\n",
            "Train Epoch: 2 [179200/910592 (20%)]\tLoss: 0.021893\tWorker: bob\n",
            "Train Epoch: 2 [204800/910592 (22%)]\tLoss: 0.037809\tWorker: bob\n",
            "Train Epoch: 2 [230400/910592 (25%)]\tLoss: 0.013883\tWorker: bob\n",
            "Train Epoch: 2 [256000/910592 (28%)]\tLoss: 0.038095\tWorker: bob\n",
            "Train Epoch: 2 [281600/910592 (31%)]\tLoss: 0.046794\tWorker: bob\n",
            "Train Epoch: 2 [307200/910592 (34%)]\tLoss: 0.103155\tWorker: alice\n",
            "Train Epoch: 2 [332800/910592 (37%)]\tLoss: 0.044937\tWorker: alice\n",
            "Train Epoch: 2 [358400/910592 (39%)]\tLoss: 0.026088\tWorker: alice\n",
            "Train Epoch: 2 [384000/910592 (42%)]\tLoss: 0.064555\tWorker: alice\n",
            "Train Epoch: 2 [409600/910592 (45%)]\tLoss: 0.048736\tWorker: alice\n",
            "Train Epoch: 2 [435200/910592 (48%)]\tLoss: 0.065312\tWorker: alice\n",
            "Train Epoch: 2 [460800/910592 (51%)]\tLoss: 0.044053\tWorker: alice\n",
            "Train Epoch: 2 [486400/910592 (53%)]\tLoss: 0.040690\tWorker: alice\n",
            "Train Epoch: 2 [512000/910592 (56%)]\tLoss: 0.056182\tWorker: alice\n",
            "Train Epoch: 2 [537600/910592 (59%)]\tLoss: 0.025494\tWorker: alice\n",
            "Train Epoch: 2 [563200/910592 (62%)]\tLoss: 0.011277\tWorker: alice\n",
            "Train Epoch: 2 [588800/910592 (65%)]\tLoss: 0.026892\tWorker: alice\n",
            "Train Epoch: 2 [614400/910592 (67%)]\tLoss: 0.097768\tWorker: untrustful\n",
            "Train Epoch: 2 [640000/910592 (70%)]\tLoss: 0.077266\tWorker: untrustful\n",
            "Train Epoch: 2 [665600/910592 (73%)]\tLoss: 0.037226\tWorker: untrustful\n",
            "Train Epoch: 2 [691200/910592 (76%)]\tLoss: 0.055228\tWorker: untrustful\n",
            "Train Epoch: 2 [716800/910592 (79%)]\tLoss: 0.046541\tWorker: untrustful\n",
            "Train Epoch: 2 [742400/910592 (82%)]\tLoss: 0.030524\tWorker: untrustful\n",
            "Train Epoch: 2 [768000/910592 (84%)]\tLoss: 0.072922\tWorker: untrustful\n",
            "Train Epoch: 2 [793600/910592 (87%)]\tLoss: 0.036258\tWorker: untrustful\n",
            "Train Epoch: 2 [819200/910592 (90%)]\tLoss: 0.043139\tWorker: untrustful\n",
            "Train Epoch: 2 [844800/910592 (93%)]\tLoss: 0.027254\tWorker: untrustful\n",
            "Train Epoch: 2 [870400/910592 (96%)]\tLoss: 0.029602\tWorker: untrustful\n",
            "Train Epoch: 2 [896000/910592 (98%)]\tLoss: 0.035745\tWorker: untrustful\n",
            "\n",
            "Test set: Average loss: 0.2269, Accuracy: 375419/390252 (96%)\n",
            "\n",
            "Train Epoch: 3 [0/910592 (0%)]\tLoss: 0.259033\tWorker: bob\n",
            "Train Epoch: 3 [25600/910592 (3%)]\tLoss: 0.026711\tWorker: bob\n",
            "Train Epoch: 3 [51200/910592 (6%)]\tLoss: 0.045700\tWorker: bob\n",
            "Train Epoch: 3 [76800/910592 (8%)]\tLoss: 0.040921\tWorker: bob\n",
            "Train Epoch: 3 [102400/910592 (11%)]\tLoss: 0.039032\tWorker: bob\n",
            "Train Epoch: 3 [128000/910592 (14%)]\tLoss: 0.043420\tWorker: bob\n",
            "Train Epoch: 3 [153600/910592 (17%)]\tLoss: 0.056364\tWorker: bob\n",
            "Train Epoch: 3 [179200/910592 (20%)]\tLoss: 0.072539\tWorker: bob\n",
            "Train Epoch: 3 [204800/910592 (22%)]\tLoss: 0.027140\tWorker: bob\n",
            "Train Epoch: 3 [230400/910592 (25%)]\tLoss: 0.034967\tWorker: bob\n",
            "Train Epoch: 3 [256000/910592 (28%)]\tLoss: 0.036595\tWorker: bob\n",
            "Train Epoch: 3 [281600/910592 (31%)]\tLoss: 0.020825\tWorker: bob\n",
            "Train Epoch: 3 [307200/910592 (34%)]\tLoss: 0.055324\tWorker: alice\n",
            "Train Epoch: 3 [332800/910592 (37%)]\tLoss: 0.078960\tWorker: alice\n",
            "Train Epoch: 3 [358400/910592 (39%)]\tLoss: 0.036221\tWorker: alice\n",
            "Train Epoch: 3 [384000/910592 (42%)]\tLoss: 0.036227\tWorker: alice\n",
            "Train Epoch: 3 [409600/910592 (45%)]\tLoss: 0.041573\tWorker: alice\n",
            "Train Epoch: 3 [435200/910592 (48%)]\tLoss: 0.048755\tWorker: alice\n",
            "Train Epoch: 3 [460800/910592 (51%)]\tLoss: 0.049918\tWorker: alice\n",
            "Train Epoch: 3 [486400/910592 (53%)]\tLoss: 0.052307\tWorker: alice\n",
            "Train Epoch: 3 [512000/910592 (56%)]\tLoss: 0.027299\tWorker: alice\n",
            "Train Epoch: 3 [537600/910592 (59%)]\tLoss: 0.046113\tWorker: alice\n",
            "Train Epoch: 3 [563200/910592 (62%)]\tLoss: 0.042057\tWorker: alice\n",
            "Train Epoch: 3 [588800/910592 (65%)]\tLoss: 0.018115\tWorker: alice\n",
            "Train Epoch: 3 [614400/910592 (67%)]\tLoss: 0.056159\tWorker: untrustful\n",
            "Train Epoch: 3 [640000/910592 (70%)]\tLoss: 0.047547\tWorker: untrustful\n",
            "Train Epoch: 3 [665600/910592 (73%)]\tLoss: 0.047662\tWorker: untrustful\n",
            "Train Epoch: 3 [691200/910592 (76%)]\tLoss: 0.051528\tWorker: untrustful\n",
            "Train Epoch: 3 [716800/910592 (79%)]\tLoss: 0.032743\tWorker: untrustful\n",
            "Train Epoch: 3 [742400/910592 (82%)]\tLoss: 0.070202\tWorker: untrustful\n",
            "Train Epoch: 3 [768000/910592 (84%)]\tLoss: 0.018848\tWorker: untrustful\n",
            "Train Epoch: 3 [793600/910592 (87%)]\tLoss: 0.032581\tWorker: untrustful\n",
            "Train Epoch: 3 [819200/910592 (90%)]\tLoss: 0.052283\tWorker: untrustful\n",
            "Train Epoch: 3 [844800/910592 (93%)]\tLoss: 0.028629\tWorker: untrustful\n",
            "Train Epoch: 3 [870400/910592 (96%)]\tLoss: 0.039492\tWorker: untrustful\n",
            "Train Epoch: 3 [896000/910592 (98%)]\tLoss: 0.048475\tWorker: untrustful\n",
            "\n",
            "Test set: Average loss: 0.2229, Accuracy: 380003/390252 (97%)\n",
            "\n",
            "Train Epoch: 4 [0/910592 (0%)]\tLoss: 0.212317\tWorker: bob\n",
            "Train Epoch: 4 [25600/910592 (3%)]\tLoss: 0.024978\tWorker: bob\n",
            "Train Epoch: 4 [51200/910592 (6%)]\tLoss: 0.059673\tWorker: bob\n",
            "Train Epoch: 4 [76800/910592 (8%)]\tLoss: 0.038069\tWorker: bob\n",
            "Train Epoch: 4 [102400/910592 (11%)]\tLoss: 0.032846\tWorker: bob\n",
            "Train Epoch: 4 [128000/910592 (14%)]\tLoss: 0.016306\tWorker: bob\n",
            "Train Epoch: 4 [153600/910592 (17%)]\tLoss: 0.046582\tWorker: bob\n",
            "Train Epoch: 4 [179200/910592 (20%)]\tLoss: 0.039914\tWorker: bob\n",
            "Train Epoch: 4 [204800/910592 (22%)]\tLoss: 0.042224\tWorker: bob\n",
            "Train Epoch: 4 [230400/910592 (25%)]\tLoss: 0.037755\tWorker: bob\n",
            "Train Epoch: 4 [256000/910592 (28%)]\tLoss: 0.038716\tWorker: bob\n",
            "Train Epoch: 4 [281600/910592 (31%)]\tLoss: 0.033961\tWorker: bob\n",
            "Train Epoch: 4 [307200/910592 (34%)]\tLoss: 0.044924\tWorker: alice\n",
            "Train Epoch: 4 [332800/910592 (37%)]\tLoss: 0.034857\tWorker: alice\n",
            "Train Epoch: 4 [358400/910592 (39%)]\tLoss: 0.027235\tWorker: alice\n",
            "Train Epoch: 4 [384000/910592 (42%)]\tLoss: 0.023945\tWorker: alice\n",
            "Train Epoch: 4 [409600/910592 (45%)]\tLoss: 0.026300\tWorker: alice\n",
            "Train Epoch: 4 [435200/910592 (48%)]\tLoss: 0.041034\tWorker: alice\n",
            "Train Epoch: 4 [460800/910592 (51%)]\tLoss: 0.030640\tWorker: alice\n",
            "Train Epoch: 4 [486400/910592 (53%)]\tLoss: 0.048972\tWorker: alice\n",
            "Train Epoch: 4 [512000/910592 (56%)]\tLoss: 0.034811\tWorker: alice\n",
            "Train Epoch: 4 [537600/910592 (59%)]\tLoss: 0.028963\tWorker: alice\n",
            "Train Epoch: 4 [563200/910592 (62%)]\tLoss: 0.034313\tWorker: alice\n",
            "Train Epoch: 4 [588800/910592 (65%)]\tLoss: 0.045385\tWorker: alice\n",
            "Train Epoch: 4 [614400/910592 (67%)]\tLoss: 0.051380\tWorker: untrustful\n",
            "Train Epoch: 4 [640000/910592 (70%)]\tLoss: 0.016503\tWorker: untrustful\n",
            "Train Epoch: 4 [665600/910592 (73%)]\tLoss: 0.035604\tWorker: untrustful\n",
            "Train Epoch: 4 [691200/910592 (76%)]\tLoss: 0.042736\tWorker: untrustful\n",
            "Train Epoch: 4 [716800/910592 (79%)]\tLoss: 0.048849\tWorker: untrustful\n",
            "Train Epoch: 4 [742400/910592 (82%)]\tLoss: 0.046743\tWorker: untrustful\n",
            "Train Epoch: 4 [768000/910592 (84%)]\tLoss: 0.032579\tWorker: untrustful\n",
            "Train Epoch: 4 [793600/910592 (87%)]\tLoss: 0.056427\tWorker: untrustful\n",
            "Train Epoch: 4 [819200/910592 (90%)]\tLoss: 0.022954\tWorker: untrustful\n",
            "Train Epoch: 4 [844800/910592 (93%)]\tLoss: 0.036208\tWorker: untrustful\n",
            "Train Epoch: 4 [870400/910592 (96%)]\tLoss: 0.033997\tWorker: untrustful\n",
            "Train Epoch: 4 [896000/910592 (98%)]\tLoss: 0.036168\tWorker: untrustful\n",
            "\n",
            "Test set: Average loss: 0.2611, Accuracy: 377319/390252 (97%)\n",
            "\n",
            "Train Epoch: 5 [0/910592 (0%)]\tLoss: 0.270338\tWorker: bob\n",
            "Train Epoch: 5 [25600/910592 (3%)]\tLoss: 0.022723\tWorker: bob\n",
            "Train Epoch: 5 [51200/910592 (6%)]\tLoss: 0.041824\tWorker: bob\n",
            "Train Epoch: 5 [76800/910592 (8%)]\tLoss: 0.034549\tWorker: bob\n",
            "Train Epoch: 5 [102400/910592 (11%)]\tLoss: 0.015578\tWorker: bob\n",
            "Train Epoch: 5 [128000/910592 (14%)]\tLoss: 0.046094\tWorker: bob\n",
            "Train Epoch: 5 [153600/910592 (17%)]\tLoss: 0.021636\tWorker: bob\n",
            "Train Epoch: 5 [179200/910592 (20%)]\tLoss: 0.037915\tWorker: bob\n",
            "Train Epoch: 5 [204800/910592 (22%)]\tLoss: 0.034200\tWorker: bob\n",
            "Train Epoch: 5 [230400/910592 (25%)]\tLoss: 0.059644\tWorker: bob\n",
            "Train Epoch: 5 [256000/910592 (28%)]\tLoss: 0.020101\tWorker: bob\n",
            "Train Epoch: 5 [281600/910592 (31%)]\tLoss: 0.009083\tWorker: bob\n",
            "Train Epoch: 5 [307200/910592 (34%)]\tLoss: 0.059036\tWorker: alice\n",
            "Train Epoch: 5 [332800/910592 (37%)]\tLoss: 0.051627\tWorker: alice\n",
            "Train Epoch: 5 [358400/910592 (39%)]\tLoss: 0.037214\tWorker: alice\n",
            "Train Epoch: 5 [384000/910592 (42%)]\tLoss: 0.023226\tWorker: alice\n",
            "Train Epoch: 5 [409600/910592 (45%)]\tLoss: 0.034193\tWorker: alice\n",
            "Train Epoch: 5 [435200/910592 (48%)]\tLoss: 0.024455\tWorker: alice\n",
            "Train Epoch: 5 [460800/910592 (51%)]\tLoss: 0.021474\tWorker: alice\n",
            "Train Epoch: 5 [486400/910592 (53%)]\tLoss: 0.029690\tWorker: alice\n",
            "Train Epoch: 5 [512000/910592 (56%)]\tLoss: 0.026354\tWorker: alice\n",
            "Train Epoch: 5 [537600/910592 (59%)]\tLoss: 0.019070\tWorker: alice\n",
            "Train Epoch: 5 [563200/910592 (62%)]\tLoss: 0.030538\tWorker: alice\n",
            "Train Epoch: 5 [588800/910592 (65%)]\tLoss: 0.026005\tWorker: alice\n",
            "Train Epoch: 5 [614400/910592 (67%)]\tLoss: 0.048683\tWorker: untrustful\n",
            "Train Epoch: 5 [640000/910592 (70%)]\tLoss: 0.041500\tWorker: untrustful\n",
            "Train Epoch: 5 [665600/910592 (73%)]\tLoss: 0.044010\tWorker: untrustful\n",
            "Train Epoch: 5 [691200/910592 (76%)]\tLoss: 0.024222\tWorker: untrustful\n",
            "Train Epoch: 5 [716800/910592 (79%)]\tLoss: 0.035477\tWorker: untrustful\n",
            "Train Epoch: 5 [742400/910592 (82%)]\tLoss: 0.064472\tWorker: untrustful\n",
            "Train Epoch: 5 [768000/910592 (84%)]\tLoss: 0.041656\tWorker: untrustful\n",
            "Train Epoch: 5 [793600/910592 (87%)]\tLoss: 0.025046\tWorker: untrustful\n",
            "Train Epoch: 5 [819200/910592 (90%)]\tLoss: 0.019981\tWorker: untrustful\n",
            "Train Epoch: 5 [844800/910592 (93%)]\tLoss: 0.016049\tWorker: untrustful\n",
            "Train Epoch: 5 [870400/910592 (96%)]\tLoss: 0.016148\tWorker: untrustful\n",
            "Train Epoch: 5 [896000/910592 (98%)]\tLoss: 0.051844\tWorker: untrustful\n",
            "\n",
            "Test set: Average loss: 0.3599, Accuracy: 308265/390252 (79%)\n",
            "\n",
            "Train Epoch: 6 [0/910592 (0%)]\tLoss: 0.408094\tWorker: bob\n",
            "Train Epoch: 6 [25600/910592 (3%)]\tLoss: 0.022261\tWorker: bob\n",
            "Train Epoch: 6 [51200/910592 (6%)]\tLoss: 0.021185\tWorker: bob\n",
            "Train Epoch: 6 [76800/910592 (8%)]\tLoss: 0.028752\tWorker: bob\n",
            "Train Epoch: 6 [102400/910592 (11%)]\tLoss: 0.013126\tWorker: bob\n",
            "Train Epoch: 6 [128000/910592 (14%)]\tLoss: 0.012407\tWorker: bob\n",
            "Train Epoch: 6 [153600/910592 (17%)]\tLoss: 0.025008\tWorker: bob\n",
            "Train Epoch: 6 [179200/910592 (20%)]\tLoss: 0.017523\tWorker: bob\n",
            "Train Epoch: 6 [204800/910592 (22%)]\tLoss: 0.026510\tWorker: bob\n",
            "Train Epoch: 6 [230400/910592 (25%)]\tLoss: 0.007477\tWorker: bob\n",
            "Train Epoch: 6 [256000/910592 (28%)]\tLoss: 0.035208\tWorker: bob\n",
            "Train Epoch: 6 [281600/910592 (31%)]\tLoss: 0.015049\tWorker: bob\n",
            "Train Epoch: 6 [307200/910592 (34%)]\tLoss: 0.042485\tWorker: alice\n",
            "Train Epoch: 6 [332800/910592 (37%)]\tLoss: 0.028406\tWorker: alice\n",
            "Train Epoch: 6 [358400/910592 (39%)]\tLoss: 0.025744\tWorker: alice\n",
            "Train Epoch: 6 [384000/910592 (42%)]\tLoss: 0.039627\tWorker: alice\n",
            "Train Epoch: 6 [409600/910592 (45%)]\tLoss: 0.012205\tWorker: alice\n",
            "Train Epoch: 6 [435200/910592 (48%)]\tLoss: 0.006467\tWorker: alice\n",
            "Train Epoch: 6 [460800/910592 (51%)]\tLoss: 0.018958\tWorker: alice\n",
            "Train Epoch: 6 [486400/910592 (53%)]\tLoss: 0.029505\tWorker: alice\n",
            "Train Epoch: 6 [512000/910592 (56%)]\tLoss: 0.014105\tWorker: alice\n",
            "Train Epoch: 6 [537600/910592 (59%)]\tLoss: 0.025538\tWorker: alice\n",
            "Train Epoch: 6 [563200/910592 (62%)]\tLoss: 0.014861\tWorker: alice\n",
            "Train Epoch: 6 [588800/910592 (65%)]\tLoss: 0.016559\tWorker: alice\n",
            "Train Epoch: 6 [614400/910592 (67%)]\tLoss: 0.032533\tWorker: untrustful\n",
            "Train Epoch: 6 [640000/910592 (70%)]\tLoss: 0.045392\tWorker: untrustful\n",
            "Train Epoch: 6 [665600/910592 (73%)]\tLoss: 0.031564\tWorker: untrustful\n",
            "Train Epoch: 6 [691200/910592 (76%)]\tLoss: 0.016984\tWorker: untrustful\n",
            "Train Epoch: 6 [716800/910592 (79%)]\tLoss: 0.025458\tWorker: untrustful\n",
            "Train Epoch: 6 [742400/910592 (82%)]\tLoss: 0.017019\tWorker: untrustful\n",
            "Train Epoch: 6 [768000/910592 (84%)]\tLoss: 0.029142\tWorker: untrustful\n",
            "Train Epoch: 6 [793600/910592 (87%)]\tLoss: 0.016850\tWorker: untrustful\n",
            "Train Epoch: 6 [819200/910592 (90%)]\tLoss: 0.044372\tWorker: untrustful\n",
            "Train Epoch: 6 [844800/910592 (93%)]\tLoss: 0.026458\tWorker: untrustful\n",
            "Train Epoch: 6 [870400/910592 (96%)]\tLoss: 0.052582\tWorker: untrustful\n",
            "Train Epoch: 6 [896000/910592 (98%)]\tLoss: 0.057151\tWorker: untrustful\n",
            "\n",
            "Test set: Average loss: 0.4762, Accuracy: 294724/390252 (76%)\n",
            "\n",
            "Train Epoch: 7 [0/910592 (0%)]\tLoss: 0.618609\tWorker: bob\n",
            "Train Epoch: 7 [25600/910592 (3%)]\tLoss: 0.027464\tWorker: bob\n",
            "Train Epoch: 7 [51200/910592 (6%)]\tLoss: 0.017854\tWorker: bob\n",
            "Train Epoch: 7 [76800/910592 (8%)]\tLoss: 0.014844\tWorker: bob\n",
            "Train Epoch: 7 [102400/910592 (11%)]\tLoss: 0.004070\tWorker: bob\n",
            "Train Epoch: 7 [128000/910592 (14%)]\tLoss: 0.030859\tWorker: bob\n",
            "Train Epoch: 7 [153600/910592 (17%)]\tLoss: 0.007533\tWorker: bob\n",
            "Train Epoch: 7 [179200/910592 (20%)]\tLoss: 0.010243\tWorker: bob\n",
            "Train Epoch: 7 [204800/910592 (22%)]\tLoss: 0.053694\tWorker: bob\n",
            "Train Epoch: 7 [230400/910592 (25%)]\tLoss: 0.012103\tWorker: bob\n",
            "Train Epoch: 7 [256000/910592 (28%)]\tLoss: 0.045268\tWorker: bob\n",
            "Train Epoch: 7 [281600/910592 (31%)]\tLoss: 0.006118\tWorker: bob\n",
            "Train Epoch: 7 [307200/910592 (34%)]\tLoss: 0.028755\tWorker: alice\n",
            "Train Epoch: 7 [332800/910592 (37%)]\tLoss: 0.014724\tWorker: alice\n",
            "Train Epoch: 7 [358400/910592 (39%)]\tLoss: 0.012021\tWorker: alice\n",
            "Train Epoch: 7 [384000/910592 (42%)]\tLoss: 0.005299\tWorker: alice\n",
            "Train Epoch: 7 [409600/910592 (45%)]\tLoss: 0.006316\tWorker: alice\n",
            "Train Epoch: 7 [435200/910592 (48%)]\tLoss: 0.016267\tWorker: alice\n",
            "Train Epoch: 7 [460800/910592 (51%)]\tLoss: 0.052808\tWorker: alice\n",
            "Train Epoch: 7 [486400/910592 (53%)]\tLoss: 0.022224\tWorker: alice\n",
            "Train Epoch: 7 [512000/910592 (56%)]\tLoss: 0.006077\tWorker: alice\n",
            "Train Epoch: 7 [537600/910592 (59%)]\tLoss: 0.012095\tWorker: alice\n",
            "Train Epoch: 7 [563200/910592 (62%)]\tLoss: 0.010420\tWorker: alice\n",
            "Train Epoch: 7 [588800/910592 (65%)]\tLoss: 0.013804\tWorker: alice\n",
            "Train Epoch: 7 [614400/910592 (67%)]\tLoss: 0.029410\tWorker: untrustful\n",
            "Train Epoch: 7 [640000/910592 (70%)]\tLoss: 0.043235\tWorker: untrustful\n",
            "Train Epoch: 7 [665600/910592 (73%)]\tLoss: 0.027205\tWorker: untrustful\n",
            "Train Epoch: 7 [691200/910592 (76%)]\tLoss: 0.040888\tWorker: untrustful\n",
            "Train Epoch: 7 [716800/910592 (79%)]\tLoss: 0.013155\tWorker: untrustful\n",
            "Train Epoch: 7 [742400/910592 (82%)]\tLoss: 0.023040\tWorker: untrustful\n",
            "Train Epoch: 7 [768000/910592 (84%)]\tLoss: 0.012474\tWorker: untrustful\n",
            "Train Epoch: 7 [793600/910592 (87%)]\tLoss: 0.031027\tWorker: untrustful\n",
            "Train Epoch: 7 [819200/910592 (90%)]\tLoss: 0.019845\tWorker: untrustful\n",
            "Train Epoch: 7 [844800/910592 (93%)]\tLoss: 0.024488\tWorker: untrustful\n",
            "Train Epoch: 7 [870400/910592 (96%)]\tLoss: 0.014646\tWorker: untrustful\n",
            "Train Epoch: 7 [896000/910592 (98%)]\tLoss: 0.016762\tWorker: untrustful\n",
            "\n",
            "Test set: Average loss: 0.4342, Accuracy: 305649/390252 (78%)\n",
            "\n",
            "Train Epoch: 8 [0/910592 (0%)]\tLoss: 0.371001\tWorker: bob\n",
            "Train Epoch: 8 [25600/910592 (3%)]\tLoss: 0.036612\tWorker: bob\n",
            "Train Epoch: 8 [51200/910592 (6%)]\tLoss: 0.031008\tWorker: bob\n",
            "Train Epoch: 8 [76800/910592 (8%)]\tLoss: 0.011822\tWorker: bob\n",
            "Train Epoch: 8 [102400/910592 (11%)]\tLoss: 0.003260\tWorker: bob\n",
            "Train Epoch: 8 [128000/910592 (14%)]\tLoss: 0.032486\tWorker: bob\n",
            "Train Epoch: 8 [153600/910592 (17%)]\tLoss: 0.007020\tWorker: bob\n",
            "Train Epoch: 8 [179200/910592 (20%)]\tLoss: 0.002022\tWorker: bob\n",
            "Train Epoch: 8 [204800/910592 (22%)]\tLoss: 0.015796\tWorker: bob\n",
            "Train Epoch: 8 [230400/910592 (25%)]\tLoss: 0.026430\tWorker: bob\n",
            "Train Epoch: 8 [256000/910592 (28%)]\tLoss: 0.011465\tWorker: bob\n",
            "Train Epoch: 8 [281600/910592 (31%)]\tLoss: 0.003789\tWorker: bob\n",
            "Train Epoch: 8 [307200/910592 (34%)]\tLoss: 0.023425\tWorker: alice\n",
            "Train Epoch: 8 [332800/910592 (37%)]\tLoss: 0.024379\tWorker: alice\n",
            "Train Epoch: 8 [358400/910592 (39%)]\tLoss: 0.014531\tWorker: alice\n",
            "Train Epoch: 8 [384000/910592 (42%)]\tLoss: 0.017893\tWorker: alice\n",
            "Train Epoch: 8 [409600/910592 (45%)]\tLoss: 0.017981\tWorker: alice\n",
            "Train Epoch: 8 [435200/910592 (48%)]\tLoss: 0.032251\tWorker: alice\n",
            "Train Epoch: 8 [460800/910592 (51%)]\tLoss: 0.014500\tWorker: alice\n",
            "Train Epoch: 8 [486400/910592 (53%)]\tLoss: 0.020360\tWorker: alice\n",
            "Train Epoch: 8 [512000/910592 (56%)]\tLoss: 0.017808\tWorker: alice\n",
            "Train Epoch: 8 [537600/910592 (59%)]\tLoss: 0.024718\tWorker: alice\n",
            "Train Epoch: 8 [563200/910592 (62%)]\tLoss: 0.016342\tWorker: alice\n",
            "Train Epoch: 8 [588800/910592 (65%)]\tLoss: 0.011531\tWorker: alice\n",
            "Train Epoch: 8 [614400/910592 (67%)]\tLoss: 0.014467\tWorker: untrustful\n",
            "Train Epoch: 8 [640000/910592 (70%)]\tLoss: 0.019083\tWorker: untrustful\n",
            "Train Epoch: 8 [665600/910592 (73%)]\tLoss: 0.016909\tWorker: untrustful\n",
            "Train Epoch: 8 [691200/910592 (76%)]\tLoss: 0.036829\tWorker: untrustful\n",
            "Train Epoch: 8 [716800/910592 (79%)]\tLoss: 0.024216\tWorker: untrustful\n",
            "Train Epoch: 8 [742400/910592 (82%)]\tLoss: 0.071568\tWorker: untrustful\n",
            "Train Epoch: 8 [768000/910592 (84%)]\tLoss: 0.012668\tWorker: untrustful\n",
            "Train Epoch: 8 [793600/910592 (87%)]\tLoss: 0.013889\tWorker: untrustful\n",
            "Train Epoch: 8 [819200/910592 (90%)]\tLoss: 0.024254\tWorker: untrustful\n",
            "Train Epoch: 8 [844800/910592 (93%)]\tLoss: 0.024422\tWorker: untrustful\n",
            "Train Epoch: 8 [870400/910592 (96%)]\tLoss: 0.035890\tWorker: untrustful\n",
            "Train Epoch: 8 [896000/910592 (98%)]\tLoss: 0.020246\tWorker: untrustful\n",
            "\n",
            "Test set: Average loss: 0.7058, Accuracy: 291269/390252 (75%)\n",
            "\n",
            "Train Epoch: 9 [0/910592 (0%)]\tLoss: 0.591564\tWorker: bob\n",
            "Train Epoch: 9 [25600/910592 (3%)]\tLoss: 0.011929\tWorker: bob\n",
            "Train Epoch: 9 [51200/910592 (6%)]\tLoss: 0.008440\tWorker: bob\n",
            "Train Epoch: 9 [76800/910592 (8%)]\tLoss: 0.011243\tWorker: bob\n",
            "Train Epoch: 9 [102400/910592 (11%)]\tLoss: 0.012868\tWorker: bob\n",
            "Train Epoch: 9 [128000/910592 (14%)]\tLoss: 0.002331\tWorker: bob\n",
            "Train Epoch: 9 [153600/910592 (17%)]\tLoss: 0.022468\tWorker: bob\n",
            "Train Epoch: 9 [179200/910592 (20%)]\tLoss: 0.024245\tWorker: bob\n",
            "Train Epoch: 9 [204800/910592 (22%)]\tLoss: 0.008595\tWorker: bob\n",
            "Train Epoch: 9 [230400/910592 (25%)]\tLoss: 0.002812\tWorker: bob\n",
            "Train Epoch: 9 [256000/910592 (28%)]\tLoss: 0.016453\tWorker: bob\n",
            "Train Epoch: 9 [281600/910592 (31%)]\tLoss: 0.006933\tWorker: bob\n",
            "Train Epoch: 9 [307200/910592 (34%)]\tLoss: 0.018855\tWorker: alice\n",
            "Train Epoch: 9 [332800/910592 (37%)]\tLoss: 0.255894\tWorker: alice\n",
            "Train Epoch: 9 [358400/910592 (39%)]\tLoss: 0.011465\tWorker: alice\n",
            "Train Epoch: 9 [384000/910592 (42%)]\tLoss: 0.016433\tWorker: alice\n",
            "Train Epoch: 9 [409600/910592 (45%)]\tLoss: 0.011550\tWorker: alice\n",
            "Train Epoch: 9 [435200/910592 (48%)]\tLoss: 0.001958\tWorker: alice\n",
            "Train Epoch: 9 [460800/910592 (51%)]\tLoss: 0.005177\tWorker: alice\n",
            "Train Epoch: 9 [486400/910592 (53%)]\tLoss: 0.007927\tWorker: alice\n",
            "Train Epoch: 9 [512000/910592 (56%)]\tLoss: 0.007340\tWorker: alice\n",
            "Train Epoch: 9 [537600/910592 (59%)]\tLoss: 0.006536\tWorker: alice\n",
            "Train Epoch: 9 [563200/910592 (62%)]\tLoss: 0.005522\tWorker: alice\n",
            "Train Epoch: 9 [588800/910592 (65%)]\tLoss: 0.006877\tWorker: alice\n",
            "Train Epoch: 9 [614400/910592 (67%)]\tLoss: 0.030794\tWorker: untrustful\n",
            "Train Epoch: 9 [640000/910592 (70%)]\tLoss: 0.015698\tWorker: untrustful\n",
            "Train Epoch: 9 [665600/910592 (73%)]\tLoss: 0.011469\tWorker: untrustful\n",
            "Train Epoch: 9 [691200/910592 (76%)]\tLoss: 0.018802\tWorker: untrustful\n",
            "Train Epoch: 9 [716800/910592 (79%)]\tLoss: 0.007469\tWorker: untrustful\n",
            "Train Epoch: 9 [742400/910592 (82%)]\tLoss: 0.033588\tWorker: untrustful\n",
            "Train Epoch: 9 [768000/910592 (84%)]\tLoss: 0.013880\tWorker: untrustful\n",
            "Train Epoch: 9 [793600/910592 (87%)]\tLoss: 0.006286\tWorker: untrustful\n",
            "Train Epoch: 9 [819200/910592 (90%)]\tLoss: 0.004914\tWorker: untrustful\n",
            "Train Epoch: 9 [844800/910592 (93%)]\tLoss: 0.011315\tWorker: untrustful\n",
            "Train Epoch: 9 [870400/910592 (96%)]\tLoss: 0.006477\tWorker: untrustful\n",
            "Train Epoch: 9 [896000/910592 (98%)]\tLoss: 0.015696\tWorker: untrustful\n",
            "\n",
            "Test set: Average loss: 0.9808, Accuracy: 279224/390252 (72%)\n",
            "\n",
            "Train Epoch: 10 [0/910592 (0%)]\tLoss: 1.229912\tWorker: bob\n",
            "Train Epoch: 10 [25600/910592 (3%)]\tLoss: 0.011338\tWorker: bob\n",
            "Train Epoch: 10 [51200/910592 (6%)]\tLoss: 0.002703\tWorker: bob\n",
            "Train Epoch: 10 [76800/910592 (8%)]\tLoss: 0.006452\tWorker: bob\n",
            "Train Epoch: 10 [102400/910592 (11%)]\tLoss: 0.002347\tWorker: bob\n",
            "Train Epoch: 10 [128000/910592 (14%)]\tLoss: 0.008814\tWorker: bob\n",
            "Train Epoch: 10 [153600/910592 (17%)]\tLoss: 0.004720\tWorker: bob\n",
            "Train Epoch: 10 [179200/910592 (20%)]\tLoss: 0.077748\tWorker: bob\n",
            "Train Epoch: 10 [204800/910592 (22%)]\tLoss: 0.026101\tWorker: bob\n",
            "Train Epoch: 10 [230400/910592 (25%)]\tLoss: 0.013640\tWorker: bob\n",
            "Train Epoch: 10 [256000/910592 (28%)]\tLoss: 0.002658\tWorker: bob\n",
            "Train Epoch: 10 [281600/910592 (31%)]\tLoss: 0.027294\tWorker: bob\n",
            "Train Epoch: 10 [307200/910592 (34%)]\tLoss: 0.035302\tWorker: alice\n",
            "Train Epoch: 10 [332800/910592 (37%)]\tLoss: 0.012137\tWorker: alice\n",
            "Train Epoch: 10 [358400/910592 (39%)]\tLoss: 0.005838\tWorker: alice\n",
            "Train Epoch: 10 [384000/910592 (42%)]\tLoss: 0.001395\tWorker: alice\n",
            "Train Epoch: 10 [409600/910592 (45%)]\tLoss: 0.014204\tWorker: alice\n",
            "Train Epoch: 10 [435200/910592 (48%)]\tLoss: 0.011814\tWorker: alice\n",
            "Train Epoch: 10 [460800/910592 (51%)]\tLoss: 0.027217\tWorker: alice\n",
            "Train Epoch: 10 [486400/910592 (53%)]\tLoss: 0.018228\tWorker: alice\n",
            "Train Epoch: 10 [512000/910592 (56%)]\tLoss: 0.002559\tWorker: alice\n",
            "Train Epoch: 10 [537600/910592 (59%)]\tLoss: 0.005752\tWorker: alice\n",
            "Train Epoch: 10 [563200/910592 (62%)]\tLoss: 0.018014\tWorker: alice\n",
            "Train Epoch: 10 [588800/910592 (65%)]\tLoss: 0.015661\tWorker: alice\n",
            "Train Epoch: 10 [614400/910592 (67%)]\tLoss: 0.079495\tWorker: untrustful\n",
            "Train Epoch: 10 [640000/910592 (70%)]\tLoss: 0.038232\tWorker: untrustful\n",
            "Train Epoch: 10 [665600/910592 (73%)]\tLoss: 0.005628\tWorker: untrustful\n",
            "Train Epoch: 10 [691200/910592 (76%)]\tLoss: 0.020405\tWorker: untrustful\n",
            "Train Epoch: 10 [716800/910592 (79%)]\tLoss: 0.010764\tWorker: untrustful\n",
            "Train Epoch: 10 [742400/910592 (82%)]\tLoss: 0.013482\tWorker: untrustful\n",
            "Train Epoch: 10 [768000/910592 (84%)]\tLoss: 0.021642\tWorker: untrustful\n",
            "Train Epoch: 10 [793600/910592 (87%)]\tLoss: 0.006472\tWorker: untrustful\n",
            "Train Epoch: 10 [819200/910592 (90%)]\tLoss: 0.006320\tWorker: untrustful\n",
            "Train Epoch: 10 [844800/910592 (93%)]\tLoss: 0.011987\tWorker: untrustful\n",
            "Train Epoch: 10 [870400/910592 (96%)]\tLoss: 0.003509\tWorker: untrustful\n",
            "Train Epoch: 10 [896000/910592 (98%)]\tLoss: 0.027544\tWorker: untrustful\n",
            "\n",
            "Test set: Average loss: 0.9856, Accuracy: 288053/390252 (74%)\n",
            "\n"
          ]
        }
      ],
      "source": [
        "# results of training with an untrustful client same as above, but with simple FedAvg\n",
        "\n",
        "# central model\n",
        "central_model = Net().to(device)\n",
        "\n",
        "# clients' models, optimizers and schedulers for learning rate\n",
        "models = {i:Net().to(device) for i in clients}\n",
        "optimizers = {i:optim.SGD(models[i].parameters(), lr=args['lr']) for i in clients}\n",
        "\n",
        "# initialization of dictionary for models aggregation\n",
        "weights = {'hidden_mean_weight' : torch.zeros(size=central_model.layers[0].weight.shape).to(device),\n",
        "           'hidden_mean_bias' : torch.zeros(size=central_model.layers[0].bias.shape).to(device),\n",
        "           'output_mean_weight' : torch.zeros(size=central_model.layers[2].weight.shape).to(device),\n",
        "           'output_mean_bias' : torch.zeros(size=central_model.layers[2].bias.shape).to(device)}\n",
        "\n",
        "# trust values\n",
        "trust = {i:1 for i in clients}\n",
        "\n",
        "for epoch in range(1, args['epochs'] + 1):\n",
        "    # below function is modified to simulate untrustful behavior of a client\n",
        "    train_locally(args, models, device, federated_train_loader, optimizers, epoch)\n",
        "    # we also shift the weights of the untrustful client to have more influence on the aggregated model\n",
        "    models[clients[2]].layers[0].weight.data *= 1.5\n",
        "    models[clients[2]].layers[2].weight.data *= 1.5\n",
        "    aggregate(central_model, models, weights, trust)\n",
        "    test(central_model, device, test_loader)"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "collapsed_sections": [
        "bgSLI4kdwT_r",
        "wv6y7T3jwbDq",
        "5x-VNSm7PjMa",
        "PkbCZpmfLQ7M",
        "Lu1jzpb05zXC"
      ],
      "machine_shape": "hm",
      "name": "DNS_traffic_Trusted.ipynb",
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
