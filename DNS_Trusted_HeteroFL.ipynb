{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "ej63Q9F5VDWy"
   },
   "source": [
    "Federated Learning with heterogeneous clients using PyTorch and PySyft with Trusted FedAvg on DNS traffic datasets.\n",
    "\n",
    "Trusted FedAvg paper: https://arxiv.org/pdf/2104.07853.pdf\n",
    "\n",
    "Heterogeneous clients paper: https://arxiv.org/pdf/2010.01264.pdf"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "bgSLI4kdwT_r"
   },
   "source": [
    "## Add libraries, define FL clients"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "p9Z-tl_jWgeE"
   },
   "outputs": [],
   "source": [
    "!pip install syft==0.2.9"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "eVP2vWh8pzsS"
   },
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import torch.optim.lr_scheduler as sched\n",
    "from torch.nn import BCELoss\n",
    "import torch.utils.data as tud\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from statistics import median\n",
    "import syft as sy\n",
    "\n",
    "# hook PyTorch to PySyft, i.e. add extra functionalities to support Federated Learning and other private AI tools\n",
    "hook = sy.TorchHook(torch)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "R515eomFzXMl"
   },
   "outputs": [],
   "source": [
    "# create clients\n",
    "clients = []\n",
    "for i in range(0, 21):\n",
    "    clients.append(sy.VirtualWorker(hook, id='client'+str(i+1)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "i7GnUhGXwYM3"
   },
   "source": [
    "## Load data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "qHlQttzvcOiI"
   },
   "outputs": [],
   "source": [
    "# load clients' datasets and test sets\n",
    "clients_datasets = []\n",
    "testsets = []\n",
    "for i in range(0, 21):\n",
    "    clients_datasets.append(pd.read_csv('client' + str(i+1) + '.csv').astype('float32'))\n",
    "\n",
    "for n in [7, 14, 21]:\n",
    "    testsets.append(pd.read_csv('test' + str(n) + '.csv').astype('float32'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "hHNAbnLA14Fa"
   },
   "outputs": [],
   "source": [
    "# transform to tensors\n",
    "features_train = [torch.tensor(cd.iloc[:,:-1].to_numpy()) for cd in clients_datasets]\n",
    "target_train = [torch.tensor(cd['target'].to_numpy()) for cd in clients_datasets]\n",
    "\n",
    "features_test = [torch.tensor(t.iloc[:,:-1].to_numpy()) for t in testsets]\n",
    "target_test = [torch.tensor(t['target'].to_numpy()) for t in testsets]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "wv6y7T3jwbDq"
   },
   "source": [
    "## Define training parameters and models, send data to clients"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "fd5-982uY-6L"
   },
   "outputs": [],
   "source": [
    "# define the args\n",
    "args = {\n",
    "    'use_cuda' : True,\n",
    "    'batch_size' : 128,\n",
    "    'test_batch_size' : 1000,\n",
    "    'lr' : 0.001,\n",
    "    'log_interval' : 200,\n",
    "    'epochs' : 7\n",
    "}\n",
    "\n",
    "# check to use GPU or not\n",
    "use_cuda = args['use_cuda'] and torch.cuda.is_available()\n",
    "device = torch.device(\"cuda\" if use_cuda else \"cpu\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "VShbMriNZC6I"
   },
   "outputs": [],
   "source": [
    "# create a simple feedforward network\n",
    "# n features as input, 2*n+1 hidden layer neurons, 1 output for binary classification\n",
    "class MLP(nn.Module):\n",
    "    \n",
    "    def __init__(self, n):\n",
    "        super(MLP, self).__init__()\n",
    "        self.n = n\n",
    "        \n",
    "        self.layers = nn.Sequential(\n",
    "            nn.Linear(in_features=n, out_features=2*n+1),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(in_features=2*n+1, out_features=1),\n",
    "            nn.Sigmoid()\n",
    "        )\n",
    "            \n",
    "    def forward(self, x):\n",
    "        return self.layers(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "_McSsgBEZq1f"
   },
   "outputs": [],
   "source": [
    "# distribute data across workers\n",
    "# normally there is no need to distribute data, since it is already at the clients\n",
    "# this is more of a simulation of federated learning\n",
    "train_datasets = [sy.BaseDataset(features_train[i].send(clients[i]), target_train[i].send(clients[i])) for i in range(len(clients))]\n",
    "federated_datasets = [sy.FederatedDataset(train_datasets[:n]) for n in [7, 14, 21]]\n",
    "federated_train_loaders = [sy.FederatedDataLoader(fd, batch_size=args['batch_size'], shuffle=True) for fd in federated_datasets]\n",
    "\n",
    "# test data remains at the central entity\n",
    "test_datasets = [tud.TensorDataset(features_test[i], target_test[i]) for i in range(len(testsets))]\n",
    "test_loaders = [tud.DataLoader(td, batch_size=args['test_batch_size'], shuffle=True) for td in test_datasets]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "5x-VNSm7PjMa"
   },
   "source": [
    "## Train, test, aggregation, trust computation functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "bP_p7GrAlXgB"
   },
   "outputs": [],
   "source": [
    "# classic torch code for training except for the federated part\n",
    "def train_federated(args, models, device, train_loader, optimizers, epoch):\n",
    "    for c, m in models.items():\n",
    "        m.train()\n",
    "        # send models to workers\n",
    "        m.send(c)\n",
    "\n",
    "    # iterate over federated data client by client\n",
    "    # of course, in reality all clients would train their models at the same time\n",
    "    for batch_idx, (data, target) in enumerate(train_loader):\n",
    "        data, target = data.to(device), target.to(device)\n",
    "\n",
    "        optimizers[data.location].zero_grad()\n",
    "        output = models[data.location](data)\n",
    "\n",
    "        # loss is a ptr to the tensor loss at the remote location\n",
    "        loss = BCELoss()(output, torch.reshape(target, [len(target),1]))\n",
    "        # call backward() on the loss ptr, that will send the command to call\n",
    "        # backward on the actual loss tensor present on the remote machine\n",
    "        loss.backward()\n",
    "        optimizers[data.location].step()\n",
    "\n",
    "        if batch_idx % args['log_interval'] == 0:\n",
    "            # get back loss, that was created at remote worker\n",
    "            loss = loss.get()\n",
    "            print('Train Epoch: {} [{}/{} ({:.0f}%)]\\tLoss: {:.6f}\\tWorker: {}'.format(\n",
    "                    epoch, \n",
    "                    batch_idx * args['batch_size'], # number of packets done\n",
    "                    len(train_loader) * args['batch_size'], # total packets\n",
    "                    100. * batch_idx / len(train_loader), # percentage of batches done\n",
    "                    loss,\n",
    "                    data.location.id\n",
    "                )\n",
    "            )\n",
    "\n",
    "    # get back models for aggregation\n",
    "    for m in models.values():\n",
    "        m = m.get()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "cHZpA0PtaKtW"
   },
   "outputs": [],
   "source": [
    "# classic torch code for testing\n",
    "def test(model, device, test_loader):\n",
    "    model.eval()\n",
    "    test_loss = 0\n",
    "    correct = 0\n",
    "    with torch.no_grad():\n",
    "        for data, target in test_loader:\n",
    "            data, target = data.to(device), target.to(device)\n",
    "            output = model(data)\n",
    "\n",
    "            # add losses together\n",
    "            test_loss += BCELoss(reduction='sum')(output, torch.reshape(target, [len(target),1])).item()\n",
    "\n",
    "            # get the index of the max probability class\n",
    "            pred = pred = torch.round(output)\n",
    "            correct += pred.eq(target.view_as(pred)).sum().item()\n",
    "\n",
    "    test_loss /= len(test_loader.dataset)\n",
    "\n",
    "    print('\\nTest set: Average loss: {:.4f}, Accuracy: {}/{} ({:.0f}%)\\n'.format(\n",
    "        test_loss, correct, len(test_loader.dataset),\n",
    "        100. * correct / len(test_loader.dataset)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "Wmz8eKE3bYrt"
   },
   "outputs": [],
   "source": [
    "def aggregate(central_model, models, weights, trust):\n",
    "    with torch.no_grad():\n",
    "        # dataXtrust values needed for normalization later\n",
    "        dataXtrust_hidden_weight = np.zeros(central_model.layers[0].weight.shape, dtype='float32')\n",
    "        dataXtrust_hidden_bias = np.zeros(central_model.layers[0].bias.shape, dtype='float32')\n",
    "        dataXtrust_output_weight = np.zeros(central_model.layers[2].weight.shape, dtype='float32')\n",
    "        dataXtrust_output_bias = 0\n",
    "        # firstly compute new aggregated weight values\n",
    "        # to do so we start by taking the sum of the weights of all clients\n",
    "        for i, c in enumerate(used_clients):\n",
    "            # each client only contributes to chosen features (i.e. columns of weights arrays)\n",
    "            # for each of these features (columns), the aggregation uses the first x elements (rows) of central model weights\n",
    "            # where x is the number of hidden layer neurons of client and is equal to 2*(number_of_features_of_client)+1\n",
    "            rows = 2*models[c].n+1\n",
    "            for j, feature in enumerate(clients_datasets[i].columns[:-1]):\n",
    "                # find the index of feature in the central_model\n",
    "                index = testsets[0].columns[:-1].get_loc(feature)\n",
    "                weights['hidden_mean_weight'][:rows, index] += models[c].layers[0].weight.data[:, j].clone()*len(clients_datasets[i])*trust[c]\n",
    "                dataXtrust_hidden_weight[:rows, index] += len(clients_datasets[i])*trust[c]\n",
    "            # the rest of the weights don't have to be calculated feature-wise\n",
    "            weights['hidden_mean_bias'][:rows] += models[c].layers[0].bias.data.clone()*len(clients_datasets[i])*trust[c]\n",
    "            dataXtrust_hidden_bias[:rows] += len(clients_datasets[i])*trust[c]\n",
    "            weights['output_mean_weight'][0, :rows] += models[c].layers[2].weight.data[0, :].clone()*len(clients_datasets[i])*trust[c]\n",
    "            dataXtrust_output_weight[0, :rows] += len(clients_datasets[i])*trust[c]\n",
    "            weights['output_mean_bias'] += models[c].layers[2].bias.data.clone()*len(clients_datasets[i])*trust[c]\n",
    "            dataXtrust_output_bias += len(clients_datasets[i])*trust[c]\n",
    "\n",
    "        # and then we normalize the sum taking into account number of data and trust value for each client\n",
    "        # again parts of weights' arrays are normalized with respect only to clients that contributed to these parts\n",
    "        # change zero dataXtrust values to ones\n",
    "        dataXtrust_hidden_weight[dataXtrust_hidden_weight == 0] = 1\n",
    "        dataXtrust_hidden_bias[dataXtrust_hidden_bias == 0] = 1\n",
    "        dataXtrust_output_weight[dataXtrust_output_weight == 0] = 1\n",
    "        weights['hidden_mean_weight'] /= dataXtrust_hidden_weight\n",
    "        weights['hidden_mean_bias'] /= dataXtrust_hidden_bias\n",
    "        weights['output_mean_weight'] /= dataXtrust_output_weight\n",
    "        weights['output_mean_bias'] /= dataXtrust_output_bias\n",
    "\n",
    "        # secondly copy new weight values to the local models of all clients\n",
    "        for i, c in enumerate(used_clients):\n",
    "            rows = 2*models[c].n+1\n",
    "            for j, feature in enumerate(clients_datasets[i].columns[:-1]):\n",
    "                index = testsets[0].columns[:-1].get_loc(feature)\n",
    "                models[c].layers[0].weight.data[:, j] = weights['hidden_mean_weight'][:rows, index].clone()\n",
    "            # the rest of the weights don't have to be copied feature-wise\n",
    "            models[c].layers[0].bias.data = weights['hidden_mean_bias'][:rows].clone()\n",
    "            models[c].layers[2].weight.data[0, :] = weights['output_mean_weight'][0, :rows].clone()\n",
    "            models[c].layers[2].bias.data = weights['output_mean_bias'].clone()\n",
    "\n",
    "        # and finally copy to the central model for the test set\n",
    "        central_model.layers[0].weight.data = weights['hidden_mean_weight'].clone()\n",
    "        central_model.layers[0].bias.data = weights['hidden_mean_bias'].clone()\n",
    "        central_model.layers[2].weight.data = weights['output_mean_weight'].clone()\n",
    "        central_model.layers[2].bias.data = weights['output_mean_bias'].clone()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "VKXEFqRzdSvj"
   },
   "outputs": [],
   "source": [
    "def computeTrust(models, trust, r, s, num_of_clients_in_weights):\n",
    "    # dev[i] shows how much the weights of model of client i differ from the models of all other clients\n",
    "    # it is calculated in accordance with the relevant paper, but also taking into account the heterogeneity of models \n",
    "    dev = [0 for i in used_clients]\n",
    "    for i, c in enumerate(used_clients):\n",
    "        for j, cc in enumerate(used_clients):\n",
    "            # the smallest model defines the number of weights of rows (neurons) that will be compared\n",
    "            rows = min(2*models[c].n+1, 2*models[cc].n+1)\n",
    "            # between 2 clients, only weights of features that both have chosen are compared\n",
    "            for indexi, feature in enumerate(clients_datasets[i].columns[:-1]): \n",
    "                try:\n",
    "                    # find the index of the column of feature in cc client, provided that cc has chosen this feature\n",
    "                    indexj = clients_datasets[j].columns[:-1].get_loc(feature)\n",
    "                except:\n",
    "                    # go to the next feature, if current feature not chosen by cc\n",
    "                    continue\n",
    "                # for hidden layer, add to dev the sum of squared differences of weights of models divided by the number of clients which have each weight\n",
    "                to_divide = num_of_clients_in_weights['hidden'][:rows, testsets[0].columns[:-1].get_loc(feature)]\n",
    "                difference = models[cc].layers[0].weight.data[:rows, indexj].cpu() - models[c].layers[0].weight.data[:rows, indexi].cpu()\n",
    "                dev[i] += np.sum(difference.numpy()**2 / to_divide)\n",
    "            # output layer weights don't have to be compared feature-wise\n",
    "            # same as above for the output layer\n",
    "            difference = models[cc].layers[2].weight.data[0, :rows].cpu() - models[c].layers[2].weight.data[0, :rows].cpu()\n",
    "            dev[i] += np.sum(difference.numpy()**2 / num_of_clients_in_weights['output'][0, :rows])\n",
    "\n",
    "    # I[i] = 1 if client i acts normally and 0 if malicious or malfunctions\n",
    "    I = [1 if d <= 1.3*median(sorted(dev)) else 0 for d in dev]\n",
    "    print(\"dev: \",dev) # testing\n",
    "    print(\"median*1.3: \", 1.3*median(sorted(dev))) # testing\n",
    "    print(\"I: \", I) # testing\n",
    " \n",
    "    # compute new r, s values for every client\n",
    "    for i in range(len(used_clients)):\n",
    "        p1 = 0.5\n",
    "        #p2 = lambda x: x/median(sorted(dev)) if x/median(sorted(dev)) > 3 and x > 30 else (x/1000 if x > 1000 else (0.01 if I[i] == 1 and s[i] > 10 else 0.7))\n",
    "        p2 = lambda x: 0.8\n",
    "        r[i] = p1*r[i] + I[i]\n",
    "        s[i] = p2(dev[i])*s[i] + 1 - I[i]\n",
    "\n",
    "    # compute new trust value of every client\n",
    "    for i, c in enumerate(used_clients):\n",
    "        trust[c] = (r[i]+1)/(r[i]+s[i]+2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "PkbCZpmfLQ7M"
   },
   "source": [
    "## FL training with 7 clients"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "B_jH4dE8FVs1",
    "executionInfo": {
     "status": "ok",
     "timestamp": 1645476082798,
     "user_tz": -120,
     "elapsed": 2590964,
     "user": {
      "displayName": "Vasilis Petrakopoulos",
      "photoUrl": "https://lh3.googleusercontent.com/a/default-user=s64",
      "userId": "11664790255209564953"
     }
    },
    "outputId": "f1ec84b2-64b1-4e59-f950-9d1e327c9966"
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "Train Epoch: 1 [0/1168384 (0%)]\tLoss: 0.714087\tWorker: client1\n",
      "Train Epoch: 1 [25600/1168384 (2%)]\tLoss: 0.671301\tWorker: client1\n",
      "Train Epoch: 1 [51200/1168384 (4%)]\tLoss: 0.611931\tWorker: client1\n",
      "Train Epoch: 1 [76800/1168384 (7%)]\tLoss: 0.654635\tWorker: client2\n",
      "Train Epoch: 1 [102400/1168384 (9%)]\tLoss: 0.593969\tWorker: client2\n",
      "Train Epoch: 1 [128000/1168384 (11%)]\tLoss: 0.531032\tWorker: client2\n",
      "Train Epoch: 1 [153600/1168384 (13%)]\tLoss: 0.715500\tWorker: client3\n",
      "Train Epoch: 1 [179200/1168384 (15%)]\tLoss: 0.650567\tWorker: client3\n",
      "Train Epoch: 1 [204800/1168384 (18%)]\tLoss: 0.620186\tWorker: client3\n",
      "Train Epoch: 1 [230400/1168384 (20%)]\tLoss: 0.647581\tWorker: client4\n",
      "Train Epoch: 1 [256000/1168384 (22%)]\tLoss: 0.610654\tWorker: client4\n",
      "Train Epoch: 1 [281600/1168384 (24%)]\tLoss: 0.580527\tWorker: client4\n",
      "Train Epoch: 1 [307200/1168384 (26%)]\tLoss: 0.546155\tWorker: client4\n",
      "Train Epoch: 1 [332800/1168384 (28%)]\tLoss: 0.528478\tWorker: client4\n",
      "Train Epoch: 1 [358400/1168384 (31%)]\tLoss: 0.659841\tWorker: client5\n",
      "Train Epoch: 1 [384000/1168384 (33%)]\tLoss: 0.567387\tWorker: client5\n",
      "Train Epoch: 1 [409600/1168384 (35%)]\tLoss: 0.541775\tWorker: client5\n",
      "Train Epoch: 1 [435200/1168384 (37%)]\tLoss: 0.463185\tWorker: client5\n",
      "Train Epoch: 1 [460800/1168384 (39%)]\tLoss: 0.435618\tWorker: client5\n",
      "Train Epoch: 1 [486400/1168384 (42%)]\tLoss: 0.768427\tWorker: client6\n",
      "Train Epoch: 1 [512000/1168384 (44%)]\tLoss: 0.666346\tWorker: client6\n",
      "Train Epoch: 1 [537600/1168384 (46%)]\tLoss: 0.597622\tWorker: client6\n",
      "Train Epoch: 1 [563200/1168384 (48%)]\tLoss: 0.521962\tWorker: client6\n",
      "Train Epoch: 1 [588800/1168384 (50%)]\tLoss: 0.682069\tWorker: client7\n",
      "Train Epoch: 1 [614400/1168384 (53%)]\tLoss: 0.627924\tWorker: client7\n",
      "Train Epoch: 1 [640000/1168384 (55%)]\tLoss: 0.564691\tWorker: client7\n",
      "Train Epoch: 1 [665600/1168384 (57%)]\tLoss: 0.522268\tWorker: client7\n",
      "Train Epoch: 1 [691200/1168384 (59%)]\tLoss: 0.473065\tWorker: client7\n",
      "Train Epoch: 1 [716800/1168384 (61%)]\tLoss: 0.424317\tWorker: client7\n",
      "Train Epoch: 1 [742400/1168384 (64%)]\tLoss: 0.404299\tWorker: client7\n",
      "Train Epoch: 1 [768000/1168384 (66%)]\tLoss: 0.344779\tWorker: client7\n",
      "Train Epoch: 1 [793600/1168384 (68%)]\tLoss: 0.355195\tWorker: client7\n",
      "Train Epoch: 1 [819200/1168384 (70%)]\tLoss: 0.336425\tWorker: client7\n",
      "Train Epoch: 1 [844800/1168384 (72%)]\tLoss: 0.253768\tWorker: client7\n",
      "Train Epoch: 1 [870400/1168384 (74%)]\tLoss: 0.220093\tWorker: client7\n",
      "Train Epoch: 1 [896000/1168384 (77%)]\tLoss: 0.200351\tWorker: client7\n",
      "Train Epoch: 1 [921600/1168384 (79%)]\tLoss: 0.194034\tWorker: client7\n",
      "Train Epoch: 1 [947200/1168384 (81%)]\tLoss: 0.181136\tWorker: client7\n",
      "Train Epoch: 1 [972800/1168384 (83%)]\tLoss: 0.147521\tWorker: client7\n",
      "Train Epoch: 1 [998400/1168384 (85%)]\tLoss: 0.137662\tWorker: client7\n",
      "Train Epoch: 1 [1024000/1168384 (88%)]\tLoss: 0.130002\tWorker: client7\n",
      "Train Epoch: 1 [1049600/1168384 (90%)]\tLoss: 0.151443\tWorker: client7\n",
      "Train Epoch: 1 [1075200/1168384 (92%)]\tLoss: 0.121528\tWorker: client7\n",
      "Train Epoch: 1 [1100800/1168384 (94%)]\tLoss: 0.094473\tWorker: client7\n",
      "Train Epoch: 1 [1126400/1168384 (96%)]\tLoss: 0.078314\tWorker: client7\n",
      "Train Epoch: 1 [1152000/1168384 (99%)]\tLoss: 0.094337\tWorker: client7\n",
      "dev:  [11.937261551306479, 13.101656778337404, 10.436748481400839, 10.301939393553068, 13.255829285578896, 10.324052673415911, 12.82865822033509]\n",
      "median*1.3:  15.518440016698424\n",
      "I:  [1, 1, 1, 1, 1, 1, 1]\n",
      "\n",
      "Test set: Average loss: 0.5069, Accuracy: 342685/387902 (88%)\n",
      "\n",
      "Train Epoch: 2 [0/1168384 (0%)]\tLoss: 0.432632\tWorker: client1\n",
      "Train Epoch: 2 [25600/1168384 (2%)]\tLoss: 0.404135\tWorker: client1\n",
      "Train Epoch: 2 [51200/1168384 (4%)]\tLoss: 0.366493\tWorker: client1\n",
      "Train Epoch: 2 [76800/1168384 (7%)]\tLoss: 0.466716\tWorker: client2\n",
      "Train Epoch: 2 [102400/1168384 (9%)]\tLoss: 0.422429\tWorker: client2\n",
      "Train Epoch: 2 [128000/1168384 (11%)]\tLoss: 0.369524\tWorker: client2\n",
      "Train Epoch: 2 [153600/1168384 (13%)]\tLoss: 0.455845\tWorker: client3\n",
      "Train Epoch: 2 [179200/1168384 (15%)]\tLoss: 0.410730\tWorker: client3\n",
      "Train Epoch: 2 [204800/1168384 (18%)]\tLoss: 0.364309\tWorker: client3\n",
      "Train Epoch: 2 [230400/1168384 (20%)]\tLoss: 0.463485\tWorker: client4\n",
      "Train Epoch: 2 [256000/1168384 (22%)]\tLoss: 0.413548\tWorker: client4\n",
      "Train Epoch: 2 [281600/1168384 (24%)]\tLoss: 0.375592\tWorker: client4\n",
      "Train Epoch: 2 [307200/1168384 (26%)]\tLoss: 0.330633\tWorker: client4\n",
      "Train Epoch: 2 [332800/1168384 (28%)]\tLoss: 0.307773\tWorker: client4\n",
      "Train Epoch: 2 [358400/1168384 (31%)]\tLoss: 0.564951\tWorker: client5\n",
      "Train Epoch: 2 [384000/1168384 (33%)]\tLoss: 0.496241\tWorker: client5\n",
      "Train Epoch: 2 [409600/1168384 (35%)]\tLoss: 0.452882\tWorker: client5\n",
      "Train Epoch: 2 [435200/1168384 (37%)]\tLoss: 0.419399\tWorker: client5\n",
      "Train Epoch: 2 [460800/1168384 (39%)]\tLoss: 0.376941\tWorker: client5\n",
      "Train Epoch: 2 [486400/1168384 (42%)]\tLoss: 0.392245\tWorker: client6\n",
      "Train Epoch: 2 [512000/1168384 (44%)]\tLoss: 0.356873\tWorker: client6\n",
      "Train Epoch: 2 [537600/1168384 (46%)]\tLoss: 0.320149\tWorker: client6\n",
      "Train Epoch: 2 [563200/1168384 (48%)]\tLoss: 0.295457\tWorker: client6\n",
      "Train Epoch: 2 [588800/1168384 (50%)]\tLoss: 0.408238\tWorker: client7\n",
      "Train Epoch: 2 [614400/1168384 (53%)]\tLoss: 0.364477\tWorker: client7\n",
      "Train Epoch: 2 [640000/1168384 (55%)]\tLoss: 0.329565\tWorker: client7\n",
      "Train Epoch: 2 [665600/1168384 (57%)]\tLoss: 0.290597\tWorker: client7\n",
      "Train Epoch: 2 [691200/1168384 (59%)]\tLoss: 0.272191\tWorker: client7\n",
      "Train Epoch: 2 [716800/1168384 (61%)]\tLoss: 0.233087\tWorker: client7\n",
      "Train Epoch: 2 [742400/1168384 (64%)]\tLoss: 0.204944\tWorker: client7\n",
      "Train Epoch: 2 [768000/1168384 (66%)]\tLoss: 0.176244\tWorker: client7\n",
      "Train Epoch: 2 [793600/1168384 (68%)]\tLoss: 0.161895\tWorker: client7\n",
      "Train Epoch: 2 [819200/1168384 (70%)]\tLoss: 0.153291\tWorker: client7\n",
      "Train Epoch: 2 [844800/1168384 (72%)]\tLoss: 0.128120\tWorker: client7\n",
      "Train Epoch: 2 [870400/1168384 (74%)]\tLoss: 0.132789\tWorker: client7\n",
      "Train Epoch: 2 [896000/1168384 (77%)]\tLoss: 0.128555\tWorker: client7\n",
      "Train Epoch: 2 [921600/1168384 (79%)]\tLoss: 0.103470\tWorker: client7\n",
      "Train Epoch: 2 [947200/1168384 (81%)]\tLoss: 0.082935\tWorker: client7\n",
      "Train Epoch: 2 [972800/1168384 (83%)]\tLoss: 0.097275\tWorker: client7\n",
      "Train Epoch: 2 [998400/1168384 (85%)]\tLoss: 0.077266\tWorker: client7\n",
      "Train Epoch: 2 [1024000/1168384 (88%)]\tLoss: 0.069006\tWorker: client7\n",
      "Train Epoch: 2 [1049600/1168384 (90%)]\tLoss: 0.074877\tWorker: client7\n",
      "Train Epoch: 2 [1075200/1168384 (92%)]\tLoss: 0.066278\tWorker: client7\n",
      "Train Epoch: 2 [1100800/1168384 (94%)]\tLoss: 0.061063\tWorker: client7\n",
      "Train Epoch: 2 [1126400/1168384 (96%)]\tLoss: 0.054017\tWorker: client7\n",
      "Train Epoch: 2 [1152000/1168384 (99%)]\tLoss: 0.051683\tWorker: client7\n",
      "dev:  [0.16585561326104625, 0.17980549314526437, 0.16179204629390426, 0.18786363223504154, 0.26976724305282257, 0.1321316018695104, 0.7189396872945761]\n",
      "median*1.3:  0.23374714108884367\n",
      "I:  [1, 1, 1, 1, 0, 1, 0]\n",
      "\n",
      "Test set: Average loss: 0.3668, Accuracy: 346976/387902 (89%)\n",
      "\n",
      "Train Epoch: 3 [0/1168384 (0%)]\tLoss: 0.215441\tWorker: client1\n",
      "Train Epoch: 3 [25600/1168384 (2%)]\tLoss: 0.185320\tWorker: client1\n",
      "Train Epoch: 3 [51200/1168384 (4%)]\tLoss: 0.191671\tWorker: client1\n",
      "Train Epoch: 3 [76800/1168384 (7%)]\tLoss: 0.278913\tWorker: client2\n",
      "Train Epoch: 3 [102400/1168384 (9%)]\tLoss: 0.203005\tWorker: client2\n",
      "Train Epoch: 3 [128000/1168384 (11%)]\tLoss: 0.212308\tWorker: client2\n",
      "Train Epoch: 3 [153600/1168384 (13%)]\tLoss: 0.206298\tWorker: client3\n",
      "Train Epoch: 3 [179200/1168384 (15%)]\tLoss: 0.180183\tWorker: client3\n",
      "Train Epoch: 3 [204800/1168384 (18%)]\tLoss: 0.163097\tWorker: client3\n",
      "Train Epoch: 3 [230400/1168384 (20%)]\tLoss: 0.248748\tWorker: client4\n",
      "Train Epoch: 3 [256000/1168384 (22%)]\tLoss: 0.266832\tWorker: client4\n",
      "Train Epoch: 3 [281600/1168384 (24%)]\tLoss: 0.205733\tWorker: client4\n",
      "Train Epoch: 3 [307200/1168384 (26%)]\tLoss: 0.204442\tWorker: client4\n",
      "Train Epoch: 3 [332800/1168384 (28%)]\tLoss: 0.200166\tWorker: client4\n",
      "Train Epoch: 3 [358400/1168384 (31%)]\tLoss: 0.401743\tWorker: client5\n",
      "Train Epoch: 3 [384000/1168384 (33%)]\tLoss: 0.324995\tWorker: client5\n",
      "Train Epoch: 3 [409600/1168384 (35%)]\tLoss: 0.291684\tWorker: client5\n",
      "Train Epoch: 3 [435200/1168384 (37%)]\tLoss: 0.223983\tWorker: client5\n",
      "Train Epoch: 3 [460800/1168384 (39%)]\tLoss: 0.210672\tWorker: client5\n",
      "Train Epoch: 3 [486400/1168384 (42%)]\tLoss: 0.139947\tWorker: client6\n",
      "Train Epoch: 3 [512000/1168384 (44%)]\tLoss: 0.133906\tWorker: client6\n",
      "Train Epoch: 3 [537600/1168384 (46%)]\tLoss: 0.122675\tWorker: client6\n",
      "Train Epoch: 3 [563200/1168384 (48%)]\tLoss: 0.128287\tWorker: client6\n",
      "Train Epoch: 3 [588800/1168384 (50%)]\tLoss: 0.159725\tWorker: client7\n",
      "Train Epoch: 3 [614400/1168384 (53%)]\tLoss: 0.144551\tWorker: client7\n",
      "Train Epoch: 3 [640000/1168384 (55%)]\tLoss: 0.131170\tWorker: client7\n",
      "Train Epoch: 3 [665600/1168384 (57%)]\tLoss: 0.114490\tWorker: client7\n",
      "Train Epoch: 3 [691200/1168384 (59%)]\tLoss: 0.101180\tWorker: client7\n",
      "Train Epoch: 3 [716800/1168384 (61%)]\tLoss: 0.103266\tWorker: client7\n",
      "Train Epoch: 3 [742400/1168384 (64%)]\tLoss: 0.083983\tWorker: client7\n",
      "Train Epoch: 3 [768000/1168384 (66%)]\tLoss: 0.087853\tWorker: client7\n",
      "Train Epoch: 3 [793600/1168384 (68%)]\tLoss: 0.074749\tWorker: client7\n",
      "Train Epoch: 3 [819200/1168384 (70%)]\tLoss: 0.065157\tWorker: client7\n",
      "Train Epoch: 3 [844800/1168384 (72%)]\tLoss: 0.066794\tWorker: client7\n",
      "Train Epoch: 3 [870400/1168384 (74%)]\tLoss: 0.066211\tWorker: client7\n",
      "Train Epoch: 3 [896000/1168384 (77%)]\tLoss: 0.059517\tWorker: client7\n",
      "Train Epoch: 3 [921600/1168384 (79%)]\tLoss: 0.055339\tWorker: client7\n",
      "Train Epoch: 3 [947200/1168384 (81%)]\tLoss: 0.062710\tWorker: client7\n",
      "Train Epoch: 3 [972800/1168384 (83%)]\tLoss: 0.041741\tWorker: client7\n",
      "Train Epoch: 3 [998400/1168384 (85%)]\tLoss: 0.050191\tWorker: client7\n",
      "Train Epoch: 3 [1024000/1168384 (88%)]\tLoss: 0.038478\tWorker: client7\n",
      "Train Epoch: 3 [1049600/1168384 (90%)]\tLoss: 0.044874\tWorker: client7\n",
      "Train Epoch: 3 [1075200/1168384 (92%)]\tLoss: 0.061505\tWorker: client7\n",
      "Train Epoch: 3 [1100800/1168384 (94%)]\tLoss: 0.037377\tWorker: client7\n",
      "Train Epoch: 3 [1126400/1168384 (96%)]\tLoss: 0.031456\tWorker: client7\n",
      "Train Epoch: 3 [1152000/1168384 (99%)]\tLoss: 0.033323\tWorker: client7\n",
      "dev:  [0.0794230192353506, 0.08900408387428455, 0.07075150615368425, 0.0971061189109085, 0.1700279687323348, 0.06299712152486678, 0.2647891024498614]\n",
      "median*1.3:  0.1157053090365699\n",
      "I:  [1, 1, 1, 1, 0, 1, 0]\n",
      "\n",
      "Test set: Average loss: 0.3421, Accuracy: 347956/387902 (90%)\n",
      "\n",
      "Train Epoch: 4 [0/1168384 (0%)]\tLoss: 0.213306\tWorker: client1\n",
      "Train Epoch: 4 [25600/1168384 (2%)]\tLoss: 0.127904\tWorker: client1\n",
      "Train Epoch: 4 [51200/1168384 (4%)]\tLoss: 0.153491\tWorker: client1\n",
      "Train Epoch: 4 [76800/1168384 (7%)]\tLoss: 0.192044\tWorker: client2\n",
      "Train Epoch: 4 [102400/1168384 (9%)]\tLoss: 0.151272\tWorker: client2\n",
      "Train Epoch: 4 [128000/1168384 (11%)]\tLoss: 0.175357\tWorker: client2\n",
      "Train Epoch: 4 [153600/1168384 (13%)]\tLoss: 0.107905\tWorker: client3\n",
      "Train Epoch: 4 [179200/1168384 (15%)]\tLoss: 0.103811\tWorker: client3\n",
      "Train Epoch: 4 [204800/1168384 (18%)]\tLoss: 0.099216\tWorker: client3\n",
      "Train Epoch: 4 [230400/1168384 (20%)]\tLoss: 0.199045\tWorker: client4\n",
      "Train Epoch: 4 [256000/1168384 (22%)]\tLoss: 0.161197\tWorker: client4\n",
      "Train Epoch: 4 [281600/1168384 (24%)]\tLoss: 0.162593\tWorker: client4\n",
      "Train Epoch: 4 [307200/1168384 (26%)]\tLoss: 0.241934\tWorker: client4\n",
      "Train Epoch: 4 [332800/1168384 (28%)]\tLoss: 0.169940\tWorker: client4\n",
      "Train Epoch: 4 [358400/1168384 (31%)]\tLoss: 0.264318\tWorker: client5\n",
      "Train Epoch: 4 [384000/1168384 (33%)]\tLoss: 0.336640\tWorker: client5\n",
      "Train Epoch: 4 [409600/1168384 (35%)]\tLoss: 0.226438\tWorker: client5\n",
      "Train Epoch: 4 [435200/1168384 (37%)]\tLoss: 0.234482\tWorker: client5\n",
      "Train Epoch: 4 [460800/1168384 (39%)]\tLoss: 0.263224\tWorker: client5\n",
      "Train Epoch: 4 [486400/1168384 (42%)]\tLoss: 0.075314\tWorker: client6\n",
      "Train Epoch: 4 [512000/1168384 (44%)]\tLoss: 0.063130\tWorker: client6\n",
      "Train Epoch: 4 [537600/1168384 (46%)]\tLoss: 0.071243\tWorker: client6\n",
      "Train Epoch: 4 [563200/1168384 (48%)]\tLoss: 0.063484\tWorker: client6\n",
      "Train Epoch: 4 [588800/1168384 (50%)]\tLoss: 0.076639\tWorker: client7\n",
      "Train Epoch: 4 [614400/1168384 (53%)]\tLoss: 0.080478\tWorker: client7\n",
      "Train Epoch: 4 [640000/1168384 (55%)]\tLoss: 0.074706\tWorker: client7\n",
      "Train Epoch: 4 [665600/1168384 (57%)]\tLoss: 0.080125\tWorker: client7\n",
      "Train Epoch: 4 [691200/1168384 (59%)]\tLoss: 0.070512\tWorker: client7\n",
      "Train Epoch: 4 [716800/1168384 (61%)]\tLoss: 0.068605\tWorker: client7\n",
      "Train Epoch: 4 [742400/1168384 (64%)]\tLoss: 0.070577\tWorker: client7\n",
      "Train Epoch: 4 [768000/1168384 (66%)]\tLoss: 0.066477\tWorker: client7\n",
      "Train Epoch: 4 [793600/1168384 (68%)]\tLoss: 0.054299\tWorker: client7\n",
      "Train Epoch: 4 [819200/1168384 (70%)]\tLoss: 0.064708\tWorker: client7\n",
      "Train Epoch: 4 [844800/1168384 (72%)]\tLoss: 0.066899\tWorker: client7\n",
      "Train Epoch: 4 [870400/1168384 (74%)]\tLoss: 0.056295\tWorker: client7\n",
      "Train Epoch: 4 [896000/1168384 (77%)]\tLoss: 0.054376\tWorker: client7\n",
      "Train Epoch: 4 [921600/1168384 (79%)]\tLoss: 0.065195\tWorker: client7\n",
      "Train Epoch: 4 [947200/1168384 (81%)]\tLoss: 0.049110\tWorker: client7\n",
      "Train Epoch: 4 [972800/1168384 (83%)]\tLoss: 0.058455\tWorker: client7\n",
      "Train Epoch: 4 [998400/1168384 (85%)]\tLoss: 0.043063\tWorker: client7\n",
      "Train Epoch: 4 [1024000/1168384 (88%)]\tLoss: 0.044143\tWorker: client7\n",
      "Train Epoch: 4 [1049600/1168384 (90%)]\tLoss: 0.033765\tWorker: client7\n",
      "Train Epoch: 4 [1075200/1168384 (92%)]\tLoss: 0.041459\tWorker: client7\n",
      "Train Epoch: 4 [1100800/1168384 (94%)]\tLoss: 0.039277\tWorker: client7\n",
      "Train Epoch: 4 [1126400/1168384 (96%)]\tLoss: 0.044337\tWorker: client7\n",
      "Train Epoch: 4 [1152000/1168384 (99%)]\tLoss: 0.044149\tWorker: client7\n",
      "dev:  [0.018255273151173784, 0.020319157088546, 0.01527230775428249, 0.021618007772387236, 0.04491545102590407, 0.014925336571474036, 0.052635614853107957]\n",
      "median*1.3:  0.0264149042151098\n",
      "I:  [1, 1, 1, 1, 0, 1, 0]\n",
      "\n",
      "Test set: Average loss: 0.3453, Accuracy: 348282/387902 (90%)\n",
      "\n",
      "Train Epoch: 5 [0/1168384 (0%)]\tLoss: 0.147068\tWorker: client1\n",
      "Train Epoch: 5 [25600/1168384 (2%)]\tLoss: 0.119175\tWorker: client1\n",
      "Train Epoch: 5 [51200/1168384 (4%)]\tLoss: 0.123850\tWorker: client1\n",
      "Train Epoch: 5 [76800/1168384 (7%)]\tLoss: 0.182480\tWorker: client2\n",
      "Train Epoch: 5 [102400/1168384 (9%)]\tLoss: 0.151809\tWorker: client2\n",
      "Train Epoch: 5 [128000/1168384 (11%)]\tLoss: 0.123525\tWorker: client2\n",
      "Train Epoch: 5 [153600/1168384 (13%)]\tLoss: 0.089453\tWorker: client3\n",
      "Train Epoch: 5 [179200/1168384 (15%)]\tLoss: 0.094408\tWorker: client3\n",
      "Train Epoch: 5 [204800/1168384 (18%)]\tLoss: 0.065500\tWorker: client3\n",
      "Train Epoch: 5 [230400/1168384 (20%)]\tLoss: 0.148912\tWorker: client4\n",
      "Train Epoch: 5 [256000/1168384 (22%)]\tLoss: 0.116136\tWorker: client4\n",
      "Train Epoch: 5 [281600/1168384 (24%)]\tLoss: 0.139310\tWorker: client4\n",
      "Train Epoch: 5 [307200/1168384 (26%)]\tLoss: 0.142728\tWorker: client4\n",
      "Train Epoch: 5 [332800/1168384 (28%)]\tLoss: 0.094237\tWorker: client4\n",
      "Train Epoch: 5 [358400/1168384 (31%)]\tLoss: 0.307629\tWorker: client5\n",
      "Train Epoch: 5 [384000/1168384 (33%)]\tLoss: 0.278472\tWorker: client5\n",
      "Train Epoch: 5 [409600/1168384 (35%)]\tLoss: 0.222716\tWorker: client5\n",
      "Train Epoch: 5 [435200/1168384 (37%)]\tLoss: 0.206476\tWorker: client5\n",
      "Train Epoch: 5 [460800/1168384 (39%)]\tLoss: 0.186877\tWorker: client5\n",
      "Train Epoch: 5 [486400/1168384 (42%)]\tLoss: 0.068226\tWorker: client6\n",
      "Train Epoch: 5 [512000/1168384 (44%)]\tLoss: 0.053600\tWorker: client6\n",
      "Train Epoch: 5 [537600/1168384 (46%)]\tLoss: 0.050538\tWorker: client6\n",
      "Train Epoch: 5 [563200/1168384 (48%)]\tLoss: 0.046713\tWorker: client6\n",
      "Train Epoch: 5 [588800/1168384 (50%)]\tLoss: 0.062088\tWorker: client7\n",
      "Train Epoch: 5 [614400/1168384 (53%)]\tLoss: 0.056160\tWorker: client7\n",
      "Train Epoch: 5 [640000/1168384 (55%)]\tLoss: 0.062079\tWorker: client7\n",
      "Train Epoch: 5 [665600/1168384 (57%)]\tLoss: 0.059334\tWorker: client7\n",
      "Train Epoch: 5 [691200/1168384 (59%)]\tLoss: 0.058242\tWorker: client7\n",
      "Train Epoch: 5 [716800/1168384 (61%)]\tLoss: 0.050014\tWorker: client7\n",
      "Train Epoch: 5 [742400/1168384 (64%)]\tLoss: 0.046025\tWorker: client7\n",
      "Train Epoch: 5 [768000/1168384 (66%)]\tLoss: 0.051929\tWorker: client7\n",
      "Train Epoch: 5 [793600/1168384 (68%)]\tLoss: 0.043500\tWorker: client7\n",
      "Train Epoch: 5 [819200/1168384 (70%)]\tLoss: 0.048111\tWorker: client7\n",
      "Train Epoch: 5 [844800/1168384 (72%)]\tLoss: 0.046544\tWorker: client7\n",
      "Train Epoch: 5 [870400/1168384 (74%)]\tLoss: 0.048500\tWorker: client7\n",
      "Train Epoch: 5 [896000/1168384 (77%)]\tLoss: 0.036809\tWorker: client7\n",
      "Train Epoch: 5 [921600/1168384 (79%)]\tLoss: 0.037099\tWorker: client7\n",
      "Train Epoch: 5 [947200/1168384 (81%)]\tLoss: 0.044325\tWorker: client7\n",
      "Train Epoch: 5 [972800/1168384 (83%)]\tLoss: 0.064429\tWorker: client7\n",
      "Train Epoch: 5 [998400/1168384 (85%)]\tLoss: 0.039328\tWorker: client7\n",
      "Train Epoch: 5 [1024000/1168384 (88%)]\tLoss: 0.040520\tWorker: client7\n",
      "Train Epoch: 5 [1049600/1168384 (90%)]\tLoss: 0.033769\tWorker: client7\n",
      "Train Epoch: 5 [1075200/1168384 (92%)]\tLoss: 0.049051\tWorker: client7\n",
      "Train Epoch: 5 [1100800/1168384 (94%)]\tLoss: 0.033621\tWorker: client7\n",
      "Train Epoch: 5 [1126400/1168384 (96%)]\tLoss: 0.032703\tWorker: client7\n",
      "Train Epoch: 5 [1152000/1168384 (99%)]\tLoss: 0.040873\tWorker: client7\n",
      "dev:  [0.014723631314848459, 0.016430339463566875, 0.011890309643049359, 0.01771292357633327, 0.03862132354334826, 0.012051115703673483, 0.03772824880119645]\n",
      "median*1.3:  0.021359441302636936\n",
      "I:  [1, 1, 1, 1, 0, 1, 0]\n",
      "\n",
      "Test set: Average loss: 0.3525, Accuracy: 348966/387902 (90%)\n",
      "\n",
      "Train Epoch: 6 [0/1168384 (0%)]\tLoss: 0.088192\tWorker: client1\n",
      "Train Epoch: 6 [25600/1168384 (2%)]\tLoss: 0.121965\tWorker: client1\n",
      "Train Epoch: 6 [51200/1168384 (4%)]\tLoss: 0.137840\tWorker: client1\n",
      "Train Epoch: 6 [76800/1168384 (7%)]\tLoss: 0.146117\tWorker: client2\n",
      "Train Epoch: 6 [102400/1168384 (9%)]\tLoss: 0.112919\tWorker: client2\n",
      "Train Epoch: 6 [128000/1168384 (11%)]\tLoss: 0.136985\tWorker: client2\n",
      "Train Epoch: 6 [153600/1168384 (13%)]\tLoss: 0.059431\tWorker: client3\n",
      "Train Epoch: 6 [179200/1168384 (15%)]\tLoss: 0.057272\tWorker: client3\n",
      "Train Epoch: 6 [204800/1168384 (18%)]\tLoss: 0.051498\tWorker: client3\n",
      "Train Epoch: 6 [230400/1168384 (20%)]\tLoss: 0.161835\tWorker: client4\n",
      "Train Epoch: 6 [256000/1168384 (22%)]\tLoss: 0.166527\tWorker: client4\n",
      "Train Epoch: 6 [281600/1168384 (24%)]\tLoss: 0.124115\tWorker: client4\n",
      "Train Epoch: 6 [307200/1168384 (26%)]\tLoss: 0.191242\tWorker: client4\n",
      "Train Epoch: 6 [332800/1168384 (28%)]\tLoss: 0.163775\tWorker: client4\n",
      "Train Epoch: 6 [358400/1168384 (31%)]\tLoss: 0.236732\tWorker: client5\n",
      "Train Epoch: 6 [384000/1168384 (33%)]\tLoss: 0.250801\tWorker: client5\n",
      "Train Epoch: 6 [409600/1168384 (35%)]\tLoss: 0.237470\tWorker: client5\n",
      "Train Epoch: 6 [435200/1168384 (37%)]\tLoss: 0.153102\tWorker: client5\n",
      "Train Epoch: 6 [460800/1168384 (39%)]\tLoss: 0.162855\tWorker: client5\n",
      "Train Epoch: 6 [486400/1168384 (42%)]\tLoss: 0.045107\tWorker: client6\n",
      "Train Epoch: 6 [512000/1168384 (44%)]\tLoss: 0.035586\tWorker: client6\n",
      "Train Epoch: 6 [537600/1168384 (46%)]\tLoss: 0.045913\tWorker: client6\n",
      "Train Epoch: 6 [563200/1168384 (48%)]\tLoss: 0.032383\tWorker: client6\n",
      "Train Epoch: 6 [588800/1168384 (50%)]\tLoss: 0.047375\tWorker: client7\n",
      "Train Epoch: 6 [614400/1168384 (53%)]\tLoss: 0.044476\tWorker: client7\n",
      "Train Epoch: 6 [640000/1168384 (55%)]\tLoss: 0.051116\tWorker: client7\n",
      "Train Epoch: 6 [665600/1168384 (57%)]\tLoss: 0.049551\tWorker: client7\n",
      "Train Epoch: 6 [691200/1168384 (59%)]\tLoss: 0.045833\tWorker: client7\n",
      "Train Epoch: 6 [716800/1168384 (61%)]\tLoss: 0.038750\tWorker: client7\n",
      "Train Epoch: 6 [742400/1168384 (64%)]\tLoss: 0.047318\tWorker: client7\n",
      "Train Epoch: 6 [768000/1168384 (66%)]\tLoss: 0.044844\tWorker: client7\n",
      "Train Epoch: 6 [793600/1168384 (68%)]\tLoss: 0.047824\tWorker: client7\n",
      "Train Epoch: 6 [819200/1168384 (70%)]\tLoss: 0.036214\tWorker: client7\n",
      "Train Epoch: 6 [844800/1168384 (72%)]\tLoss: 0.040252\tWorker: client7\n",
      "Train Epoch: 6 [870400/1168384 (74%)]\tLoss: 0.037488\tWorker: client7\n",
      "Train Epoch: 6 [896000/1168384 (77%)]\tLoss: 0.036666\tWorker: client7\n",
      "Train Epoch: 6 [921600/1168384 (79%)]\tLoss: 0.044405\tWorker: client7\n",
      "Train Epoch: 6 [947200/1168384 (81%)]\tLoss: 0.042179\tWorker: client7\n",
      "Train Epoch: 6 [972800/1168384 (83%)]\tLoss: 0.036948\tWorker: client7\n",
      "Train Epoch: 6 [998400/1168384 (85%)]\tLoss: 0.027401\tWorker: client7\n",
      "Train Epoch: 6 [1024000/1168384 (88%)]\tLoss: 0.036714\tWorker: client7\n",
      "Train Epoch: 6 [1049600/1168384 (90%)]\tLoss: 0.029276\tWorker: client7\n",
      "Train Epoch: 6 [1075200/1168384 (92%)]\tLoss: 0.029946\tWorker: client7\n",
      "Train Epoch: 6 [1100800/1168384 (94%)]\tLoss: 0.028684\tWorker: client7\n",
      "Train Epoch: 6 [1126400/1168384 (96%)]\tLoss: 0.034883\tWorker: client7\n",
      "Train Epoch: 6 [1152000/1168384 (99%)]\tLoss: 0.029803\tWorker: client7\n",
      "dev:  [0.012430218230472048, 0.013855609725913013, 0.009736782008495707, 0.015119127745065523, 0.033931093790756546, 0.010145108667456116, 0.028867192465279794]\n",
      "median*1.3:  0.01801229264368692\n",
      "I:  [1, 1, 1, 1, 0, 1, 0]\n",
      "\n",
      "Test set: Average loss: 0.3614, Accuracy: 349254/387902 (90%)\n",
      "\n",
      "Train Epoch: 7 [0/1168384 (0%)]\tLoss: 0.106324\tWorker: client1\n",
      "Train Epoch: 7 [25600/1168384 (2%)]\tLoss: 0.095708\tWorker: client1\n",
      "Train Epoch: 7 [51200/1168384 (4%)]\tLoss: 0.097211\tWorker: client1\n",
      "Train Epoch: 7 [76800/1168384 (7%)]\tLoss: 0.120289\tWorker: client2\n",
      "Train Epoch: 7 [102400/1168384 (9%)]\tLoss: 0.121306\tWorker: client2\n",
      "Train Epoch: 7 [128000/1168384 (11%)]\tLoss: 0.103284\tWorker: client2\n",
      "Train Epoch: 7 [153600/1168384 (13%)]\tLoss: 0.053572\tWorker: client3\n",
      "Train Epoch: 7 [179200/1168384 (15%)]\tLoss: 0.049446\tWorker: client3\n",
      "Train Epoch: 7 [204800/1168384 (18%)]\tLoss: 0.049122\tWorker: client3\n",
      "Train Epoch: 7 [230400/1168384 (20%)]\tLoss: 0.187079\tWorker: client4\n",
      "Train Epoch: 7 [256000/1168384 (22%)]\tLoss: 0.082454\tWorker: client4\n",
      "Train Epoch: 7 [281600/1168384 (24%)]\tLoss: 0.130782\tWorker: client4\n",
      "Train Epoch: 7 [307200/1168384 (26%)]\tLoss: 0.127029\tWorker: client4\n",
      "Train Epoch: 7 [332800/1168384 (28%)]\tLoss: 0.144633\tWorker: client4\n",
      "Train Epoch: 7 [358400/1168384 (31%)]\tLoss: 0.226190\tWorker: client5\n",
      "Train Epoch: 7 [384000/1168384 (33%)]\tLoss: 0.252984\tWorker: client5\n",
      "Train Epoch: 7 [409600/1168384 (35%)]\tLoss: 0.164938\tWorker: client5\n",
      "Train Epoch: 7 [435200/1168384 (37%)]\tLoss: 0.152240\tWorker: client5\n",
      "Train Epoch: 7 [460800/1168384 (39%)]\tLoss: 0.153830\tWorker: client5\n",
      "Train Epoch: 7 [486400/1168384 (42%)]\tLoss: 0.035721\tWorker: client6\n",
      "Train Epoch: 7 [512000/1168384 (44%)]\tLoss: 0.032295\tWorker: client6\n",
      "Train Epoch: 7 [537600/1168384 (46%)]\tLoss: 0.030288\tWorker: client6\n",
      "Train Epoch: 7 [563200/1168384 (48%)]\tLoss: 0.027183\tWorker: client6\n",
      "Train Epoch: 7 [588800/1168384 (50%)]\tLoss: 0.033085\tWorker: client7\n",
      "Train Epoch: 7 [614400/1168384 (53%)]\tLoss: 0.042234\tWorker: client7\n",
      "Train Epoch: 7 [640000/1168384 (55%)]\tLoss: 0.041580\tWorker: client7\n",
      "Train Epoch: 7 [665600/1168384 (57%)]\tLoss: 0.044525\tWorker: client7\n",
      "Train Epoch: 7 [691200/1168384 (59%)]\tLoss: 0.032050\tWorker: client7\n",
      "Train Epoch: 7 [716800/1168384 (61%)]\tLoss: 0.033997\tWorker: client7\n",
      "Train Epoch: 7 [742400/1168384 (64%)]\tLoss: 0.042264\tWorker: client7\n",
      "Train Epoch: 7 [768000/1168384 (66%)]\tLoss: 0.030522\tWorker: client7\n",
      "Train Epoch: 7 [793600/1168384 (68%)]\tLoss: 0.037791\tWorker: client7\n",
      "Train Epoch: 7 [819200/1168384 (70%)]\tLoss: 0.033832\tWorker: client7\n",
      "Train Epoch: 7 [844800/1168384 (72%)]\tLoss: 0.029833\tWorker: client7\n",
      "Train Epoch: 7 [870400/1168384 (74%)]\tLoss: 0.039903\tWorker: client7\n",
      "Train Epoch: 7 [896000/1168384 (77%)]\tLoss: 0.027506\tWorker: client7\n",
      "Train Epoch: 7 [921600/1168384 (79%)]\tLoss: 0.035588\tWorker: client7\n",
      "Train Epoch: 7 [947200/1168384 (81%)]\tLoss: 0.031358\tWorker: client7\n",
      "Train Epoch: 7 [972800/1168384 (83%)]\tLoss: 0.029838\tWorker: client7\n",
      "Train Epoch: 7 [998400/1168384 (85%)]\tLoss: 0.029493\tWorker: client7\n",
      "Train Epoch: 7 [1024000/1168384 (88%)]\tLoss: 0.031117\tWorker: client7\n",
      "Train Epoch: 7 [1049600/1168384 (90%)]\tLoss: 0.028811\tWorker: client7\n",
      "Train Epoch: 7 [1075200/1168384 (92%)]\tLoss: 0.029442\tWorker: client7\n",
      "Train Epoch: 7 [1100800/1168384 (94%)]\tLoss: 0.023309\tWorker: client7\n",
      "Train Epoch: 7 [1126400/1168384 (96%)]\tLoss: 0.024863\tWorker: client7\n",
      "Train Epoch: 7 [1152000/1168384 (99%)]\tLoss: 0.025594\tWorker: client7\n",
      "dev:  [0.010913331976244254, 0.011960968489373176, 0.008278465086089317, 0.013390515130541403, 0.029996257548803185, 0.008864243543444438, 0.023110856489315813]\n",
      "median*1.3:  0.01554925903618513\n",
      "I:  [1, 1, 1, 1, 0, 1, 0]\n",
      "\n",
      "Test set: Average loss: 0.3711, Accuracy: 349273/387902 (90%)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# take appropriate number of clients\n",
    "used_clients = clients[:7]\n",
    "\n",
    "# central model\n",
    "central_model = MLP(len(testsets[0].columns[:-1])).to(device)\n",
    "# initialize weights of central model to zero,\n",
    "# so that features which are dropped by all clients do not affect testing\n",
    "central_model.layers[0].weight.data.fill_(0)\n",
    "central_model.layers[0].bias.data.fill_(0)\n",
    "central_model.layers[2].weight.data.fill_(0)\n",
    "central_model.layers[2].bias.data.fill_(0)\n",
    "\n",
    "# clients' models, optimizers and schedulers for learning rate\n",
    "# note that central entity knows the chosen features of each client from the preprocessing procedure\n",
    "models = {used_clients[i]:MLP(len(clients_datasets[i].columns[:-1])).to(device) for i in range(len(used_clients))}\n",
    "optimizers = {i:optim.SGD(models[i].parameters(), lr=args['lr']) for i in used_clients}\n",
    "# decreasing learning rate\n",
    "lamda = lambda epoch: 1 if epoch < 3 else 0.5\n",
    "schedulers = {i:sched.LambdaLR(optimizers[i], lr_lambda=lamda) for i in used_clients}\n",
    "\n",
    "# initialization of dictionary for models aggregation\n",
    "weights = {'hidden_mean_weight' : torch.zeros(size=central_model.layers[0].weight.shape).to(device),\n",
    "           'hidden_mean_bias' : torch.zeros(size=central_model.layers[0].bias.shape).to(device),\n",
    "           'output_mean_weight' : torch.zeros(size=central_model.layers[2].weight.shape).to(device),\n",
    "           'output_mean_bias' : torch.zeros(size=central_model.layers[2].bias.shape).to(device)}\n",
    "\n",
    "# trust values\n",
    "trust = {i:0 for i in used_clients}\n",
    "r = [0 for i in used_clients]\n",
    "s = [0 for i in used_clients]\n",
    "\n",
    "# for each weight of central_model, count the number of clients which contain this weight in their models\n",
    "# needed to compute the trust value of each client\n",
    "num_of_clients_in_weights = {'hidden' : np.zeros(central_model.layers[0].weight.shape),\n",
    "                             'output' : np.zeros(central_model.layers[2].weight.shape)}\n",
    "for i, c in enumerate(used_clients):\n",
    "    rows = 2*models[c].n+1\n",
    "    num_of_clients_in_weights['output'][0, :rows] += 1\n",
    "    for j, feature in enumerate(clients_datasets[i].columns[:-1]):\n",
    "        index = testsets[0].columns[:-1].get_loc(feature)\n",
    "        num_of_clients_in_weights['hidden'][:rows, index] += 1\n",
    "\n",
    "for epoch in range(1, args['epochs'] + 1):\n",
    "    train_federated(args, models, device, federated_train_loaders[0], optimizers, epoch)\n",
    "    for scheduler in schedulers.values():\n",
    "        scheduler.step()\n",
    "    computeTrust(models, trust, r, s, num_of_clients_in_weights)\n",
    "    aggregate(central_model, models, weights, trust)\n",
    "    test(central_model, device, test_loaders[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "WsN3oJfiocLm"
   },
   "source": [
    "## Non-federated training results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "WCpgeKm2o8BI"
   },
   "outputs": [],
   "source": [
    "def train(models, device, train_loader, optimizers):\n",
    "    for c, m in models.items():\n",
    "        m.train()\n",
    "        m.send(c)\n",
    "\n",
    "    for batch_idx, (data, target) in enumerate(train_loader):\n",
    "        data, target = data.to(device), target.to(device)\n",
    "        optimizers[data.location].zero_grad()\n",
    "        output = models[data.location](data)\n",
    "        loss = BCELoss()(output, torch.reshape(target, [len(target),1]))\n",
    "        loss.backward()\n",
    "        optimizers[data.location].step()\n",
    "\n",
    "    for m in models.values():\n",
    "        m = m.get()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "m2UF4KCSr9ps",
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "def test_all(test_model, models, device, test_loader):\n",
    "    test_model.eval()\n",
    "    with torch.no_grad():\n",
    "        for i, c in enumerate(used_clients):\n",
    "            rows = 2*models[c].n+1\n",
    "            for j, feature in enumerate(clients_datasets[i].columns[:-1]):\n",
    "                index = testsets[0].columns[:-1].get_loc(feature)\n",
    "                test_model.layers[0].weight.data[:rows, index] = models[c].layers[0].weight.data[:, j].clone()\n",
    "            test_model.layers[0].bias.data[:rows] = models[c].layers[0].bias.data.clone()\n",
    "            test_model.layers[2].weight.data[0, :rows] = models[c].layers[2].weight.data[0, :].clone()\n",
    "            test_model.layers[2].bias.data = models[c].layers[2].bias.data.clone()\n",
    "\n",
    "            test_loss = 0\n",
    "            correct = 0\n",
    "            for data, target in test_loader:\n",
    "                data, target = data.to(device), target.to(device)\n",
    "                output = test_model(data)\n",
    "                test_loss += BCELoss(reduction='sum')(output, torch.reshape(target, [len(target),1])).item()\n",
    "                pred = pred = torch.round(output)\n",
    "                correct += pred.eq(target.view_as(pred)).sum().item()\n",
    "            test_loss /= len(test_loader.dataset)\n",
    "\n",
    "            print('\\tClient' + str(i+1) + ': Average loss: {:.4f}, Accuracy: {}/{} ({:.0f}%)'.format(\n",
    "            test_loss, correct, len(test_loader.dataset),\n",
    "            100. * correct / len(test_loader.dataset)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 14734039,
     "status": "ok",
     "timestamp": 1645284807524,
     "user": {
      "displayName": "Vasilis Petrakopoulos",
      "photoUrl": "https://lh3.googleusercontent.com/a/default-user=s64",
      "userId": "11664790255209564953"
     },
     "user_tz": -120
    },
    "id": "CHumscmWqgrK",
    "outputId": "7da1623c-50d4-4ca1-ace5-58aa7bd49567"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Epoch 1:\n",
      "\tClient1: Average loss: 0.5419, Accuracy: 253311/387902 (65%)\n",
      "\tClient2: Average loss: 0.4953, Accuracy: 312430/387902 (81%)\n",
      "\tClient3: Average loss: 0.5538, Accuracy: 344678/387902 (89%)\n",
      "\tClient4: Average loss: 0.6030, Accuracy: 276110/387902 (71%)\n",
      "\tClient5: Average loss: 0.6955, Accuracy: 181814/387902 (47%)\n",
      "\tClient6: Average loss: 0.5911, Accuracy: 304388/387902 (78%)\n",
      "\tClient7: Average loss: 0.3941, Accuracy: 348026/387902 (90%)\n",
      "\n",
      "Train Epoch 2:\n",
      "\tClient1: Average loss: 0.4920, Accuracy: 238917/387902 (62%)\n",
      "\tClient2: Average loss: 0.4601, Accuracy: 277588/387902 (72%)\n",
      "\tClient3: Average loss: 0.4703, Accuracy: 342672/387902 (88%)\n",
      "\tClient4: Average loss: 0.5662, Accuracy: 260011/387902 (67%)\n",
      "\tClient5: Average loss: 0.6160, Accuracy: 332187/387902 (86%)\n",
      "\tClient6: Average loss: 0.4201, Accuracy: 352529/387902 (91%)\n",
      "\tClient7: Average loss: 0.4154, Accuracy: 348877/387902 (90%)\n",
      "\n",
      "Train Epoch 3:\n",
      "\tClient1: Average loss: 0.4712, Accuracy: 250316/387902 (65%)\n",
      "\tClient2: Average loss: 0.4256, Accuracy: 281045/387902 (72%)\n",
      "\tClient3: Average loss: 0.4122, Accuracy: 340565/387902 (88%)\n",
      "\tClient4: Average loss: 0.5943, Accuracy: 258445/387902 (67%)\n",
      "\tClient5: Average loss: 0.5357, Accuracy: 335197/387902 (86%)\n",
      "\tClient6: Average loss: 0.3336, Accuracy: 351704/387902 (91%)\n",
      "\tClient7: Average loss: 0.4450, Accuracy: 349328/387902 (90%)\n",
      "\n",
      "Train Epoch 4:\n",
      "\tClient1: Average loss: 0.4680, Accuracy: 254838/387902 (66%)\n",
      "\tClient2: Average loss: 0.4158, Accuracy: 285854/387902 (74%)\n",
      "\tClient3: Average loss: 0.3926, Accuracy: 339801/387902 (88%)\n",
      "\tClient4: Average loss: 0.6174, Accuracy: 258367/387902 (67%)\n",
      "\tClient5: Average loss: 0.5064, Accuracy: 334093/387902 (86%)\n",
      "\tClient6: Average loss: 0.3119, Accuracy: 352374/387902 (91%)\n",
      "\tClient7: Average loss: 0.4578, Accuracy: 349503/387902 (90%)\n",
      "\n",
      "Train Epoch 5:\n",
      "\tClient1: Average loss: 0.4667, Accuracy: 260165/387902 (67%)\n",
      "\tClient2: Average loss: 0.4105, Accuracy: 289112/387902 (75%)\n",
      "\tClient3: Average loss: 0.3780, Accuracy: 339569/387902 (88%)\n",
      "\tClient4: Average loss: 0.6419, Accuracy: 258335/387902 (67%)\n",
      "\tClient5: Average loss: 0.4853, Accuracy: 333329/387902 (86%)\n",
      "\tClient6: Average loss: 0.2985, Accuracy: 351617/387902 (91%)\n",
      "\tClient7: Average loss: 0.4693, Accuracy: 349582/387902 (90%)\n",
      "\n",
      "Train Epoch 6:\n",
      "\tClient1: Average loss: 0.4673, Accuracy: 263886/387902 (68%)\n",
      "\tClient2: Average loss: 0.4077, Accuracy: 291269/387902 (75%)\n",
      "\tClient3: Average loss: 0.3676, Accuracy: 339267/387902 (87%)\n",
      "\tClient4: Average loss: 0.6666, Accuracy: 258289/387902 (67%)\n",
      "\tClient5: Average loss: 0.4715, Accuracy: 332975/387902 (86%)\n",
      "\tClient6: Average loss: 0.2905, Accuracy: 350770/387902 (90%)\n",
      "\tClient7: Average loss: 0.4797, Accuracy: 349610/387902 (90%)\n",
      "\n",
      "Train Epoch 7:\n",
      "\tClient1: Average loss: 0.4693, Accuracy: 266855/387902 (69%)\n",
      "\tClient2: Average loss: 0.4066, Accuracy: 293687/387902 (76%)\n",
      "\tClient3: Average loss: 0.3605, Accuracy: 339182/387902 (87%)\n",
      "\tClient4: Average loss: 0.6908, Accuracy: 258164/387902 (67%)\n",
      "\tClient5: Average loss: 0.4635, Accuracy: 332619/387902 (86%)\n",
      "\tClient6: Average loss: 0.2861, Accuracy: 350300/387902 (90%)\n",
      "\tClient7: Average loss: 0.4892, Accuracy: 349627/387902 (90%)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "used_clients = clients[:7]\n",
    "test_model = MLP(len(testsets[0].columns[:-1])).to(device)\n",
    "test_model.layers[0].weight.data.fill_(0)\n",
    "test_model.layers[0].bias.data.fill_(0)\n",
    "test_model.layers[2].weight.data.fill_(0)\n",
    "test_model.layers[2].bias.data.fill_(0)\n",
    "models = {used_clients[i]:MLP(len(clients_datasets[i].columns[:-1])).to(device) for i in range(len(used_clients))}\n",
    "optimizers = {i:optim.SGD(models[i].parameters(), lr=args['lr']) for i in used_clients}\n",
    "lamda = lambda epoch: 1 if epoch < 3 else 0.5\n",
    "schedulers = {i:sched.LambdaLR(optimizers[i], lr_lambda=lamda) for i in used_clients}\n",
    "\n",
    "for epoch in range(1, args['epochs'] + 1):\n",
    "    train(models, device, federated_train_loaders[0], optimizers)\n",
    "    for scheduler in schedulers.values():\n",
    "        scheduler.step()\n",
    "    print('Train Epoch ' + str(epoch) + ':')\n",
    "    test_all(test_model, models, device, test_loaders[0])\n",
    "    print()"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "collapsed_sections": [
    "bgSLI4kdwT_r",
    "i7GnUhGXwYM3",
    "wv6y7T3jwbDq",
    "5x-VNSm7PjMa",
    "WsN3oJfiocLm"
   ],
   "machine_shape": "hm",
   "name": "DNS_Trusted_HeteroFL.ipynb",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "name": "python3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}