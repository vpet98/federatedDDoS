{"cells":[{"cell_type":"markdown","metadata":{"id":"ej63Q9F5VDWy"},"source":["Tutorial on Federated Learning with FedAvg using PyTorch and PySyft based on https://learnopencv.com/federated-learning-using-pytorch-and-pysyft/ and https://towardsdatascience.com/federated-learning-a-simple-implementation-of-fedavg-federated-averaging-with-pytorch-90187c9c9577"]},{"cell_type":"markdown","metadata":{"id":"dNwIEBajagKA"},"source":["## Install PySyft and Torchvision"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"p9Z-tl_jWgeE"},"outputs":[],"source":["!pip install syft==0.2.9"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"FQBYDiJbdQHG"},"outputs":[],"source":["!pip install torchvision==0.2.0"]},{"cell_type":"markdown","metadata":{"id":"x2WDKGKIYdUp"},"source":["## Simple Testing on PySyft (can be skipped)"]},{"cell_type":"code","execution_count":1,"metadata":{"id":"tVNZDzTLVUS8","executionInfo":{"status":"ok","timestamp":1645532659365,"user_tz":-120,"elapsed":8585,"user":{"displayName":"Vasilis Petrakopoulos","photoUrl":"https://lh3.googleusercontent.com/a/default-user=s64","userId":"11664790255209564953"}}},"outputs":[],"source":["import torch\n","import syft as sy\n","hook = sy.TorchHook(torch) # add extra functionality to PyTorch"]},{"cell_type":"code","execution_count":2,"metadata":{"id":"MyXfr3LhV91R","executionInfo":{"status":"ok","timestamp":1645532659366,"user_tz":-120,"elapsed":32,"user":{"displayName":"Vasilis Petrakopoulos","photoUrl":"https://lh3.googleusercontent.com/a/default-user=s64","userId":"11664790255209564953"}}},"outputs":[],"source":["# create a machine owned by harmony clinic\n","harmony_clinic = sy.VirtualWorker(hook=hook,id='clinic')"]},{"cell_type":"code","execution_count":3,"metadata":{"id":"M75kQXdSXXtC","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1645532659366,"user_tz":-120,"elapsed":29,"user":{"displayName":"Vasilis Petrakopoulos","photoUrl":"https://lh3.googleusercontent.com/a/default-user=s64","userId":"11664790255209564953"}},"outputId":"23ad8a86-8929-4495-840a-3a25f90368a0"},"outputs":[{"output_type":"stream","name":"stdout","text":["(Wrapper)>[PointerTensor | me:5296137061 -> clinic:98943550915]\n","{98943550915: tensor([0, 1, 2, 1, 2])}\n"]}],"source":["# we create a Tensor, maybe this is some gene sequence\n","dna = torch.tensor([0,1,2,1,2])\n","\n","# and now I send it, and in turn we get a pointer back that\n","# points to that Tensor\n","dna_ptr = dna.send(harmony_clinic)\n","\n","print(dna_ptr)\n","print(harmony_clinic._objects)"]},{"cell_type":"code","execution_count":4,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"pmEReTW6YB3d","outputId":"1853c35d-d1e8-4982-8509-ca13d60bf1d3","executionInfo":{"status":"ok","timestamp":1645532659367,"user_tz":-120,"elapsed":26,"user":{"displayName":"Vasilis Petrakopoulos","photoUrl":"https://lh3.googleusercontent.com/a/default-user=s64","userId":"11664790255209564953"}}},"outputs":[{"output_type":"stream","name":"stdout","text":["tensor([0, 1, 2, 1, 2])\n","{}\n"]}],"source":["# get back dna\n","dna = dna_ptr.get()\n","print(dna)\n","\n","# And as you can see... clinic no longer has the tensor dna anymore!!! It has moved back to our machine!\n","print(harmony_clinic._objects)"]},{"cell_type":"code","execution_count":5,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"2hpF1HRnY1Tv","outputId":"ba084b08-0049-464f-d662-68197a584f75","executionInfo":{"status":"ok","timestamp":1645532659367,"user_tz":-120,"elapsed":24,"user":{"displayName":"Vasilis Petrakopoulos","photoUrl":"https://lh3.googleusercontent.com/a/default-user=s64","userId":"11664790255209564953"}}},"outputs":[{"output_type":"stream","name":"stdout","text":["tensor([2.4000, 6.2000], requires_grad=True)\n","tensor([1., 1.])\n"]}],"source":["# we create two tensors and send them to clinic\n","train = torch.tensor([2.4, 6.2], requires_grad=True).send(harmony_clinic)\n","label = torch.tensor([2, 6.]).send(harmony_clinic)\n","\n","# we apply some function, in this case a rather simple one, just to show the idea, we use L1 loss\n","loss = (train-label).abs().sum()\n","\n","# Yes, even .backward() works when working with Pointers\n","loss.backward()\n","\n","# now we retreive back the train tensor\n","train = train.get()\n","\n","print(train)\n","\n","# If everything went well, we will see gradients accumulated \n","# in .grad attribute of train\n","print(train.grad)"]},{"cell_type":"markdown","metadata":{"id":"AKNKzhn9YVE0"},"source":["## FL on MNIST dataset"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"eVP2vWh8pzsS"},"outputs":[],"source":["import torch\n","import torch.nn as nn\n","import torch.nn.functional as F\n","import torch.optim as optim\n","from torchvision import datasets, transforms\n","import logging\n","\n","# import Pysyft to help us to simulate federated learning\n","import syft as sy\n","\n","# hook PyTorch to PySyft, i.e. add extra functionalities to support Federated Learning and other private AI tools\n","hook = sy.TorchHook(torch) \n","\n","# create clients\n","clients = []\n","clients.append(sy.VirtualWorker(hook, id=\"bob\"))\n","clients.append(sy.VirtualWorker(hook, id=\"alice\"))\n","clients.append(sy.VirtualWorker(hook, id=\"charlie\"))"]},{"cell_type":"code","execution_count":7,"metadata":{"id":"fd5-982uY-6L","executionInfo":{"status":"ok","timestamp":1645532659710,"user_tz":-120,"elapsed":363,"user":{"displayName":"Vasilis Petrakopoulos","photoUrl":"https://lh3.googleusercontent.com/a/default-user=s64","userId":"11664790255209564953"}}},"outputs":[],"source":["# define the args\n","args = {\n","    'use_cuda' : True,\n","    'batch_size' : 64,\n","    'test_batch_size' : 1000,\n","    'lr' : 0.01,\n","    'log_interval' : 10,\n","    # with federated learning convergence is faster and less epochs are needed\n","    'epochs' : 7\n","}\n","\n","# check to use GPU or not\n","use_cuda = args['use_cuda'] and torch.cuda.is_available()\n","device = torch.device(\"cuda\" if use_cuda else \"cpu\")"]},{"cell_type":"code","execution_count":8,"metadata":{"id":"VShbMriNZC6I","executionInfo":{"status":"ok","timestamp":1645532659713,"user_tz":-120,"elapsed":7,"user":{"displayName":"Vasilis Petrakopoulos","photoUrl":"https://lh3.googleusercontent.com/a/default-user=s64","userId":"11664790255209564953"}}},"outputs":[],"source":["# create a simple CNN net\n","class Net(nn.Module):\n","    \n","    def __init__(self):\n","        super(Net, self).__init__()\n","        \n","        self.conv = nn.Sequential(\n","            nn.Conv2d(in_channels = 1, out_channels = 32, kernel_size = 3, stride = 1),\n","            nn.ReLU(),\n","            nn.Conv2d(in_channels=32,out_channels = 64, kernel_size = 3, stride = 1),\n","            nn.ReLU()\n","        )\n","        \n","        self.fc = nn.Sequential(\n","            nn.Linear(in_features=64*12*12, out_features=128),\n","            nn.ReLU(),\n","            nn.Linear(in_features=128, out_features=10),\n","        )\n","\n","        self.dropout = nn.Dropout2d(0.25)\n","    \n","    def forward(self, x):\n","        x = self.conv(x)\n","        x = F.max_pool2d(x,2)\n","        x = x.view(-1, 64*12*12)\n","        x = self.fc(x)\n","        x = F.log_softmax(x, dim=1)\n","        return x"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"_McSsgBEZq1f"},"outputs":[],"source":["# prepare and distribute the data across workers\n","# normally there is no need to distribute data, since it is already at the clients\n","# this is more of a simulation of federated learning\n","federated_train_loader = sy.FederatedDataLoader(\n","    datasets.MNIST('../data', train=True, download=True,\n","                   transform=transforms.Compose([\n","                       transforms.ToTensor(),\n","                       transforms.Normalize((0.1307,), (0.3081,))\n","                   ]))\n","    .federate(tuple(clients)),\n","    batch_size=args['batch_size'], shuffle=True)\n","\n","# test data remains at the central entity\n","test_loader = torch.utils.data.DataLoader(\n","        datasets.MNIST('../data', train=False, transform=transforms.Compose([\n","                           transforms.ToTensor(),\n","                           transforms.Normalize((0.1307,), (0.3081,))\n","                       ])),\n","        batch_size=args['test_batch_size'], shuffle=True)"]},{"cell_type":"code","execution_count":10,"metadata":{"id":"bP_p7GrAlXgB","executionInfo":{"status":"ok","timestamp":1645532698536,"user_tz":-120,"elapsed":21,"user":{"displayName":"Vasilis Petrakopoulos","photoUrl":"https://lh3.googleusercontent.com/a/default-user=s64","userId":"11664790255209564953"}}},"outputs":[],"source":["# classic torch code for training except for the federated part\n","def train_locally(args, models, device, train_loader, optimizers, epoch):\n","    for c, m in models.items():\n","        m.train()\n","        # send models to workers\n","        m.send(c)\n","\n","    # iterate over federated data\n","    for batch_idx, (data, target) in enumerate(train_loader):\n","        data, target = data.to(device), target.to(device)\n","\n","        optimizers[data.location].zero_grad()\n","        output = models[data.location](data)\n","        # loss is a ptr to the tensor loss at the remote location\n","        loss = F.nll_loss(output, target)\n","        # call backward() on the loss ptr, that will send the command to call\n","        # backward on the actual loss tensor present on the remote machine\n","        loss.backward()\n","        optimizers[data.location].step()\n","\n","        if batch_idx % args['log_interval'] == 0:\n","\n","            # get back loss, that was created at remote worker\n","            loss = loss.get()\n","\n","            print('Train Epoch: {} [{}/{} ({:.0f}%)]\\tLoss: {:.6f}\\tWorker: {}'.format(\n","                    epoch, \n","                    batch_idx * args['batch_size'], # no of images done\n","                    len(train_loader) * args['batch_size'], # total images left\n","                    100. * batch_idx / len(train_loader),\n","                    loss,\n","                    data.location.id\n","                )\n","            )\n","    \n","    # get back models for aggregation\n","    for m in models.values():\n","        m = m.get()"]},{"cell_type":"code","execution_count":11,"metadata":{"id":"cHZpA0PtaKtW","executionInfo":{"status":"ok","timestamp":1645532698537,"user_tz":-120,"elapsed":20,"user":{"displayName":"Vasilis Petrakopoulos","photoUrl":"https://lh3.googleusercontent.com/a/default-user=s64","userId":"11664790255209564953"}}},"outputs":[],"source":["# classic torch code for testing\n","def test(model, device, test_loader):\n","    model.eval()\n","    test_loss = 0\n","    correct = 0\n","    with torch.no_grad():\n","        for data, target in test_loader:\n","            data, target = data.to(device), target.to(device)\n","            output = model(data)\n","\n","            # add losses together\n","            test_loss += F.nll_loss(output, target, reduction='sum').item() \n","\n","            # get the index of the max probability class\n","            pred = output.argmax(dim=1, keepdim=True)  \n","            correct += pred.eq(target.view_as(pred)).sum().item()\n","\n","    test_loss /= len(test_loader.dataset)\n","\n","    print('\\nTest set: Average loss: {:.4f}, Accuracy: {}/{} ({:.0f}%)\\n'.format(\n","        test_loss, correct, len(test_loader.dataset),\n","        100. * correct / len(test_loader.dataset)))"]},{"cell_type":"code","execution_count":12,"metadata":{"id":"Wmz8eKE3bYrt","executionInfo":{"status":"ok","timestamp":1645532698538,"user_tz":-120,"elapsed":20,"user":{"displayName":"Vasilis Petrakopoulos","photoUrl":"https://lh3.googleusercontent.com/a/default-user=s64","userId":"11664790255209564953"}}},"outputs":[],"source":["def aggregate(central_model, models, weights):\n","    # firstly compute mean weight values\n","    with torch.no_grad():\n","        for m in models.values():\n","            weights['conv0_mean_weight'] += m.conv[0].weight.data.clone()\n","            weights['conv0_mean_bias'] += m.conv[0].bias.data.clone()\n","            weights['conv2_mean_weight'] += m.conv[2].weight.data.clone()\n","            weights['conv2_mean_bias'] += m.conv[2].bias.data.clone()            \n","            weights['fc0_mean_weight'] += m.fc[0].weight.data.clone()\n","            weights['fc0_mean_bias'] += m.fc[0].bias.data.clone()\n","            weights['fc2_mean_weight'] += m.fc[2].weight.data.clone()\n","            weights['fc2_mean_bias'] += m.fc[2].bias.data.clone()            \n","\n","        weights['conv0_mean_weight'] = weights['conv0_mean_weight']/len(clients)\n","        weights['conv0_mean_bias'] = weights['conv0_mean_bias']/len(clients)\n","        weights['conv2_mean_weight'] = weights['conv2_mean_weight']/len(clients)\n","        weights['conv2_mean_bias'] = weights['conv2_mean_bias']/len(clients)\n","        weights['fc0_mean_weight'] = weights['fc0_mean_weight']/len(clients)\n","        weights['fc0_mean_bias'] = weights['fc0_mean_bias']/len(clients)\n","        weights['fc2_mean_weight'] = weights['fc2_mean_weight']/len(clients)\n","        weights['fc2_mean_bias'] = weights['fc2_mean_bias']/len(clients)\n","\n","        # then copy them to the local models\n","        for m in models.values():\n","            m.conv[0].weight.data = weights['conv0_mean_weight'].data.clone()\n","            m.conv[0].bias.data = weights['conv0_mean_bias'].data.clone()\n","            m.conv[2].weight.data = weights['conv2_mean_weight'].data.clone()\n","            m.conv[2].bias.data = weights['conv2_mean_bias'].data.clone()\n","            m.fc[0].weight.data = weights['fc0_mean_weight'].data.clone()\n","            m.fc[0].bias.data = weights['fc0_mean_bias'].data.clone()\n","            m.fc[2].weight.data = weights['fc2_mean_weight'].data.clone()\n","            m.fc[2].bias.data = weights['fc2_mean_bias'].data.clone()\n","\n","        # and to the central model for the test set\n","        central_model.conv[0].weight.data = weights['conv0_mean_weight'].data.clone()\n","        central_model.conv[0].bias.data = weights['conv0_mean_bias'].data.clone()\n","        central_model.conv[2].weight.data = weights['conv2_mean_weight'].data.clone()\n","        central_model.conv[2].bias.data = weights['conv2_mean_bias'].data.clone()\n","        central_model.fc[0].weight.data = weights['fc0_mean_weight'].data.clone()\n","        central_model.fc[0].bias.data = weights['fc0_mean_bias'].data.clone()\n","        central_model.fc[2].weight.data = weights['fc2_mean_weight'].data.clone()\n","        central_model.fc[2].bias.data = weights['fc2_mean_bias'].data.clone()"]},{"cell_type":"code","execution_count":13,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":1053517,"status":"ok","timestamp":1645533752037,"user":{"displayName":"Vasilis Petrakopoulos","photoUrl":"https://lh3.googleusercontent.com/a/default-user=s64","userId":"11664790255209564953"},"user_tz":-120},"id":"B_jH4dE8FVs1","outputId":"39048e34-fc8c-4308-dd18-03fac711769e"},"outputs":[{"output_type":"stream","name":"stdout","text":["Train Epoch: 1 [0/60032 (0%)]\tLoss: 2.279968\tWorker: bob\n","Train Epoch: 1 [640/60032 (1%)]\tLoss: 2.185375\tWorker: bob\n","Train Epoch: 1 [1280/60032 (2%)]\tLoss: 1.963375\tWorker: bob\n","Train Epoch: 1 [1920/60032 (3%)]\tLoss: 1.664402\tWorker: bob\n","Train Epoch: 1 [2560/60032 (4%)]\tLoss: 1.220316\tWorker: bob\n","Train Epoch: 1 [3200/60032 (5%)]\tLoss: 0.809935\tWorker: bob\n","Train Epoch: 1 [3840/60032 (6%)]\tLoss: 0.718656\tWorker: bob\n","Train Epoch: 1 [4480/60032 (7%)]\tLoss: 0.623755\tWorker: bob\n","Train Epoch: 1 [5120/60032 (9%)]\tLoss: 0.710627\tWorker: bob\n","Train Epoch: 1 [5760/60032 (10%)]\tLoss: 0.559490\tWorker: bob\n","Train Epoch: 1 [6400/60032 (11%)]\tLoss: 0.619238\tWorker: bob\n","Train Epoch: 1 [7040/60032 (12%)]\tLoss: 0.659039\tWorker: bob\n","Train Epoch: 1 [7680/60032 (13%)]\tLoss: 0.412358\tWorker: bob\n","Train Epoch: 1 [8320/60032 (14%)]\tLoss: 0.412839\tWorker: bob\n","Train Epoch: 1 [8960/60032 (15%)]\tLoss: 0.478337\tWorker: bob\n","Train Epoch: 1 [9600/60032 (16%)]\tLoss: 0.329867\tWorker: bob\n","Train Epoch: 1 [10240/60032 (17%)]\tLoss: 0.378060\tWorker: bob\n","Train Epoch: 1 [10880/60032 (18%)]\tLoss: 0.465908\tWorker: bob\n","Train Epoch: 1 [11520/60032 (19%)]\tLoss: 0.297851\tWorker: bob\n","Train Epoch: 1 [12160/60032 (20%)]\tLoss: 0.322249\tWorker: bob\n","Train Epoch: 1 [12800/60032 (21%)]\tLoss: 0.310137\tWorker: bob\n","Train Epoch: 1 [13440/60032 (22%)]\tLoss: 0.568840\tWorker: bob\n","Train Epoch: 1 [14080/60032 (23%)]\tLoss: 0.360705\tWorker: bob\n","Train Epoch: 1 [14720/60032 (25%)]\tLoss: 0.428869\tWorker: bob\n","Train Epoch: 1 [15360/60032 (26%)]\tLoss: 0.346598\tWorker: bob\n","Train Epoch: 1 [16000/60032 (27%)]\tLoss: 0.259513\tWorker: bob\n","Train Epoch: 1 [16640/60032 (28%)]\tLoss: 0.344243\tWorker: bob\n","Train Epoch: 1 [17280/60032 (29%)]\tLoss: 0.203755\tWorker: bob\n","Train Epoch: 1 [17920/60032 (30%)]\tLoss: 0.421983\tWorker: bob\n","Train Epoch: 1 [18560/60032 (31%)]\tLoss: 0.363670\tWorker: bob\n","Train Epoch: 1 [19200/60032 (32%)]\tLoss: 0.225853\tWorker: bob\n","Train Epoch: 1 [19840/60032 (33%)]\tLoss: 0.245731\tWorker: bob\n","Train Epoch: 1 [20480/60032 (34%)]\tLoss: 2.215872\tWorker: alice\n","Train Epoch: 1 [21120/60032 (35%)]\tLoss: 2.022123\tWorker: alice\n","Train Epoch: 1 [21760/60032 (36%)]\tLoss: 1.794998\tWorker: alice\n","Train Epoch: 1 [22400/60032 (37%)]\tLoss: 1.442749\tWorker: alice\n","Train Epoch: 1 [23040/60032 (38%)]\tLoss: 0.941680\tWorker: alice\n","Train Epoch: 1 [23680/60032 (39%)]\tLoss: 0.829201\tWorker: alice\n","Train Epoch: 1 [24320/60032 (41%)]\tLoss: 0.656844\tWorker: alice\n","Train Epoch: 1 [24960/60032 (42%)]\tLoss: 0.608039\tWorker: alice\n","Train Epoch: 1 [25600/60032 (43%)]\tLoss: 0.542867\tWorker: alice\n","Train Epoch: 1 [26240/60032 (44%)]\tLoss: 0.447435\tWorker: alice\n","Train Epoch: 1 [26880/60032 (45%)]\tLoss: 0.431763\tWorker: alice\n","Train Epoch: 1 [27520/60032 (46%)]\tLoss: 0.616842\tWorker: alice\n","Train Epoch: 1 [28160/60032 (47%)]\tLoss: 0.515623\tWorker: alice\n","Train Epoch: 1 [28800/60032 (48%)]\tLoss: 0.347945\tWorker: alice\n","Train Epoch: 1 [29440/60032 (49%)]\tLoss: 0.494610\tWorker: alice\n","Train Epoch: 1 [30080/60032 (50%)]\tLoss: 0.367565\tWorker: alice\n","Train Epoch: 1 [30720/60032 (51%)]\tLoss: 0.388058\tWorker: alice\n","Train Epoch: 1 [31360/60032 (52%)]\tLoss: 0.402376\tWorker: alice\n","Train Epoch: 1 [32000/60032 (53%)]\tLoss: 0.450586\tWorker: alice\n","Train Epoch: 1 [32640/60032 (54%)]\tLoss: 0.351420\tWorker: alice\n","Train Epoch: 1 [33280/60032 (55%)]\tLoss: 0.286451\tWorker: alice\n","Train Epoch: 1 [33920/60032 (57%)]\tLoss: 0.559025\tWorker: alice\n","Train Epoch: 1 [34560/60032 (58%)]\tLoss: 0.381913\tWorker: alice\n","Train Epoch: 1 [35200/60032 (59%)]\tLoss: 0.266708\tWorker: alice\n","Train Epoch: 1 [35840/60032 (60%)]\tLoss: 0.354158\tWorker: alice\n","Train Epoch: 1 [36480/60032 (61%)]\tLoss: 0.355305\tWorker: alice\n","Train Epoch: 1 [37120/60032 (62%)]\tLoss: 0.488440\tWorker: alice\n","Train Epoch: 1 [37760/60032 (63%)]\tLoss: 0.514619\tWorker: alice\n","Train Epoch: 1 [38400/60032 (64%)]\tLoss: 0.296367\tWorker: alice\n","Train Epoch: 1 [39040/60032 (65%)]\tLoss: 0.309073\tWorker: alice\n","Train Epoch: 1 [39680/60032 (66%)]\tLoss: 0.281765\tWorker: alice\n","Train Epoch: 1 [40320/60032 (67%)]\tLoss: 2.275351\tWorker: charlie\n","Train Epoch: 1 [40960/60032 (68%)]\tLoss: 2.243897\tWorker: charlie\n","Train Epoch: 1 [41600/60032 (69%)]\tLoss: 2.190919\tWorker: charlie\n","Train Epoch: 1 [42240/60032 (70%)]\tLoss: 2.082954\tWorker: charlie\n","Train Epoch: 1 [42880/60032 (71%)]\tLoss: 1.891099\tWorker: charlie\n","Train Epoch: 1 [43520/60032 (72%)]\tLoss: 1.657526\tWorker: charlie\n","Train Epoch: 1 [44160/60032 (74%)]\tLoss: 1.441275\tWorker: charlie\n","Train Epoch: 1 [44800/60032 (75%)]\tLoss: 1.056198\tWorker: charlie\n","Train Epoch: 1 [45440/60032 (76%)]\tLoss: 0.743574\tWorker: charlie\n","Train Epoch: 1 [46080/60032 (77%)]\tLoss: 0.778330\tWorker: charlie\n","Train Epoch: 1 [46720/60032 (78%)]\tLoss: 0.639557\tWorker: charlie\n","Train Epoch: 1 [47360/60032 (79%)]\tLoss: 0.463589\tWorker: charlie\n","Train Epoch: 1 [48000/60032 (80%)]\tLoss: 0.576271\tWorker: charlie\n","Train Epoch: 1 [48640/60032 (81%)]\tLoss: 0.611434\tWorker: charlie\n","Train Epoch: 1 [49280/60032 (82%)]\tLoss: 0.369889\tWorker: charlie\n","Train Epoch: 1 [49920/60032 (83%)]\tLoss: 0.292000\tWorker: charlie\n","Train Epoch: 1 [50560/60032 (84%)]\tLoss: 0.580652\tWorker: charlie\n","Train Epoch: 1 [51200/60032 (85%)]\tLoss: 0.416897\tWorker: charlie\n","Train Epoch: 1 [51840/60032 (86%)]\tLoss: 0.359089\tWorker: charlie\n","Train Epoch: 1 [52480/60032 (87%)]\tLoss: 0.377400\tWorker: charlie\n","Train Epoch: 1 [53120/60032 (88%)]\tLoss: 0.226825\tWorker: charlie\n","Train Epoch: 1 [53760/60032 (90%)]\tLoss: 0.229107\tWorker: charlie\n","Train Epoch: 1 [54400/60032 (91%)]\tLoss: 0.457958\tWorker: charlie\n","Train Epoch: 1 [55040/60032 (92%)]\tLoss: 0.462138\tWorker: charlie\n","Train Epoch: 1 [55680/60032 (93%)]\tLoss: 0.553414\tWorker: charlie\n","Train Epoch: 1 [56320/60032 (94%)]\tLoss: 0.332130\tWorker: charlie\n","Train Epoch: 1 [56960/60032 (95%)]\tLoss: 0.509775\tWorker: charlie\n","Train Epoch: 1 [57600/60032 (96%)]\tLoss: 0.364544\tWorker: charlie\n","Train Epoch: 1 [58240/60032 (97%)]\tLoss: 0.249395\tWorker: charlie\n","Train Epoch: 1 [58880/60032 (98%)]\tLoss: 0.354864\tWorker: charlie\n","Train Epoch: 1 [59520/60032 (99%)]\tLoss: 0.142188\tWorker: charlie\n","\n","Test set: Average loss: 1.8007, Accuracy: 7564/10000 (76%)\n","\n","Train Epoch: 2 [0/60032 (0%)]\tLoss: 1.801035\tWorker: bob\n","Train Epoch: 2 [640/60032 (1%)]\tLoss: 1.491879\tWorker: bob\n","Train Epoch: 2 [1280/60032 (2%)]\tLoss: 1.025551\tWorker: bob\n","Train Epoch: 2 [1920/60032 (3%)]\tLoss: 0.780934\tWorker: bob\n","Train Epoch: 2 [2560/60032 (4%)]\tLoss: 0.687871\tWorker: bob\n","Train Epoch: 2 [3200/60032 (5%)]\tLoss: 0.423957\tWorker: bob\n","Train Epoch: 2 [3840/60032 (6%)]\tLoss: 0.557964\tWorker: bob\n","Train Epoch: 2 [4480/60032 (7%)]\tLoss: 0.325696\tWorker: bob\n","Train Epoch: 2 [5120/60032 (9%)]\tLoss: 0.543362\tWorker: bob\n","Train Epoch: 2 [5760/60032 (10%)]\tLoss: 0.436249\tWorker: bob\n","Train Epoch: 2 [6400/60032 (11%)]\tLoss: 0.320802\tWorker: bob\n","Train Epoch: 2 [7040/60032 (12%)]\tLoss: 0.699225\tWorker: bob\n","Train Epoch: 2 [7680/60032 (13%)]\tLoss: 0.344476\tWorker: bob\n","Train Epoch: 2 [8320/60032 (14%)]\tLoss: 0.578886\tWorker: bob\n","Train Epoch: 2 [8960/60032 (15%)]\tLoss: 0.478613\tWorker: bob\n","Train Epoch: 2 [9600/60032 (16%)]\tLoss: 0.216863\tWorker: bob\n","Train Epoch: 2 [10240/60032 (17%)]\tLoss: 0.264748\tWorker: bob\n","Train Epoch: 2 [10880/60032 (18%)]\tLoss: 0.385136\tWorker: bob\n","Train Epoch: 2 [11520/60032 (19%)]\tLoss: 0.542782\tWorker: bob\n","Train Epoch: 2 [12160/60032 (20%)]\tLoss: 0.334158\tWorker: bob\n","Train Epoch: 2 [12800/60032 (21%)]\tLoss: 0.323176\tWorker: bob\n","Train Epoch: 2 [13440/60032 (22%)]\tLoss: 0.265431\tWorker: bob\n","Train Epoch: 2 [14080/60032 (23%)]\tLoss: 0.346411\tWorker: bob\n","Train Epoch: 2 [14720/60032 (25%)]\tLoss: 0.387115\tWorker: bob\n","Train Epoch: 2 [15360/60032 (26%)]\tLoss: 0.292950\tWorker: bob\n","Train Epoch: 2 [16000/60032 (27%)]\tLoss: 0.292978\tWorker: bob\n","Train Epoch: 2 [16640/60032 (28%)]\tLoss: 0.346094\tWorker: bob\n","Train Epoch: 2 [17280/60032 (29%)]\tLoss: 0.283286\tWorker: bob\n","Train Epoch: 2 [17920/60032 (30%)]\tLoss: 0.274241\tWorker: bob\n","Train Epoch: 2 [18560/60032 (31%)]\tLoss: 0.275166\tWorker: bob\n","Train Epoch: 2 [19200/60032 (32%)]\tLoss: 0.264166\tWorker: bob\n","Train Epoch: 2 [19840/60032 (33%)]\tLoss: 0.564530\tWorker: bob\n","Train Epoch: 2 [20480/60032 (34%)]\tLoss: 1.489716\tWorker: alice\n","Train Epoch: 2 [21120/60032 (35%)]\tLoss: 1.110488\tWorker: alice\n","Train Epoch: 2 [21760/60032 (36%)]\tLoss: 0.954244\tWorker: alice\n","Train Epoch: 2 [22400/60032 (37%)]\tLoss: 0.599622\tWorker: alice\n","Train Epoch: 2 [23040/60032 (38%)]\tLoss: 0.565057\tWorker: alice\n","Train Epoch: 2 [23680/60032 (39%)]\tLoss: 0.609969\tWorker: alice\n","Train Epoch: 2 [24320/60032 (41%)]\tLoss: 0.673486\tWorker: alice\n","Train Epoch: 2 [24960/60032 (42%)]\tLoss: 0.478489\tWorker: alice\n","Train Epoch: 2 [25600/60032 (43%)]\tLoss: 0.269519\tWorker: alice\n","Train Epoch: 2 [26240/60032 (44%)]\tLoss: 0.253020\tWorker: alice\n","Train Epoch: 2 [26880/60032 (45%)]\tLoss: 0.557634\tWorker: alice\n","Train Epoch: 2 [27520/60032 (46%)]\tLoss: 0.398843\tWorker: alice\n","Train Epoch: 2 [28160/60032 (47%)]\tLoss: 0.233329\tWorker: alice\n","Train Epoch: 2 [28800/60032 (48%)]\tLoss: 0.242615\tWorker: alice\n","Train Epoch: 2 [29440/60032 (49%)]\tLoss: 0.353928\tWorker: alice\n","Train Epoch: 2 [30080/60032 (50%)]\tLoss: 0.256712\tWorker: alice\n","Train Epoch: 2 [30720/60032 (51%)]\tLoss: 0.304848\tWorker: alice\n","Train Epoch: 2 [31360/60032 (52%)]\tLoss: 0.318330\tWorker: alice\n","Train Epoch: 2 [32000/60032 (53%)]\tLoss: 0.255577\tWorker: alice\n","Train Epoch: 2 [32640/60032 (54%)]\tLoss: 0.393456\tWorker: alice\n","Train Epoch: 2 [33280/60032 (55%)]\tLoss: 0.593499\tWorker: alice\n","Train Epoch: 2 [33920/60032 (57%)]\tLoss: 0.276222\tWorker: alice\n","Train Epoch: 2 [34560/60032 (58%)]\tLoss: 0.469117\tWorker: alice\n","Train Epoch: 2 [35200/60032 (59%)]\tLoss: 0.259525\tWorker: alice\n","Train Epoch: 2 [35840/60032 (60%)]\tLoss: 0.415650\tWorker: alice\n","Train Epoch: 2 [36480/60032 (61%)]\tLoss: 0.406313\tWorker: alice\n","Train Epoch: 2 [37120/60032 (62%)]\tLoss: 0.245584\tWorker: alice\n","Train Epoch: 2 [37760/60032 (63%)]\tLoss: 0.487772\tWorker: alice\n","Train Epoch: 2 [38400/60032 (64%)]\tLoss: 0.418673\tWorker: alice\n","Train Epoch: 2 [39040/60032 (65%)]\tLoss: 0.552585\tWorker: alice\n","Train Epoch: 2 [39680/60032 (66%)]\tLoss: 0.492783\tWorker: alice\n","Train Epoch: 2 [40320/60032 (67%)]\tLoss: 1.720988\tWorker: charlie\n","Train Epoch: 2 [40960/60032 (68%)]\tLoss: 1.279149\tWorker: charlie\n","Train Epoch: 2 [41600/60032 (69%)]\tLoss: 0.798342\tWorker: charlie\n","Train Epoch: 2 [42240/60032 (70%)]\tLoss: 0.648631\tWorker: charlie\n","Train Epoch: 2 [42880/60032 (71%)]\tLoss: 0.684979\tWorker: charlie\n","Train Epoch: 2 [43520/60032 (72%)]\tLoss: 0.549094\tWorker: charlie\n","Train Epoch: 2 [44160/60032 (74%)]\tLoss: 0.482292\tWorker: charlie\n","Train Epoch: 2 [44800/60032 (75%)]\tLoss: 0.615984\tWorker: charlie\n","Train Epoch: 2 [45440/60032 (76%)]\tLoss: 0.329957\tWorker: charlie\n","Train Epoch: 2 [46080/60032 (77%)]\tLoss: 0.742403\tWorker: charlie\n","Train Epoch: 2 [46720/60032 (78%)]\tLoss: 0.571724\tWorker: charlie\n","Train Epoch: 2 [47360/60032 (79%)]\tLoss: 0.439489\tWorker: charlie\n","Train Epoch: 2 [48000/60032 (80%)]\tLoss: 0.474172\tWorker: charlie\n","Train Epoch: 2 [48640/60032 (81%)]\tLoss: 0.422402\tWorker: charlie\n","Train Epoch: 2 [49280/60032 (82%)]\tLoss: 0.713343\tWorker: charlie\n","Train Epoch: 2 [49920/60032 (83%)]\tLoss: 0.333377\tWorker: charlie\n","Train Epoch: 2 [50560/60032 (84%)]\tLoss: 0.274413\tWorker: charlie\n","Train Epoch: 2 [51200/60032 (85%)]\tLoss: 0.519531\tWorker: charlie\n","Train Epoch: 2 [51840/60032 (86%)]\tLoss: 0.363606\tWorker: charlie\n","Train Epoch: 2 [52480/60032 (87%)]\tLoss: 0.430681\tWorker: charlie\n","Train Epoch: 2 [53120/60032 (88%)]\tLoss: 0.428993\tWorker: charlie\n","Train Epoch: 2 [53760/60032 (90%)]\tLoss: 0.610128\tWorker: charlie\n","Train Epoch: 2 [54400/60032 (91%)]\tLoss: 0.459924\tWorker: charlie\n","Train Epoch: 2 [55040/60032 (92%)]\tLoss: 0.292336\tWorker: charlie\n","Train Epoch: 2 [55680/60032 (93%)]\tLoss: 0.206416\tWorker: charlie\n","Train Epoch: 2 [56320/60032 (94%)]\tLoss: 0.212901\tWorker: charlie\n","Train Epoch: 2 [56960/60032 (95%)]\tLoss: 0.302934\tWorker: charlie\n","Train Epoch: 2 [57600/60032 (96%)]\tLoss: 0.324621\tWorker: charlie\n","Train Epoch: 2 [58240/60032 (97%)]\tLoss: 0.267731\tWorker: charlie\n","Train Epoch: 2 [58880/60032 (98%)]\tLoss: 0.422707\tWorker: charlie\n","Train Epoch: 2 [59520/60032 (99%)]\tLoss: 0.300673\tWorker: charlie\n","\n","Test set: Average loss: 0.3769, Accuracy: 9108/10000 (91%)\n","\n","Train Epoch: 3 [0/60032 (0%)]\tLoss: 0.659347\tWorker: bob\n","Train Epoch: 3 [640/60032 (1%)]\tLoss: 0.586055\tWorker: bob\n","Train Epoch: 3 [1280/60032 (2%)]\tLoss: 0.206473\tWorker: bob\n","Train Epoch: 3 [1920/60032 (3%)]\tLoss: 0.283245\tWorker: bob\n","Train Epoch: 3 [2560/60032 (4%)]\tLoss: 0.207201\tWorker: bob\n","Train Epoch: 3 [3200/60032 (5%)]\tLoss: 0.309306\tWorker: bob\n","Train Epoch: 3 [3840/60032 (6%)]\tLoss: 0.216200\tWorker: bob\n","Train Epoch: 3 [4480/60032 (7%)]\tLoss: 0.265650\tWorker: bob\n","Train Epoch: 3 [5120/60032 (9%)]\tLoss: 0.486667\tWorker: bob\n","Train Epoch: 3 [5760/60032 (10%)]\tLoss: 0.188772\tWorker: bob\n","Train Epoch: 3 [6400/60032 (11%)]\tLoss: 0.264135\tWorker: bob\n","Train Epoch: 3 [7040/60032 (12%)]\tLoss: 0.343500\tWorker: bob\n","Train Epoch: 3 [7680/60032 (13%)]\tLoss: 0.153536\tWorker: bob\n","Train Epoch: 3 [8320/60032 (14%)]\tLoss: 0.155004\tWorker: bob\n","Train Epoch: 3 [8960/60032 (15%)]\tLoss: 0.187944\tWorker: bob\n","Train Epoch: 3 [9600/60032 (16%)]\tLoss: 0.192999\tWorker: bob\n","Train Epoch: 3 [10240/60032 (17%)]\tLoss: 0.203566\tWorker: bob\n","Train Epoch: 3 [10880/60032 (18%)]\tLoss: 0.369233\tWorker: bob\n","Train Epoch: 3 [11520/60032 (19%)]\tLoss: 0.265020\tWorker: bob\n","Train Epoch: 3 [12160/60032 (20%)]\tLoss: 0.187998\tWorker: bob\n","Train Epoch: 3 [12800/60032 (21%)]\tLoss: 0.362003\tWorker: bob\n","Train Epoch: 3 [13440/60032 (22%)]\tLoss: 0.262964\tWorker: bob\n","Train Epoch: 3 [14080/60032 (23%)]\tLoss: 0.224413\tWorker: bob\n","Train Epoch: 3 [14720/60032 (25%)]\tLoss: 0.176175\tWorker: bob\n","Train Epoch: 3 [15360/60032 (26%)]\tLoss: 0.123243\tWorker: bob\n","Train Epoch: 3 [16000/60032 (27%)]\tLoss: 0.184818\tWorker: bob\n","Train Epoch: 3 [16640/60032 (28%)]\tLoss: 0.259554\tWorker: bob\n","Train Epoch: 3 [17280/60032 (29%)]\tLoss: 0.215180\tWorker: bob\n","Train Epoch: 3 [17920/60032 (30%)]\tLoss: 0.282247\tWorker: bob\n","Train Epoch: 3 [18560/60032 (31%)]\tLoss: 0.270860\tWorker: bob\n","Train Epoch: 3 [19200/60032 (32%)]\tLoss: 0.184726\tWorker: bob\n","Train Epoch: 3 [19840/60032 (33%)]\tLoss: 0.181822\tWorker: bob\n","Train Epoch: 3 [20480/60032 (34%)]\tLoss: 1.096899\tWorker: alice\n","Train Epoch: 3 [21120/60032 (35%)]\tLoss: 0.575557\tWorker: alice\n","Train Epoch: 3 [21760/60032 (36%)]\tLoss: 0.304987\tWorker: alice\n","Train Epoch: 3 [22400/60032 (37%)]\tLoss: 0.451407\tWorker: alice\n","Train Epoch: 3 [23040/60032 (38%)]\tLoss: 0.655673\tWorker: alice\n","Train Epoch: 3 [23680/60032 (39%)]\tLoss: 0.264223\tWorker: alice\n","Train Epoch: 3 [24320/60032 (41%)]\tLoss: 0.316187\tWorker: alice\n","Train Epoch: 3 [24960/60032 (42%)]\tLoss: 0.402418\tWorker: alice\n","Train Epoch: 3 [25600/60032 (43%)]\tLoss: 0.249747\tWorker: alice\n","Train Epoch: 3 [26240/60032 (44%)]\tLoss: 0.311485\tWorker: alice\n","Train Epoch: 3 [26880/60032 (45%)]\tLoss: 0.258178\tWorker: alice\n","Train Epoch: 3 [27520/60032 (46%)]\tLoss: 0.180331\tWorker: alice\n","Train Epoch: 3 [28160/60032 (47%)]\tLoss: 0.233873\tWorker: alice\n","Train Epoch: 3 [28800/60032 (48%)]\tLoss: 0.126040\tWorker: alice\n","Train Epoch: 3 [29440/60032 (49%)]\tLoss: 0.226039\tWorker: alice\n","Train Epoch: 3 [30080/60032 (50%)]\tLoss: 0.309793\tWorker: alice\n","Train Epoch: 3 [30720/60032 (51%)]\tLoss: 0.383736\tWorker: alice\n","Train Epoch: 3 [31360/60032 (52%)]\tLoss: 0.205903\tWorker: alice\n","Train Epoch: 3 [32000/60032 (53%)]\tLoss: 0.162977\tWorker: alice\n","Train Epoch: 3 [32640/60032 (54%)]\tLoss: 0.207222\tWorker: alice\n","Train Epoch: 3 [33280/60032 (55%)]\tLoss: 0.283626\tWorker: alice\n","Train Epoch: 3 [33920/60032 (57%)]\tLoss: 0.534145\tWorker: alice\n","Train Epoch: 3 [34560/60032 (58%)]\tLoss: 0.101703\tWorker: alice\n","Train Epoch: 3 [35200/60032 (59%)]\tLoss: 0.248805\tWorker: alice\n","Train Epoch: 3 [35840/60032 (60%)]\tLoss: 0.222198\tWorker: alice\n","Train Epoch: 3 [36480/60032 (61%)]\tLoss: 0.296827\tWorker: alice\n","Train Epoch: 3 [37120/60032 (62%)]\tLoss: 0.530648\tWorker: alice\n","Train Epoch: 3 [37760/60032 (63%)]\tLoss: 0.149849\tWorker: alice\n","Train Epoch: 3 [38400/60032 (64%)]\tLoss: 0.294381\tWorker: alice\n","Train Epoch: 3 [39040/60032 (65%)]\tLoss: 0.268665\tWorker: alice\n","Train Epoch: 3 [39680/60032 (66%)]\tLoss: 0.224944\tWorker: alice\n","Train Epoch: 3 [40320/60032 (67%)]\tLoss: 0.557260\tWorker: charlie\n","Train Epoch: 3 [40960/60032 (68%)]\tLoss: 0.239967\tWorker: charlie\n","Train Epoch: 3 [41600/60032 (69%)]\tLoss: 0.269305\tWorker: charlie\n","Train Epoch: 3 [42240/60032 (70%)]\tLoss: 0.751255\tWorker: charlie\n","Train Epoch: 3 [42880/60032 (71%)]\tLoss: 0.103000\tWorker: charlie\n","Train Epoch: 3 [43520/60032 (72%)]\tLoss: 0.279283\tWorker: charlie\n","Train Epoch: 3 [44160/60032 (74%)]\tLoss: 0.395688\tWorker: charlie\n","Train Epoch: 3 [44800/60032 (75%)]\tLoss: 0.262775\tWorker: charlie\n","Train Epoch: 3 [45440/60032 (76%)]\tLoss: 0.405904\tWorker: charlie\n","Train Epoch: 3 [46080/60032 (77%)]\tLoss: 0.341979\tWorker: charlie\n","Train Epoch: 3 [46720/60032 (78%)]\tLoss: 0.272399\tWorker: charlie\n","Train Epoch: 3 [47360/60032 (79%)]\tLoss: 0.270588\tWorker: charlie\n","Train Epoch: 3 [48000/60032 (80%)]\tLoss: 0.158198\tWorker: charlie\n","Train Epoch: 3 [48640/60032 (81%)]\tLoss: 0.328803\tWorker: charlie\n","Train Epoch: 3 [49280/60032 (82%)]\tLoss: 0.388673\tWorker: charlie\n","Train Epoch: 3 [49920/60032 (83%)]\tLoss: 0.163904\tWorker: charlie\n","Train Epoch: 3 [50560/60032 (84%)]\tLoss: 0.383762\tWorker: charlie\n","Train Epoch: 3 [51200/60032 (85%)]\tLoss: 0.135639\tWorker: charlie\n","Train Epoch: 3 [51840/60032 (86%)]\tLoss: 0.273338\tWorker: charlie\n","Train Epoch: 3 [52480/60032 (87%)]\tLoss: 0.192697\tWorker: charlie\n","Train Epoch: 3 [53120/60032 (88%)]\tLoss: 0.265733\tWorker: charlie\n","Train Epoch: 3 [53760/60032 (90%)]\tLoss: 0.445586\tWorker: charlie\n","Train Epoch: 3 [54400/60032 (91%)]\tLoss: 0.216575\tWorker: charlie\n","Train Epoch: 3 [55040/60032 (92%)]\tLoss: 0.358055\tWorker: charlie\n","Train Epoch: 3 [55680/60032 (93%)]\tLoss: 0.333609\tWorker: charlie\n","Train Epoch: 3 [56320/60032 (94%)]\tLoss: 0.117553\tWorker: charlie\n","Train Epoch: 3 [56960/60032 (95%)]\tLoss: 0.365200\tWorker: charlie\n","Train Epoch: 3 [57600/60032 (96%)]\tLoss: 0.279558\tWorker: charlie\n","Train Epoch: 3 [58240/60032 (97%)]\tLoss: 0.383681\tWorker: charlie\n","Train Epoch: 3 [58880/60032 (98%)]\tLoss: 0.146180\tWorker: charlie\n","Train Epoch: 3 [59520/60032 (99%)]\tLoss: 0.345761\tWorker: charlie\n","\n","Test set: Average loss: 0.5059, Accuracy: 9284/10000 (93%)\n","\n","Train Epoch: 4 [0/60032 (0%)]\tLoss: 0.298031\tWorker: bob\n","Train Epoch: 4 [640/60032 (1%)]\tLoss: 0.584001\tWorker: bob\n","Train Epoch: 4 [1280/60032 (2%)]\tLoss: 0.134513\tWorker: bob\n","Train Epoch: 4 [1920/60032 (3%)]\tLoss: 0.295082\tWorker: bob\n","Train Epoch: 4 [2560/60032 (4%)]\tLoss: 0.570808\tWorker: bob\n","Train Epoch: 4 [3200/60032 (5%)]\tLoss: 0.161330\tWorker: bob\n","Train Epoch: 4 [3840/60032 (6%)]\tLoss: 0.321870\tWorker: bob\n","Train Epoch: 4 [4480/60032 (7%)]\tLoss: 0.118140\tWorker: bob\n","Train Epoch: 4 [5120/60032 (9%)]\tLoss: 0.130482\tWorker: bob\n","Train Epoch: 4 [5760/60032 (10%)]\tLoss: 0.310648\tWorker: bob\n","Train Epoch: 4 [6400/60032 (11%)]\tLoss: 0.104344\tWorker: bob\n","Train Epoch: 4 [7040/60032 (12%)]\tLoss: 0.210221\tWorker: bob\n","Train Epoch: 4 [7680/60032 (13%)]\tLoss: 0.206485\tWorker: bob\n","Train Epoch: 4 [8320/60032 (14%)]\tLoss: 0.174145\tWorker: bob\n","Train Epoch: 4 [8960/60032 (15%)]\tLoss: 0.176315\tWorker: bob\n","Train Epoch: 4 [9600/60032 (16%)]\tLoss: 0.275601\tWorker: bob\n","Train Epoch: 4 [10240/60032 (17%)]\tLoss: 0.752872\tWorker: bob\n","Train Epoch: 4 [10880/60032 (18%)]\tLoss: 0.124112\tWorker: bob\n","Train Epoch: 4 [11520/60032 (19%)]\tLoss: 0.196973\tWorker: bob\n","Train Epoch: 4 [12160/60032 (20%)]\tLoss: 0.358215\tWorker: bob\n","Train Epoch: 4 [12800/60032 (21%)]\tLoss: 0.100177\tWorker: bob\n","Train Epoch: 4 [13440/60032 (22%)]\tLoss: 0.436694\tWorker: bob\n","Train Epoch: 4 [14080/60032 (23%)]\tLoss: 0.146377\tWorker: bob\n","Train Epoch: 4 [14720/60032 (25%)]\tLoss: 0.163100\tWorker: bob\n","Train Epoch: 4 [15360/60032 (26%)]\tLoss: 0.232972\tWorker: bob\n","Train Epoch: 4 [16000/60032 (27%)]\tLoss: 0.150059\tWorker: bob\n","Train Epoch: 4 [16640/60032 (28%)]\tLoss: 0.126456\tWorker: bob\n","Train Epoch: 4 [17280/60032 (29%)]\tLoss: 0.211697\tWorker: bob\n","Train Epoch: 4 [17920/60032 (30%)]\tLoss: 0.133846\tWorker: bob\n","Train Epoch: 4 [18560/60032 (31%)]\tLoss: 0.149623\tWorker: bob\n","Train Epoch: 4 [19200/60032 (32%)]\tLoss: 0.229903\tWorker: bob\n","Train Epoch: 4 [19840/60032 (33%)]\tLoss: 0.096738\tWorker: bob\n","Train Epoch: 4 [20480/60032 (34%)]\tLoss: 1.297710\tWorker: alice\n","Train Epoch: 4 [21120/60032 (35%)]\tLoss: 0.478714\tWorker: alice\n","Train Epoch: 4 [21760/60032 (36%)]\tLoss: 0.416970\tWorker: alice\n","Train Epoch: 4 [22400/60032 (37%)]\tLoss: 0.231912\tWorker: alice\n","Train Epoch: 4 [23040/60032 (38%)]\tLoss: 0.437689\tWorker: alice\n","Train Epoch: 4 [23680/60032 (39%)]\tLoss: 0.383799\tWorker: alice\n","Train Epoch: 4 [24320/60032 (41%)]\tLoss: 0.341033\tWorker: alice\n","Train Epoch: 4 [24960/60032 (42%)]\tLoss: 0.291567\tWorker: alice\n","Train Epoch: 4 [25600/60032 (43%)]\tLoss: 0.258344\tWorker: alice\n","Train Epoch: 4 [26240/60032 (44%)]\tLoss: 0.121197\tWorker: alice\n","Train Epoch: 4 [26880/60032 (45%)]\tLoss: 0.253490\tWorker: alice\n","Train Epoch: 4 [27520/60032 (46%)]\tLoss: 0.206788\tWorker: alice\n","Train Epoch: 4 [28160/60032 (47%)]\tLoss: 0.194090\tWorker: alice\n","Train Epoch: 4 [28800/60032 (48%)]\tLoss: 0.568661\tWorker: alice\n","Train Epoch: 4 [29440/60032 (49%)]\tLoss: 0.179846\tWorker: alice\n","Train Epoch: 4 [30080/60032 (50%)]\tLoss: 0.378858\tWorker: alice\n","Train Epoch: 4 [30720/60032 (51%)]\tLoss: 0.314713\tWorker: alice\n","Train Epoch: 4 [31360/60032 (52%)]\tLoss: 0.159851\tWorker: alice\n","Train Epoch: 4 [32000/60032 (53%)]\tLoss: 0.085548\tWorker: alice\n","Train Epoch: 4 [32640/60032 (54%)]\tLoss: 0.281314\tWorker: alice\n","Train Epoch: 4 [33280/60032 (55%)]\tLoss: 0.118743\tWorker: alice\n","Train Epoch: 4 [33920/60032 (57%)]\tLoss: 0.200185\tWorker: alice\n","Train Epoch: 4 [34560/60032 (58%)]\tLoss: 0.169237\tWorker: alice\n","Train Epoch: 4 [35200/60032 (59%)]\tLoss: 0.076899\tWorker: alice\n","Train Epoch: 4 [35840/60032 (60%)]\tLoss: 0.146400\tWorker: alice\n","Train Epoch: 4 [36480/60032 (61%)]\tLoss: 0.380010\tWorker: alice\n","Train Epoch: 4 [37120/60032 (62%)]\tLoss: 0.194682\tWorker: alice\n","Train Epoch: 4 [37760/60032 (63%)]\tLoss: 0.187992\tWorker: alice\n","Train Epoch: 4 [38400/60032 (64%)]\tLoss: 0.136256\tWorker: alice\n","Train Epoch: 4 [39040/60032 (65%)]\tLoss: 0.132208\tWorker: alice\n","Train Epoch: 4 [39680/60032 (66%)]\tLoss: 0.322024\tWorker: alice\n","Train Epoch: 4 [40320/60032 (67%)]\tLoss: 3.655045\tWorker: charlie\n","Train Epoch: 4 [40960/60032 (68%)]\tLoss: 0.095522\tWorker: charlie\n","Train Epoch: 4 [41600/60032 (69%)]\tLoss: 0.277819\tWorker: charlie\n","Train Epoch: 4 [42240/60032 (70%)]\tLoss: 0.177821\tWorker: charlie\n","Train Epoch: 4 [42880/60032 (71%)]\tLoss: 0.356111\tWorker: charlie\n","Train Epoch: 4 [43520/60032 (72%)]\tLoss: 0.266060\tWorker: charlie\n","Train Epoch: 4 [44160/60032 (74%)]\tLoss: 0.170045\tWorker: charlie\n","Train Epoch: 4 [44800/60032 (75%)]\tLoss: 0.167638\tWorker: charlie\n","Train Epoch: 4 [45440/60032 (76%)]\tLoss: 0.186636\tWorker: charlie\n","Train Epoch: 4 [46080/60032 (77%)]\tLoss: 0.304342\tWorker: charlie\n","Train Epoch: 4 [46720/60032 (78%)]\tLoss: 0.109207\tWorker: charlie\n","Train Epoch: 4 [47360/60032 (79%)]\tLoss: 0.263690\tWorker: charlie\n","Train Epoch: 4 [48000/60032 (80%)]\tLoss: 0.272027\tWorker: charlie\n","Train Epoch: 4 [48640/60032 (81%)]\tLoss: 0.398095\tWorker: charlie\n","Train Epoch: 4 [49280/60032 (82%)]\tLoss: 0.343358\tWorker: charlie\n","Train Epoch: 4 [49920/60032 (83%)]\tLoss: 0.198184\tWorker: charlie\n","Train Epoch: 4 [50560/60032 (84%)]\tLoss: 0.190077\tWorker: charlie\n","Train Epoch: 4 [51200/60032 (85%)]\tLoss: 0.170657\tWorker: charlie\n","Train Epoch: 4 [51840/60032 (86%)]\tLoss: 0.178645\tWorker: charlie\n","Train Epoch: 4 [52480/60032 (87%)]\tLoss: 0.306306\tWorker: charlie\n","Train Epoch: 4 [53120/60032 (88%)]\tLoss: 0.214607\tWorker: charlie\n","Train Epoch: 4 [53760/60032 (90%)]\tLoss: 0.167459\tWorker: charlie\n","Train Epoch: 4 [54400/60032 (91%)]\tLoss: 0.272243\tWorker: charlie\n","Train Epoch: 4 [55040/60032 (92%)]\tLoss: 0.108620\tWorker: charlie\n","Train Epoch: 4 [55680/60032 (93%)]\tLoss: 0.143724\tWorker: charlie\n","Train Epoch: 4 [56320/60032 (94%)]\tLoss: 0.153513\tWorker: charlie\n","Train Epoch: 4 [56960/60032 (95%)]\tLoss: 0.115074\tWorker: charlie\n","Train Epoch: 4 [57600/60032 (96%)]\tLoss: 0.102165\tWorker: charlie\n","Train Epoch: 4 [58240/60032 (97%)]\tLoss: 0.100652\tWorker: charlie\n","Train Epoch: 4 [58880/60032 (98%)]\tLoss: 0.471205\tWorker: charlie\n","Train Epoch: 4 [59520/60032 (99%)]\tLoss: 0.147132\tWorker: charlie\n","\n","Test set: Average loss: 0.4838, Accuracy: 9403/10000 (94%)\n","\n","Train Epoch: 5 [0/60032 (0%)]\tLoss: 1.366794\tWorker: bob\n","Train Epoch: 5 [640/60032 (1%)]\tLoss: 0.745702\tWorker: bob\n","Train Epoch: 5 [1280/60032 (2%)]\tLoss: 0.709040\tWorker: bob\n","Train Epoch: 5 [1920/60032 (3%)]\tLoss: 0.103778\tWorker: bob\n","Train Epoch: 5 [2560/60032 (4%)]\tLoss: 0.265310\tWorker: bob\n","Train Epoch: 5 [3200/60032 (5%)]\tLoss: 0.225050\tWorker: bob\n","Train Epoch: 5 [3840/60032 (6%)]\tLoss: 0.370404\tWorker: bob\n","Train Epoch: 5 [4480/60032 (7%)]\tLoss: 0.121703\tWorker: bob\n","Train Epoch: 5 [5120/60032 (9%)]\tLoss: 0.438843\tWorker: bob\n","Train Epoch: 5 [5760/60032 (10%)]\tLoss: 0.429113\tWorker: bob\n","Train Epoch: 5 [6400/60032 (11%)]\tLoss: 0.232754\tWorker: bob\n","Train Epoch: 5 [7040/60032 (12%)]\tLoss: 0.180775\tWorker: bob\n","Train Epoch: 5 [7680/60032 (13%)]\tLoss: 0.129506\tWorker: bob\n","Train Epoch: 5 [8320/60032 (14%)]\tLoss: 0.351561\tWorker: bob\n","Train Epoch: 5 [8960/60032 (15%)]\tLoss: 0.155124\tWorker: bob\n","Train Epoch: 5 [9600/60032 (16%)]\tLoss: 0.213248\tWorker: bob\n","Train Epoch: 5 [10240/60032 (17%)]\tLoss: 0.324041\tWorker: bob\n","Train Epoch: 5 [10880/60032 (18%)]\tLoss: 0.151372\tWorker: bob\n","Train Epoch: 5 [11520/60032 (19%)]\tLoss: 0.142734\tWorker: bob\n","Train Epoch: 5 [12160/60032 (20%)]\tLoss: 0.208548\tWorker: bob\n","Train Epoch: 5 [12800/60032 (21%)]\tLoss: 0.123985\tWorker: bob\n","Train Epoch: 5 [13440/60032 (22%)]\tLoss: 0.412109\tWorker: bob\n","Train Epoch: 5 [14080/60032 (23%)]\tLoss: 0.103980\tWorker: bob\n","Train Epoch: 5 [14720/60032 (25%)]\tLoss: 0.464354\tWorker: bob\n","Train Epoch: 5 [15360/60032 (26%)]\tLoss: 0.110569\tWorker: bob\n","Train Epoch: 5 [16000/60032 (27%)]\tLoss: 0.107384\tWorker: bob\n","Train Epoch: 5 [16640/60032 (28%)]\tLoss: 0.177459\tWorker: bob\n","Train Epoch: 5 [17280/60032 (29%)]\tLoss: 0.257361\tWorker: bob\n","Train Epoch: 5 [17920/60032 (30%)]\tLoss: 0.228468\tWorker: bob\n","Train Epoch: 5 [18560/60032 (31%)]\tLoss: 0.101941\tWorker: bob\n","Train Epoch: 5 [19200/60032 (32%)]\tLoss: 0.120639\tWorker: bob\n","Train Epoch: 5 [19840/60032 (33%)]\tLoss: 0.114378\tWorker: bob\n","Train Epoch: 5 [20480/60032 (34%)]\tLoss: 0.634143\tWorker: alice\n","Train Epoch: 5 [21120/60032 (35%)]\tLoss: 0.694990\tWorker: alice\n","Train Epoch: 5 [21760/60032 (36%)]\tLoss: 0.127160\tWorker: alice\n","Train Epoch: 5 [22400/60032 (37%)]\tLoss: 0.462108\tWorker: alice\n","Train Epoch: 5 [23040/60032 (38%)]\tLoss: 0.147741\tWorker: alice\n","Train Epoch: 5 [23680/60032 (39%)]\tLoss: 0.219396\tWorker: alice\n","Train Epoch: 5 [24320/60032 (41%)]\tLoss: 0.417836\tWorker: alice\n","Train Epoch: 5 [24960/60032 (42%)]\tLoss: 0.531083\tWorker: alice\n","Train Epoch: 5 [25600/60032 (43%)]\tLoss: 0.378210\tWorker: alice\n","Train Epoch: 5 [26240/60032 (44%)]\tLoss: 0.157086\tWorker: alice\n","Train Epoch: 5 [26880/60032 (45%)]\tLoss: 0.389461\tWorker: alice\n","Train Epoch: 5 [27520/60032 (46%)]\tLoss: 0.314007\tWorker: alice\n","Train Epoch: 5 [28160/60032 (47%)]\tLoss: 0.312503\tWorker: alice\n","Train Epoch: 5 [28800/60032 (48%)]\tLoss: 0.125504\tWorker: alice\n","Train Epoch: 5 [29440/60032 (49%)]\tLoss: 0.472279\tWorker: alice\n","Train Epoch: 5 [30080/60032 (50%)]\tLoss: 0.118867\tWorker: alice\n","Train Epoch: 5 [30720/60032 (51%)]\tLoss: 0.051647\tWorker: alice\n","Train Epoch: 5 [31360/60032 (52%)]\tLoss: 0.285202\tWorker: alice\n","Train Epoch: 5 [32000/60032 (53%)]\tLoss: 0.262528\tWorker: alice\n","Train Epoch: 5 [32640/60032 (54%)]\tLoss: 0.614446\tWorker: alice\n","Train Epoch: 5 [33280/60032 (55%)]\tLoss: 0.052729\tWorker: alice\n","Train Epoch: 5 [33920/60032 (57%)]\tLoss: 0.472666\tWorker: alice\n","Train Epoch: 5 [34560/60032 (58%)]\tLoss: 0.240294\tWorker: alice\n","Train Epoch: 5 [35200/60032 (59%)]\tLoss: 0.183093\tWorker: alice\n","Train Epoch: 5 [35840/60032 (60%)]\tLoss: 0.135331\tWorker: alice\n","Train Epoch: 5 [36480/60032 (61%)]\tLoss: 0.284393\tWorker: alice\n","Train Epoch: 5 [37120/60032 (62%)]\tLoss: 0.074196\tWorker: alice\n","Train Epoch: 5 [37760/60032 (63%)]\tLoss: 0.217237\tWorker: alice\n","Train Epoch: 5 [38400/60032 (64%)]\tLoss: 0.316457\tWorker: alice\n","Train Epoch: 5 [39040/60032 (65%)]\tLoss: 0.168217\tWorker: alice\n","Train Epoch: 5 [39680/60032 (66%)]\tLoss: 0.141575\tWorker: alice\n","Train Epoch: 5 [40320/60032 (67%)]\tLoss: 1.367326\tWorker: charlie\n","Train Epoch: 5 [40960/60032 (68%)]\tLoss: 0.730727\tWorker: charlie\n","Train Epoch: 5 [41600/60032 (69%)]\tLoss: 0.206189\tWorker: charlie\n","Train Epoch: 5 [42240/60032 (70%)]\tLoss: 0.357374\tWorker: charlie\n","Train Epoch: 5 [42880/60032 (71%)]\tLoss: 0.077224\tWorker: charlie\n","Train Epoch: 5 [43520/60032 (72%)]\tLoss: 0.180159\tWorker: charlie\n","Train Epoch: 5 [44160/60032 (74%)]\tLoss: 0.535568\tWorker: charlie\n","Train Epoch: 5 [44800/60032 (75%)]\tLoss: 0.138616\tWorker: charlie\n","Train Epoch: 5 [45440/60032 (76%)]\tLoss: 0.293632\tWorker: charlie\n","Train Epoch: 5 [46080/60032 (77%)]\tLoss: 0.257127\tWorker: charlie\n","Train Epoch: 5 [46720/60032 (78%)]\tLoss: 0.159149\tWorker: charlie\n","Train Epoch: 5 [47360/60032 (79%)]\tLoss: 0.296146\tWorker: charlie\n","Train Epoch: 5 [48000/60032 (80%)]\tLoss: 0.095103\tWorker: charlie\n","Train Epoch: 5 [48640/60032 (81%)]\tLoss: 0.328980\tWorker: charlie\n","Train Epoch: 5 [49280/60032 (82%)]\tLoss: 0.170644\tWorker: charlie\n","Train Epoch: 5 [49920/60032 (83%)]\tLoss: 0.180358\tWorker: charlie\n","Train Epoch: 5 [50560/60032 (84%)]\tLoss: 0.173801\tWorker: charlie\n","Train Epoch: 5 [51200/60032 (85%)]\tLoss: 0.328513\tWorker: charlie\n","Train Epoch: 5 [51840/60032 (86%)]\tLoss: 0.216808\tWorker: charlie\n","Train Epoch: 5 [52480/60032 (87%)]\tLoss: 0.088614\tWorker: charlie\n","Train Epoch: 5 [53120/60032 (88%)]\tLoss: 0.180195\tWorker: charlie\n","Train Epoch: 5 [53760/60032 (90%)]\tLoss: 0.254225\tWorker: charlie\n","Train Epoch: 5 [54400/60032 (91%)]\tLoss: 0.348027\tWorker: charlie\n","Train Epoch: 5 [55040/60032 (92%)]\tLoss: 0.074157\tWorker: charlie\n","Train Epoch: 5 [55680/60032 (93%)]\tLoss: 0.327485\tWorker: charlie\n","Train Epoch: 5 [56320/60032 (94%)]\tLoss: 0.272031\tWorker: charlie\n","Train Epoch: 5 [56960/60032 (95%)]\tLoss: 0.344426\tWorker: charlie\n","Train Epoch: 5 [57600/60032 (96%)]\tLoss: 0.149653\tWorker: charlie\n","Train Epoch: 5 [58240/60032 (97%)]\tLoss: 0.193942\tWorker: charlie\n","Train Epoch: 5 [58880/60032 (98%)]\tLoss: 0.129499\tWorker: charlie\n","Train Epoch: 5 [59520/60032 (99%)]\tLoss: 0.046926\tWorker: charlie\n","\n","Test set: Average loss: 0.4675, Accuracy: 9379/10000 (94%)\n","\n","Train Epoch: 6 [0/60032 (0%)]\tLoss: 0.295181\tWorker: bob\n","Train Epoch: 6 [640/60032 (1%)]\tLoss: 1.782604\tWorker: bob\n","Train Epoch: 6 [1280/60032 (2%)]\tLoss: 22.575735\tWorker: bob\n","Train Epoch: 6 [1920/60032 (3%)]\tLoss: 0.318324\tWorker: bob\n","Train Epoch: 6 [2560/60032 (4%)]\tLoss: 0.479496\tWorker: bob\n","Train Epoch: 6 [3200/60032 (5%)]\tLoss: 0.176965\tWorker: bob\n","Train Epoch: 6 [3840/60032 (6%)]\tLoss: 0.159818\tWorker: bob\n","Train Epoch: 6 [4480/60032 (7%)]\tLoss: 0.087768\tWorker: bob\n","Train Epoch: 6 [5120/60032 (9%)]\tLoss: 0.147299\tWorker: bob\n","Train Epoch: 6 [5760/60032 (10%)]\tLoss: 0.069634\tWorker: bob\n","Train Epoch: 6 [6400/60032 (11%)]\tLoss: 0.188279\tWorker: bob\n","Train Epoch: 6 [7040/60032 (12%)]\tLoss: 0.178667\tWorker: bob\n","Train Epoch: 6 [7680/60032 (13%)]\tLoss: 0.176667\tWorker: bob\n","Train Epoch: 6 [8320/60032 (14%)]\tLoss: 0.342584\tWorker: bob\n","Train Epoch: 6 [8960/60032 (15%)]\tLoss: 0.172421\tWorker: bob\n","Train Epoch: 6 [9600/60032 (16%)]\tLoss: 0.099389\tWorker: bob\n","Train Epoch: 6 [10240/60032 (17%)]\tLoss: 0.084604\tWorker: bob\n","Train Epoch: 6 [10880/60032 (18%)]\tLoss: 0.339553\tWorker: bob\n","Train Epoch: 6 [11520/60032 (19%)]\tLoss: 0.201187\tWorker: bob\n","Train Epoch: 6 [12160/60032 (20%)]\tLoss: 0.087306\tWorker: bob\n","Train Epoch: 6 [12800/60032 (21%)]\tLoss: 0.128388\tWorker: bob\n","Train Epoch: 6 [13440/60032 (22%)]\tLoss: 0.076353\tWorker: bob\n","Train Epoch: 6 [14080/60032 (23%)]\tLoss: 0.168569\tWorker: bob\n","Train Epoch: 6 [14720/60032 (25%)]\tLoss: 0.154070\tWorker: bob\n","Train Epoch: 6 [15360/60032 (26%)]\tLoss: 0.074728\tWorker: bob\n","Train Epoch: 6 [16000/60032 (27%)]\tLoss: 0.118813\tWorker: bob\n","Train Epoch: 6 [16640/60032 (28%)]\tLoss: 0.182734\tWorker: bob\n","Train Epoch: 6 [17280/60032 (29%)]\tLoss: 0.172496\tWorker: bob\n","Train Epoch: 6 [17920/60032 (30%)]\tLoss: 0.175494\tWorker: bob\n","Train Epoch: 6 [18560/60032 (31%)]\tLoss: 0.164641\tWorker: bob\n","Train Epoch: 6 [19200/60032 (32%)]\tLoss: 0.151056\tWorker: bob\n","Train Epoch: 6 [19840/60032 (33%)]\tLoss: 0.160686\tWorker: bob\n","Train Epoch: 6 [20480/60032 (34%)]\tLoss: 0.685759\tWorker: alice\n","Train Epoch: 6 [21120/60032 (35%)]\tLoss: 17.431114\tWorker: alice\n","Train Epoch: 6 [21760/60032 (36%)]\tLoss: 1.140202\tWorker: alice\n","Train Epoch: 6 [22400/60032 (37%)]\tLoss: 0.395008\tWorker: alice\n","Train Epoch: 6 [23040/60032 (38%)]\tLoss: 0.162895\tWorker: alice\n","Train Epoch: 6 [23680/60032 (39%)]\tLoss: 0.213324\tWorker: alice\n","Train Epoch: 6 [24320/60032 (41%)]\tLoss: 0.274199\tWorker: alice\n","Train Epoch: 6 [24960/60032 (42%)]\tLoss: 0.265003\tWorker: alice\n","Train Epoch: 6 [25600/60032 (43%)]\tLoss: 0.146092\tWorker: alice\n","Train Epoch: 6 [26240/60032 (44%)]\tLoss: 0.242899\tWorker: alice\n","Train Epoch: 6 [26880/60032 (45%)]\tLoss: 0.135294\tWorker: alice\n","Train Epoch: 6 [27520/60032 (46%)]\tLoss: 0.110193\tWorker: alice\n","Train Epoch: 6 [28160/60032 (47%)]\tLoss: 0.245086\tWorker: alice\n","Train Epoch: 6 [28800/60032 (48%)]\tLoss: 0.224721\tWorker: alice\n","Train Epoch: 6 [29440/60032 (49%)]\tLoss: 0.218840\tWorker: alice\n","Train Epoch: 6 [30080/60032 (50%)]\tLoss: 0.151490\tWorker: alice\n","Train Epoch: 6 [30720/60032 (51%)]\tLoss: 0.197451\tWorker: alice\n","Train Epoch: 6 [31360/60032 (52%)]\tLoss: 0.137469\tWorker: alice\n","Train Epoch: 6 [32000/60032 (53%)]\tLoss: 0.209347\tWorker: alice\n","Train Epoch: 6 [32640/60032 (54%)]\tLoss: 0.232272\tWorker: alice\n","Train Epoch: 6 [33280/60032 (55%)]\tLoss: 0.097304\tWorker: alice\n","Train Epoch: 6 [33920/60032 (57%)]\tLoss: 0.164867\tWorker: alice\n","Train Epoch: 6 [34560/60032 (58%)]\tLoss: 0.368273\tWorker: alice\n","Train Epoch: 6 [35200/60032 (59%)]\tLoss: 0.259123\tWorker: alice\n","Train Epoch: 6 [35840/60032 (60%)]\tLoss: 0.236227\tWorker: alice\n","Train Epoch: 6 [36480/60032 (61%)]\tLoss: 0.181300\tWorker: alice\n","Train Epoch: 6 [37120/60032 (62%)]\tLoss: 0.237104\tWorker: alice\n","Train Epoch: 6 [37760/60032 (63%)]\tLoss: 0.218149\tWorker: alice\n","Train Epoch: 6 [38400/60032 (64%)]\tLoss: 0.100490\tWorker: alice\n","Train Epoch: 6 [39040/60032 (65%)]\tLoss: 0.035598\tWorker: alice\n","Train Epoch: 6 [39680/60032 (66%)]\tLoss: 0.115661\tWorker: alice\n","Train Epoch: 6 [40320/60032 (67%)]\tLoss: 0.522541\tWorker: charlie\n","Train Epoch: 6 [40960/60032 (68%)]\tLoss: 3.068741\tWorker: charlie\n","Train Epoch: 6 [41600/60032 (69%)]\tLoss: 0.297620\tWorker: charlie\n","Train Epoch: 6 [42240/60032 (70%)]\tLoss: 0.799785\tWorker: charlie\n","Train Epoch: 6 [42880/60032 (71%)]\tLoss: 0.724884\tWorker: charlie\n","Train Epoch: 6 [43520/60032 (72%)]\tLoss: 0.700703\tWorker: charlie\n","Train Epoch: 6 [44160/60032 (74%)]\tLoss: 0.123719\tWorker: charlie\n","Train Epoch: 6 [44800/60032 (75%)]\tLoss: 0.234915\tWorker: charlie\n","Train Epoch: 6 [45440/60032 (76%)]\tLoss: 0.164115\tWorker: charlie\n","Train Epoch: 6 [46080/60032 (77%)]\tLoss: 0.310234\tWorker: charlie\n","Train Epoch: 6 [46720/60032 (78%)]\tLoss: 0.284758\tWorker: charlie\n","Train Epoch: 6 [47360/60032 (79%)]\tLoss: 0.223958\tWorker: charlie\n","Train Epoch: 6 [48000/60032 (80%)]\tLoss: 0.073894\tWorker: charlie\n","Train Epoch: 6 [48640/60032 (81%)]\tLoss: 0.318504\tWorker: charlie\n","Train Epoch: 6 [49280/60032 (82%)]\tLoss: 0.639405\tWorker: charlie\n","Train Epoch: 6 [49920/60032 (83%)]\tLoss: 0.228805\tWorker: charlie\n","Train Epoch: 6 [50560/60032 (84%)]\tLoss: 0.246532\tWorker: charlie\n","Train Epoch: 6 [51200/60032 (85%)]\tLoss: 0.269338\tWorker: charlie\n","Train Epoch: 6 [51840/60032 (86%)]\tLoss: 0.157740\tWorker: charlie\n","Train Epoch: 6 [52480/60032 (87%)]\tLoss: 0.251962\tWorker: charlie\n","Train Epoch: 6 [53120/60032 (88%)]\tLoss: 0.195439\tWorker: charlie\n","Train Epoch: 6 [53760/60032 (90%)]\tLoss: 0.363731\tWorker: charlie\n","Train Epoch: 6 [54400/60032 (91%)]\tLoss: 0.279514\tWorker: charlie\n","Train Epoch: 6 [55040/60032 (92%)]\tLoss: 0.699479\tWorker: charlie\n","Train Epoch: 6 [55680/60032 (93%)]\tLoss: 0.122480\tWorker: charlie\n","Train Epoch: 6 [56320/60032 (94%)]\tLoss: 0.094224\tWorker: charlie\n","Train Epoch: 6 [56960/60032 (95%)]\tLoss: 0.224174\tWorker: charlie\n","Train Epoch: 6 [57600/60032 (96%)]\tLoss: 0.085730\tWorker: charlie\n","Train Epoch: 6 [58240/60032 (97%)]\tLoss: 0.285592\tWorker: charlie\n","Train Epoch: 6 [58880/60032 (98%)]\tLoss: 0.168457\tWorker: charlie\n","Train Epoch: 6 [59520/60032 (99%)]\tLoss: 0.104955\tWorker: charlie\n","\n","Test set: Average loss: 0.6057, Accuracy: 8807/10000 (88%)\n","\n","Train Epoch: 7 [0/60032 (0%)]\tLoss: 1.143146\tWorker: bob\n","Train Epoch: 7 [640/60032 (1%)]\tLoss: 0.572980\tWorker: bob\n","Train Epoch: 7 [1280/60032 (2%)]\tLoss: 0.528765\tWorker: bob\n","Train Epoch: 7 [1920/60032 (3%)]\tLoss: 0.145266\tWorker: bob\n","Train Epoch: 7 [2560/60032 (4%)]\tLoss: 0.362097\tWorker: bob\n","Train Epoch: 7 [3200/60032 (5%)]\tLoss: 0.232170\tWorker: bob\n","Train Epoch: 7 [3840/60032 (6%)]\tLoss: 0.330759\tWorker: bob\n","Train Epoch: 7 [4480/60032 (7%)]\tLoss: 0.710940\tWorker: bob\n","Train Epoch: 7 [5120/60032 (9%)]\tLoss: 0.222779\tWorker: bob\n","Train Epoch: 7 [5760/60032 (10%)]\tLoss: 0.416841\tWorker: bob\n","Train Epoch: 7 [6400/60032 (11%)]\tLoss: 0.944158\tWorker: bob\n","Train Epoch: 7 [7040/60032 (12%)]\tLoss: 0.730665\tWorker: bob\n","Train Epoch: 7 [7680/60032 (13%)]\tLoss: 0.778224\tWorker: bob\n","Train Epoch: 7 [8320/60032 (14%)]\tLoss: 0.189818\tWorker: bob\n","Train Epoch: 7 [8960/60032 (15%)]\tLoss: 0.195629\tWorker: bob\n","Train Epoch: 7 [9600/60032 (16%)]\tLoss: 0.448862\tWorker: bob\n","Train Epoch: 7 [10240/60032 (17%)]\tLoss: 0.361464\tWorker: bob\n","Train Epoch: 7 [10880/60032 (18%)]\tLoss: 0.283369\tWorker: bob\n","Train Epoch: 7 [11520/60032 (19%)]\tLoss: 0.656645\tWorker: bob\n","Train Epoch: 7 [12160/60032 (20%)]\tLoss: 0.253776\tWorker: bob\n","Train Epoch: 7 [12800/60032 (21%)]\tLoss: 0.320039\tWorker: bob\n","Train Epoch: 7 [13440/60032 (22%)]\tLoss: 0.444234\tWorker: bob\n","Train Epoch: 7 [14080/60032 (23%)]\tLoss: 0.193545\tWorker: bob\n","Train Epoch: 7 [14720/60032 (25%)]\tLoss: 0.406876\tWorker: bob\n","Train Epoch: 7 [15360/60032 (26%)]\tLoss: 0.570184\tWorker: bob\n","Train Epoch: 7 [16000/60032 (27%)]\tLoss: 0.062802\tWorker: bob\n","Train Epoch: 7 [16640/60032 (28%)]\tLoss: 0.254234\tWorker: bob\n","Train Epoch: 7 [17280/60032 (29%)]\tLoss: 0.609876\tWorker: bob\n","Train Epoch: 7 [17920/60032 (30%)]\tLoss: 0.036294\tWorker: bob\n","Train Epoch: 7 [18560/60032 (31%)]\tLoss: 0.344031\tWorker: bob\n","Train Epoch: 7 [19200/60032 (32%)]\tLoss: 0.152214\tWorker: bob\n","Train Epoch: 7 [19840/60032 (33%)]\tLoss: 0.341650\tWorker: bob\n","Train Epoch: 7 [20480/60032 (34%)]\tLoss: 0.866959\tWorker: alice\n","Train Epoch: 7 [21120/60032 (35%)]\tLoss: 1.128614\tWorker: alice\n","Train Epoch: 7 [21760/60032 (36%)]\tLoss: 0.355806\tWorker: alice\n","Train Epoch: 7 [22400/60032 (37%)]\tLoss: 0.363951\tWorker: alice\n","Train Epoch: 7 [23040/60032 (38%)]\tLoss: 0.652494\tWorker: alice\n","Train Epoch: 7 [23680/60032 (39%)]\tLoss: 0.495709\tWorker: alice\n","Train Epoch: 7 [24320/60032 (41%)]\tLoss: 0.285934\tWorker: alice\n","Train Epoch: 7 [24960/60032 (42%)]\tLoss: 0.173180\tWorker: alice\n","Train Epoch: 7 [25600/60032 (43%)]\tLoss: 0.690625\tWorker: alice\n","Train Epoch: 7 [26240/60032 (44%)]\tLoss: 0.324208\tWorker: alice\n","Train Epoch: 7 [26880/60032 (45%)]\tLoss: 0.992550\tWorker: alice\n","Train Epoch: 7 [27520/60032 (46%)]\tLoss: 0.185801\tWorker: alice\n","Train Epoch: 7 [28160/60032 (47%)]\tLoss: 0.415624\tWorker: alice\n","Train Epoch: 7 [28800/60032 (48%)]\tLoss: 0.254064\tWorker: alice\n","Train Epoch: 7 [29440/60032 (49%)]\tLoss: 0.356649\tWorker: alice\n","Train Epoch: 7 [30080/60032 (50%)]\tLoss: 0.253183\tWorker: alice\n","Train Epoch: 7 [30720/60032 (51%)]\tLoss: 0.157635\tWorker: alice\n","Train Epoch: 7 [31360/60032 (52%)]\tLoss: 0.404296\tWorker: alice\n","Train Epoch: 7 [32000/60032 (53%)]\tLoss: 0.337126\tWorker: alice\n","Train Epoch: 7 [32640/60032 (54%)]\tLoss: 0.446472\tWorker: alice\n","Train Epoch: 7 [33280/60032 (55%)]\tLoss: 0.582125\tWorker: alice\n","Train Epoch: 7 [33920/60032 (57%)]\tLoss: 0.113102\tWorker: alice\n","Train Epoch: 7 [34560/60032 (58%)]\tLoss: 0.080102\tWorker: alice\n","Train Epoch: 7 [35200/60032 (59%)]\tLoss: 0.362001\tWorker: alice\n","Train Epoch: 7 [35840/60032 (60%)]\tLoss: 0.308097\tWorker: alice\n","Train Epoch: 7 [36480/60032 (61%)]\tLoss: 0.172829\tWorker: alice\n","Train Epoch: 7 [37120/60032 (62%)]\tLoss: 0.435037\tWorker: alice\n","Train Epoch: 7 [37760/60032 (63%)]\tLoss: 0.359221\tWorker: alice\n","Train Epoch: 7 [38400/60032 (64%)]\tLoss: 0.392893\tWorker: alice\n","Train Epoch: 7 [39040/60032 (65%)]\tLoss: 0.269616\tWorker: alice\n","Train Epoch: 7 [39680/60032 (66%)]\tLoss: 0.426927\tWorker: alice\n","Train Epoch: 7 [40320/60032 (67%)]\tLoss: 1.430879\tWorker: charlie\n","Train Epoch: 7 [40960/60032 (68%)]\tLoss: 1.184601\tWorker: charlie\n","Train Epoch: 7 [41600/60032 (69%)]\tLoss: 0.385753\tWorker: charlie\n","Train Epoch: 7 [42240/60032 (70%)]\tLoss: 0.494130\tWorker: charlie\n","Train Epoch: 7 [42880/60032 (71%)]\tLoss: 0.371237\tWorker: charlie\n","Train Epoch: 7 [43520/60032 (72%)]\tLoss: 0.654833\tWorker: charlie\n","Train Epoch: 7 [44160/60032 (74%)]\tLoss: 0.212716\tWorker: charlie\n","Train Epoch: 7 [44800/60032 (75%)]\tLoss: 0.276607\tWorker: charlie\n","Train Epoch: 7 [45440/60032 (76%)]\tLoss: 0.328337\tWorker: charlie\n","Train Epoch: 7 [46080/60032 (77%)]\tLoss: 0.405661\tWorker: charlie\n","Train Epoch: 7 [46720/60032 (78%)]\tLoss: 0.211171\tWorker: charlie\n","Train Epoch: 7 [47360/60032 (79%)]\tLoss: 0.344434\tWorker: charlie\n","Train Epoch: 7 [48000/60032 (80%)]\tLoss: 0.082747\tWorker: charlie\n","Train Epoch: 7 [48640/60032 (81%)]\tLoss: 0.278230\tWorker: charlie\n","Train Epoch: 7 [49280/60032 (82%)]\tLoss: 0.413203\tWorker: charlie\n","Train Epoch: 7 [49920/60032 (83%)]\tLoss: 0.117603\tWorker: charlie\n","Train Epoch: 7 [50560/60032 (84%)]\tLoss: 0.354266\tWorker: charlie\n","Train Epoch: 7 [51200/60032 (85%)]\tLoss: 0.327836\tWorker: charlie\n","Train Epoch: 7 [51840/60032 (86%)]\tLoss: 0.102190\tWorker: charlie\n","Train Epoch: 7 [52480/60032 (87%)]\tLoss: 0.254117\tWorker: charlie\n","Train Epoch: 7 [53120/60032 (88%)]\tLoss: 0.267325\tWorker: charlie\n","Train Epoch: 7 [53760/60032 (90%)]\tLoss: 0.126943\tWorker: charlie\n","Train Epoch: 7 [54400/60032 (91%)]\tLoss: 0.146841\tWorker: charlie\n","Train Epoch: 7 [55040/60032 (92%)]\tLoss: 0.222056\tWorker: charlie\n","Train Epoch: 7 [55680/60032 (93%)]\tLoss: 0.399219\tWorker: charlie\n","Train Epoch: 7 [56320/60032 (94%)]\tLoss: 0.292200\tWorker: charlie\n","Train Epoch: 7 [56960/60032 (95%)]\tLoss: 0.185896\tWorker: charlie\n","Train Epoch: 7 [57600/60032 (96%)]\tLoss: 0.216157\tWorker: charlie\n","Train Epoch: 7 [58240/60032 (97%)]\tLoss: 0.219799\tWorker: charlie\n","Train Epoch: 7 [58880/60032 (98%)]\tLoss: 0.211483\tWorker: charlie\n","Train Epoch: 7 [59520/60032 (99%)]\tLoss: 0.338645\tWorker: charlie\n","\n","Test set: Average loss: 0.5496, Accuracy: 9310/10000 (93%)\n","\n"]}],"source":["# central model\n","central_model = Net().to(device)\n","# optimizer for central model not needed if model is not to be trained\n","#optimizer = optim.SGD(central_model.parameters(), lr=args['lr'])\n","# clients models and optimizers\n","models = {i:Net().to(device) for i in clients}\n","optimizers = {i:optim.SGD(models[i].parameters(), lr=args['lr']) for i in clients}\n","\n","# initialization of dictionary for model aggregation\n","weights = {'conv0_mean_weight' : torch.zeros(size=central_model.conv[0].weight.shape).to(device),\n","           'conv0_mean_bias' : torch.zeros(size=central_model.conv[0].bias.shape).to(device),\n","           'conv2_mean_weight' : torch.zeros(size=central_model.conv[2].weight.shape).to(device),\n","           'conv2_mean_bias' : torch.zeros(size=central_model.conv[2].bias.shape).to(device),\n","           'fc0_mean_weight' : torch.zeros(size=central_model.fc[0].weight.shape).to(device),\n","           'fc0_mean_bias' : torch.zeros(size=central_model.fc[0].bias.shape).to(device),\n","           'fc2_mean_weight' : torch.zeros(size=central_model.fc[2].weight.shape).to(device),\n","           'fc2_mean_bias' : torch.zeros(size=central_model.fc[2].bias.shape).to(device)}\n","\n","logging.info(\"Starting training !!\")\n","\n","for epoch in range(1, args['epochs'] + 1):\n","    train_locally(args, models, device, federated_train_loader, optimizers, epoch)\n","    aggregate(central_model, models, weights)\n","    test(central_model, device, test_loader)"]}],"metadata":{"colab":{"collapsed_sections":["dNwIEBajagKA"],"name":"FL_FedAvg_tutorial.ipynb","provenance":[]},"kernelspec":{"display_name":"Python 3","name":"python3"},"language_info":{"name":"python"},"accelerator":"GPU"},"nbformat":4,"nbformat_minor":0}