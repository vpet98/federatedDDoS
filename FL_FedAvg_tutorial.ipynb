{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "federated_learning_tutorial_test.ipynb",
      "provenance": [],
      "collapsed_sections": [
        "x2WDKGKIYdUp"
      ],
      "toc_visible": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ej63Q9F5VDWy"
      },
      "source": [
        "Tutorial on Federated Learning with FedAvg using PyTorch and PySyft based on https://learnopencv.com/federated-learning-using-pytorch-and-pysyft/ and https://towardsdatascience.com/federated-learning-a-simple-implementation-of-fedavg-federated-averaging-with-pytorch-90187c9c9577"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "dNwIEBajagKA"
      },
      "source": [
        "## Install PySyft and Torchvision"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "p9Z-tl_jWgeE"
      },
      "source": [
        "!pip install syft==0.2.9"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "FQBYDiJbdQHG"
      },
      "source": [
        "!pip install torchvision==0.2.0"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "x2WDKGKIYdUp"
      },
      "source": [
        "## Simple Testing on PySyft (can be skipped)"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "tVNZDzTLVUS8"
      },
      "source": [
        "import torch\n",
        "import syft as sy\n",
        "hook = sy.TorchHook(torch) # add extra functionality to PyTorch"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "MyXfr3LhV91R"
      },
      "source": [
        "# create a machine owned by harmony clinic\n",
        "harmony_clinic = sy.VirtualWorker(hook=hook,id='clinic')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "M75kQXdSXXtC"
      },
      "source": [
        "# we create a Tensor, maybe this is some gene sequence\n",
        "dna = torch.tensor([0,1,2,1,2])\n",
        "\n",
        "# and now I send it, and in turn we get a pointer back that\n",
        "# points to that Tensor\n",
        "dna_ptr = dna.send(harmony_clinic)\n",
        "\n",
        "print(dna_ptr)\n",
        "print(harmony_clinic._objects)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "pmEReTW6YB3d",
        "outputId": "ecdf0b36-68fa-4afe-a937-5786b17ced0b"
      },
      "source": [
        "# get back dna\n",
        "dna = dna_ptr.get()\n",
        "print(dna)\n",
        "\n",
        "# And as you can see... clinic no longer has the tensor dna anymore!!! It has moved back to our machine!\n",
        "print(harmony_clinic._objects)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "tensor([0, 1, 2, 1, 2])\n",
            "{}\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "2hpF1HRnY1Tv",
        "outputId": "89593387-ea4e-4521-c62e-2b38d87a63fa"
      },
      "source": [
        "# we create two tensors and send them to clinic\n",
        "train = torch.tensor([2.4, 6.2], requires_grad=True).send(harmony_clinic)\n",
        "label = torch.tensor([2, 6.]).send(harmony_clinic)\n",
        "\n",
        "# we apply some function, in this case a rather simple one, just to show the idea, we use L1 loss\n",
        "loss = (train-label).abs().sum()\n",
        "\n",
        "# Yes, even .backward() works when working with Pointers\n",
        "loss.backward()\n",
        "\n",
        "# now we retreive back the train tensor\n",
        "train = train.get()\n",
        "\n",
        "print(train)\n",
        "\n",
        "# If everything went well, we will see gradients accumulated \n",
        "# in .grad attribute of train\n",
        "print(train.grad)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "tensor([2.4000, 6.2000], requires_grad=True)\n",
            "tensor([1., 1.])\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "AKNKzhn9YVE0"
      },
      "source": [
        "## FL on MNIST dataset"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "eVP2vWh8pzsS"
      },
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "import torch.optim as optim\n",
        "from torchvision import datasets, transforms\n",
        "import logging\n",
        "\n",
        "# import Pysyft to help us to simulate federated learning\n",
        "import syft as sy\n",
        "\n",
        "# hook PyTorch to PySyft, i.e. add extra functionalities to support Federated Learning and other private AI tools\n",
        "hook = sy.TorchHook(torch) \n",
        "\n",
        "# create clients\n",
        "clients = []\n",
        "clients.append(sy.VirtualWorker(hook, id=\"bob\"))\n",
        "clients.append(sy.VirtualWorker(hook, id=\"alice\"))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "fd5-982uY-6L"
      },
      "source": [
        "# define the args\n",
        "args = {\n",
        "    'use_cuda' : True,\n",
        "    'batch_size' : 64,\n",
        "    'test_batch_size' : 1000,\n",
        "    'lr' : 0.01,\n",
        "    'log_interval' : 10,\n",
        "    # with federated learning convergence is faster and less epochs are needed\n",
        "    'epochs' : 7\n",
        "}\n",
        "\n",
        "# check to use GPU or not\n",
        "use_cuda = args['use_cuda'] and torch.cuda.is_available()\n",
        "device = torch.device(\"cuda\" if use_cuda else \"cpu\")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "VShbMriNZC6I"
      },
      "source": [
        "# create a simple CNN net\n",
        "class Net(nn.Module):\n",
        "    \n",
        "    def __init__(self):\n",
        "        super(Net, self).__init__()\n",
        "        \n",
        "        self.conv = nn.Sequential(\n",
        "            nn.Conv2d(in_channels = 1, out_channels = 32, kernel_size = 3, stride = 1),\n",
        "            nn.ReLU(),\n",
        "            nn.Conv2d(in_channels=32,out_channels = 64, kernel_size = 3, stride = 1),\n",
        "            nn.ReLU()\n",
        "        )\n",
        "        \n",
        "        self.fc = nn.Sequential(\n",
        "            nn.Linear(in_features=64*12*12, out_features=128),\n",
        "            nn.ReLU(),\n",
        "            nn.Linear(in_features=128, out_features=10),\n",
        "        )\n",
        "\n",
        "        self.dropout = nn.Dropout2d(0.25)\n",
        "    \n",
        "    def forward(self, x):\n",
        "        x = self.conv(x)\n",
        "        x = F.max_pool2d(x,2)\n",
        "        x = x.view(-1, 64*12*12)\n",
        "        x = self.fc(x)\n",
        "        x = F.log_softmax(x, dim=1)\n",
        "        return x"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "_McSsgBEZq1f"
      },
      "source": [
        "# prepare and distribute the data across workers\n",
        "# normally there is no need to distribute data, since it is already at the clients\n",
        "# this is more of a simulation of federated learning\n",
        "federated_train_loader = sy.FederatedDataLoader(\n",
        "    datasets.MNIST('../data', train=True, download=True,\n",
        "                   transform=transforms.Compose([\n",
        "                       transforms.ToTensor(),\n",
        "                       transforms.Normalize((0.1307,), (0.3081,))\n",
        "                   ]))\n",
        "    .federate(tuple(clients)),\n",
        "    batch_size=args['batch_size'], shuffle=True)\n",
        "\n",
        "# test data remains at the central entity\n",
        "test_loader = torch.utils.data.DataLoader(\n",
        "        datasets.MNIST('../data', train=False, transform=transforms.Compose([\n",
        "                           transforms.ToTensor(),\n",
        "                           transforms.Normalize((0.1307,), (0.3081,))\n",
        "                       ])),\n",
        "        batch_size=args['test_batch_size'], shuffle=True)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "bP_p7GrAlXgB"
      },
      "source": [
        "# classic torch code for training except for the federated part\n",
        "def train_locally(args, models, device, train_loader, optimizers, epoch):\n",
        "    for c, m in models.items():\n",
        "        m.train()\n",
        "        # send models to workers\n",
        "        m.send(c)\n",
        "\n",
        "    # iterate over federated data\n",
        "    for batch_idx, (data, target) in enumerate(train_loader):\n",
        "        data, target = data.to(device), target.to(device)\n",
        "\n",
        "        optimizers[data.location].zero_grad()\n",
        "        output = models[data.location](data)\n",
        "        # loss is a ptr to the tensor loss at the remote location\n",
        "        loss = F.nll_loss(output, target)\n",
        "        # call backward() on the loss ptr, that will send the command to call\n",
        "        # backward on the actual loss tensor present on the remote machine\n",
        "        loss.backward()\n",
        "        optimizers[data.location].step()\n",
        "\n",
        "        if batch_idx % args['log_interval'] == 0:\n",
        "\n",
        "            # get back loss, that was created at remote worker\n",
        "            loss = loss.get()\n",
        "\n",
        "            print('Train Epoch: {} [{}/{} ({:.0f}%)]\\tLoss: {:.6f}\\tWorker: {}'.format(\n",
        "                    epoch, \n",
        "                    batch_idx * args['batch_size'], # no of images done\n",
        "                    len(train_loader) * args['batch_size'], # total images left\n",
        "                    100. * batch_idx / len(train_loader),\n",
        "                    loss,\n",
        "                    data.location.id\n",
        "                )\n",
        "            )\n",
        "    \n",
        "    # get back models for aggregation\n",
        "    for m in models.values():\n",
        "        m = m.get()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "cHZpA0PtaKtW"
      },
      "source": [
        "# classic torch code for testing\n",
        "def test(model, device, test_loader):\n",
        "    model.eval()\n",
        "    test_loss = 0\n",
        "    correct = 0\n",
        "    with torch.no_grad():\n",
        "        for data, target in test_loader:\n",
        "            data, target = data.to(device), target.to(device)\n",
        "            output = model(data)\n",
        "\n",
        "            # add losses together\n",
        "            test_loss += F.nll_loss(output, target, reduction='sum').item() \n",
        "\n",
        "            # get the index of the max probability class\n",
        "            pred = output.argmax(dim=1, keepdim=True)  \n",
        "            correct += pred.eq(target.view_as(pred)).sum().item()\n",
        "\n",
        "    test_loss /= len(test_loader.dataset)\n",
        "\n",
        "    print('\\nTest set: Average loss: {:.4f}, Accuracy: {}/{} ({:.0f}%)\\n'.format(\n",
        "        test_loss, correct, len(test_loader.dataset),\n",
        "        100. * correct / len(test_loader.dataset)))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Wmz8eKE3bYrt"
      },
      "source": [
        "def aggregate(central_model, models, weights):\n",
        "    # firstly compute mean weight values\n",
        "    with torch.no_grad():\n",
        "        for m in models.values():\n",
        "            weights['conv0_mean_weight'] += m.conv[0].weight.data.clone()\n",
        "            weights['conv0_mean_bias'] += m.conv[0].bias.data.clone()\n",
        "            weights['conv2_mean_weight'] += m.conv[2].weight.data.clone()\n",
        "            weights['conv2_mean_bias'] += m.conv[2].bias.data.clone()            \n",
        "            weights['fc0_mean_weight'] += m.fc[0].weight.data.clone()\n",
        "            weights['fc0_mean_bias'] += m.fc[0].bias.data.clone()\n",
        "            weights['fc2_mean_weight'] += m.fc[2].weight.data.clone()\n",
        "            weights['fc2_mean_bias'] += m.fc[2].bias.data.clone()            \n",
        "\n",
        "        weights['conv0_mean_weight'] = weights['conv0_mean_weight']/2\n",
        "        weights['conv0_mean_bias'] = weights['conv0_mean_bias']/2\n",
        "        weights['conv2_mean_weight'] = weights['conv2_mean_weight']/2\n",
        "        weights['conv2_mean_bias'] = weights['conv2_mean_bias']/2\n",
        "        weights['fc0_mean_weight'] = weights['fc0_mean_weight']/2\n",
        "        weights['fc0_mean_bias'] = weights['fc0_mean_bias']/2\n",
        "        weights['fc2_mean_weight'] = weights['fc2_mean_weight']/2\n",
        "        weights['fc2_mean_bias'] = weights['fc2_mean_bias']/2\n",
        "\n",
        "        # then copy them to the local models\n",
        "        for m in models.values():\n",
        "            m.conv[0].weight.data = weights['conv0_mean_weight'].data.clone()\n",
        "            m.conv[0].bias.data = weights['conv0_mean_bias'].data.clone()\n",
        "            m.conv[2].weight.data = weights['conv2_mean_weight'].data.clone()\n",
        "            m.conv[2].bias.data = weights['conv2_mean_bias'].data.clone()\n",
        "            m.fc[0].weight.data = weights['fc0_mean_weight'].data.clone()\n",
        "            m.fc[0].bias.data = weights['fc0_mean_bias'].data.clone()\n",
        "            m.fc[2].weight.data = weights['fc2_mean_weight'].data.clone()\n",
        "            m.fc[2].bias.data = weights['fc2_mean_bias'].data.clone()\n",
        "\n",
        "        # and to the central model for the test set\n",
        "        central_model.conv[0].weight.data = weights['conv0_mean_weight'].data.clone()\n",
        "        central_model.conv[0].bias.data = weights['conv0_mean_bias'].data.clone()\n",
        "        central_model.conv[2].weight.data = weights['conv2_mean_weight'].data.clone()\n",
        "        central_model.conv[2].bias.data = weights['conv2_mean_bias'].data.clone()\n",
        "        central_model.fc[0].weight.data = weights['fc0_mean_weight'].data.clone()\n",
        "        central_model.fc[0].bias.data = weights['fc0_mean_bias'].data.clone()\n",
        "        central_model.fc[2].weight.data = weights['fc2_mean_weight'].data.clone()\n",
        "        central_model.fc[2].bias.data = weights['fc2_mean_bias'].data.clone()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "B_jH4dE8FVs1",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "784c9414-a9e1-4059-90c1-aac6ad845e0b"
      },
      "source": [
        "# central model\n",
        "central_model = Net().to(device)\n",
        "# optimizer for central model not needed if model is not to be trained\n",
        "#optimizer = optim.SGD(central_model.parameters(), lr=args['lr'])\n",
        "# clients models and optimizers\n",
        "models = {i:Net().to(device) for i in clients}\n",
        "optimizers = {i:optim.SGD(models[i].parameters(), lr=args['lr']) for i in clients}\n",
        "\n",
        "# initialization of dictionary for model aggregation\n",
        "weights = {'conv0_mean_weight' : torch.zeros(size=central_model.conv[0].weight.shape),\n",
        "           'conv0_mean_bias' : torch.zeros(size=central_model.conv[0].bias.shape),\n",
        "           'conv2_mean_weight' : torch.zeros(size=central_model.conv[2].weight.shape),\n",
        "           'conv2_mean_bias' : torch.zeros(size=central_model.conv[2].bias.shape),\n",
        "           'fc0_mean_weight' : torch.zeros(size=central_model.fc[0].weight.shape),\n",
        "           'fc0_mean_bias' : torch.zeros(size=central_model.fc[0].bias.shape),\n",
        "           'fc2_mean_weight' : torch.zeros(size=central_model.fc[2].weight.shape),\n",
        "           'fc2_mean_bias' : torch.zeros(size=central_model.fc[2].bias.shape)}\n",
        "\n",
        "logging.info(\"Starting training !!\")\n",
        "\n",
        "for epoch in range(1, args['epochs'] + 1):\n",
        "    train_locally(args, models, device, federated_train_loader, optimizers, epoch)\n",
        "    aggregate(central_model, models, weights)\n",
        "    test(central_model, device, test_loader)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Train Epoch: 1 [0/60032 (0%)]\tLoss: 2.302881\tWorker: bob\n",
            "Train Epoch: 1 [640/60032 (1%)]\tLoss: 2.269233\tWorker: bob\n",
            "Train Epoch: 1 [1280/60032 (2%)]\tLoss: 2.181312\tWorker: bob\n",
            "Train Epoch: 1 [1920/60032 (3%)]\tLoss: 2.080367\tWorker: bob\n",
            "Train Epoch: 1 [2560/60032 (4%)]\tLoss: 1.989511\tWorker: bob\n",
            "Train Epoch: 1 [3200/60032 (5%)]\tLoss: 1.812584\tWorker: bob\n",
            "Train Epoch: 1 [3840/60032 (6%)]\tLoss: 1.503633\tWorker: bob\n",
            "Train Epoch: 1 [4480/60032 (7%)]\tLoss: 1.192037\tWorker: bob\n",
            "Train Epoch: 1 [5120/60032 (9%)]\tLoss: 0.967128\tWorker: bob\n",
            "Train Epoch: 1 [5760/60032 (10%)]\tLoss: 0.837335\tWorker: bob\n",
            "Train Epoch: 1 [6400/60032 (11%)]\tLoss: 0.550796\tWorker: bob\n",
            "Train Epoch: 1 [7040/60032 (12%)]\tLoss: 0.544099\tWorker: bob\n",
            "Train Epoch: 1 [7680/60032 (13%)]\tLoss: 0.552291\tWorker: bob\n",
            "Train Epoch: 1 [8320/60032 (14%)]\tLoss: 0.447497\tWorker: bob\n",
            "Train Epoch: 1 [8960/60032 (15%)]\tLoss: 0.415336\tWorker: bob\n",
            "Train Epoch: 1 [9600/60032 (16%)]\tLoss: 0.479506\tWorker: bob\n",
            "Train Epoch: 1 [10240/60032 (17%)]\tLoss: 0.572583\tWorker: bob\n",
            "Train Epoch: 1 [10880/60032 (18%)]\tLoss: 0.371627\tWorker: bob\n",
            "Train Epoch: 1 [11520/60032 (19%)]\tLoss: 0.564186\tWorker: bob\n",
            "Train Epoch: 1 [12160/60032 (20%)]\tLoss: 0.370819\tWorker: bob\n",
            "Train Epoch: 1 [12800/60032 (21%)]\tLoss: 0.445130\tWorker: bob\n",
            "Train Epoch: 1 [13440/60032 (22%)]\tLoss: 0.459739\tWorker: bob\n",
            "Train Epoch: 1 [14080/60032 (23%)]\tLoss: 0.431512\tWorker: bob\n",
            "Train Epoch: 1 [14720/60032 (25%)]\tLoss: 0.498989\tWorker: bob\n",
            "Train Epoch: 1 [15360/60032 (26%)]\tLoss: 0.296497\tWorker: bob\n",
            "Train Epoch: 1 [16000/60032 (27%)]\tLoss: 0.333372\tWorker: bob\n",
            "Train Epoch: 1 [16640/60032 (28%)]\tLoss: 0.425448\tWorker: bob\n",
            "Train Epoch: 1 [17280/60032 (29%)]\tLoss: 0.315733\tWorker: bob\n",
            "Train Epoch: 1 [17920/60032 (30%)]\tLoss: 0.229337\tWorker: bob\n",
            "Train Epoch: 1 [18560/60032 (31%)]\tLoss: 0.365151\tWorker: bob\n",
            "Train Epoch: 1 [19200/60032 (32%)]\tLoss: 0.169648\tWorker: bob\n",
            "Train Epoch: 1 [19840/60032 (33%)]\tLoss: 0.358024\tWorker: bob\n",
            "Train Epoch: 1 [20480/60032 (34%)]\tLoss: 0.271713\tWorker: bob\n",
            "Train Epoch: 1 [21120/60032 (35%)]\tLoss: 0.566206\tWorker: bob\n",
            "Train Epoch: 1 [21760/60032 (36%)]\tLoss: 0.303283\tWorker: bob\n",
            "Train Epoch: 1 [22400/60032 (37%)]\tLoss: 0.292537\tWorker: bob\n",
            "Train Epoch: 1 [23040/60032 (38%)]\tLoss: 0.278613\tWorker: bob\n",
            "Train Epoch: 1 [23680/60032 (39%)]\tLoss: 0.255034\tWorker: bob\n",
            "Train Epoch: 1 [24320/60032 (41%)]\tLoss: 0.367967\tWorker: bob\n",
            "Train Epoch: 1 [24960/60032 (42%)]\tLoss: 0.677041\tWorker: bob\n",
            "Train Epoch: 1 [25600/60032 (43%)]\tLoss: 0.335732\tWorker: bob\n",
            "Train Epoch: 1 [26240/60032 (44%)]\tLoss: 0.210746\tWorker: bob\n",
            "Train Epoch: 1 [26880/60032 (45%)]\tLoss: 0.375033\tWorker: bob\n",
            "Train Epoch: 1 [27520/60032 (46%)]\tLoss: 0.266802\tWorker: bob\n",
            "Train Epoch: 1 [28160/60032 (47%)]\tLoss: 0.402803\tWorker: bob\n",
            "Train Epoch: 1 [28800/60032 (48%)]\tLoss: 0.407085\tWorker: bob\n",
            "Train Epoch: 1 [29440/60032 (49%)]\tLoss: 0.368989\tWorker: bob\n",
            "Train Epoch: 1 [30080/60032 (50%)]\tLoss: 2.311820\tWorker: alice\n",
            "Train Epoch: 1 [30720/60032 (51%)]\tLoss: 2.198309\tWorker: alice\n",
            "Train Epoch: 1 [31360/60032 (52%)]\tLoss: 1.986300\tWorker: alice\n",
            "Train Epoch: 1 [32000/60032 (53%)]\tLoss: 1.719639\tWorker: alice\n",
            "Train Epoch: 1 [32640/60032 (54%)]\tLoss: 1.391656\tWorker: alice\n",
            "Train Epoch: 1 [33280/60032 (55%)]\tLoss: 0.831682\tWorker: alice\n",
            "Train Epoch: 1 [33920/60032 (57%)]\tLoss: 0.688388\tWorker: alice\n",
            "Train Epoch: 1 [34560/60032 (58%)]\tLoss: 0.623708\tWorker: alice\n",
            "Train Epoch: 1 [35200/60032 (59%)]\tLoss: 0.528073\tWorker: alice\n",
            "Train Epoch: 1 [35840/60032 (60%)]\tLoss: 0.434371\tWorker: alice\n",
            "Train Epoch: 1 [36480/60032 (61%)]\tLoss: 0.350452\tWorker: alice\n",
            "Train Epoch: 1 [37120/60032 (62%)]\tLoss: 0.813267\tWorker: alice\n",
            "Train Epoch: 1 [37760/60032 (63%)]\tLoss: 0.611107\tWorker: alice\n",
            "Train Epoch: 1 [38400/60032 (64%)]\tLoss: 0.356318\tWorker: alice\n",
            "Train Epoch: 1 [39040/60032 (65%)]\tLoss: 0.754764\tWorker: alice\n",
            "Train Epoch: 1 [39680/60032 (66%)]\tLoss: 0.476352\tWorker: alice\n",
            "Train Epoch: 1 [40320/60032 (67%)]\tLoss: 0.369089\tWorker: alice\n",
            "Train Epoch: 1 [40960/60032 (68%)]\tLoss: 0.386960\tWorker: alice\n",
            "Train Epoch: 1 [41600/60032 (69%)]\tLoss: 0.309210\tWorker: alice\n",
            "Train Epoch: 1 [42240/60032 (70%)]\tLoss: 0.366012\tWorker: alice\n",
            "Train Epoch: 1 [42880/60032 (71%)]\tLoss: 0.380809\tWorker: alice\n",
            "Train Epoch: 1 [43520/60032 (72%)]\tLoss: 0.553540\tWorker: alice\n",
            "Train Epoch: 1 [44160/60032 (74%)]\tLoss: 0.455237\tWorker: alice\n",
            "Train Epoch: 1 [44800/60032 (75%)]\tLoss: 0.299969\tWorker: alice\n",
            "Train Epoch: 1 [45440/60032 (76%)]\tLoss: 0.216466\tWorker: alice\n",
            "Train Epoch: 1 [46080/60032 (77%)]\tLoss: 0.422969\tWorker: alice\n",
            "Train Epoch: 1 [46720/60032 (78%)]\tLoss: 0.518649\tWorker: alice\n",
            "Train Epoch: 1 [47360/60032 (79%)]\tLoss: 0.236133\tWorker: alice\n",
            "Train Epoch: 1 [48000/60032 (80%)]\tLoss: 0.301296\tWorker: alice\n",
            "Train Epoch: 1 [48640/60032 (81%)]\tLoss: 0.247940\tWorker: alice\n",
            "Train Epoch: 1 [49280/60032 (82%)]\tLoss: 0.209115\tWorker: alice\n",
            "Train Epoch: 1 [49920/60032 (83%)]\tLoss: 0.251108\tWorker: alice\n",
            "Train Epoch: 1 [50560/60032 (84%)]\tLoss: 0.256166\tWorker: alice\n",
            "Train Epoch: 1 [51200/60032 (85%)]\tLoss: 0.497965\tWorker: alice\n",
            "Train Epoch: 1 [51840/60032 (86%)]\tLoss: 0.255314\tWorker: alice\n",
            "Train Epoch: 1 [52480/60032 (87%)]\tLoss: 0.315978\tWorker: alice\n",
            "Train Epoch: 1 [53120/60032 (88%)]\tLoss: 0.259527\tWorker: alice\n",
            "Train Epoch: 1 [53760/60032 (90%)]\tLoss: 0.268639\tWorker: alice\n",
            "Train Epoch: 1 [54400/60032 (91%)]\tLoss: 0.354713\tWorker: alice\n",
            "Train Epoch: 1 [55040/60032 (92%)]\tLoss: 0.154796\tWorker: alice\n",
            "Train Epoch: 1 [55680/60032 (93%)]\tLoss: 0.305943\tWorker: alice\n",
            "Train Epoch: 1 [56320/60032 (94%)]\tLoss: 0.350776\tWorker: alice\n",
            "Train Epoch: 1 [56960/60032 (95%)]\tLoss: 0.182817\tWorker: alice\n",
            "Train Epoch: 1 [57600/60032 (96%)]\tLoss: 0.511244\tWorker: alice\n",
            "Train Epoch: 1 [58240/60032 (97%)]\tLoss: 0.267197\tWorker: alice\n",
            "Train Epoch: 1 [58880/60032 (98%)]\tLoss: 0.203942\tWorker: alice\n",
            "Train Epoch: 1 [59520/60032 (99%)]\tLoss: 0.146559\tWorker: alice\n",
            "\n",
            "Test set: Average loss: 1.2824, Accuracy: 8155/10000 (82%)\n",
            "\n",
            "Train Epoch: 2 [0/60032 (0%)]\tLoss: 1.294291\tWorker: bob\n",
            "Train Epoch: 2 [640/60032 (1%)]\tLoss: 0.774445\tWorker: bob\n",
            "Train Epoch: 2 [1280/60032 (2%)]\tLoss: 0.756981\tWorker: bob\n",
            "Train Epoch: 2 [1920/60032 (3%)]\tLoss: 0.553725\tWorker: bob\n",
            "Train Epoch: 2 [2560/60032 (4%)]\tLoss: 0.527401\tWorker: bob\n",
            "Train Epoch: 2 [3200/60032 (5%)]\tLoss: 0.570611\tWorker: bob\n",
            "Train Epoch: 2 [3840/60032 (6%)]\tLoss: 0.355485\tWorker: bob\n",
            "Train Epoch: 2 [4480/60032 (7%)]\tLoss: 0.289119\tWorker: bob\n",
            "Train Epoch: 2 [5120/60032 (9%)]\tLoss: 0.394382\tWorker: bob\n",
            "Train Epoch: 2 [5760/60032 (10%)]\tLoss: 0.518592\tWorker: bob\n",
            "Train Epoch: 2 [6400/60032 (11%)]\tLoss: 0.449150\tWorker: bob\n",
            "Train Epoch: 2 [7040/60032 (12%)]\tLoss: 0.330611\tWorker: bob\n",
            "Train Epoch: 2 [7680/60032 (13%)]\tLoss: 0.369443\tWorker: bob\n",
            "Train Epoch: 2 [8320/60032 (14%)]\tLoss: 0.141202\tWorker: bob\n",
            "Train Epoch: 2 [8960/60032 (15%)]\tLoss: 0.409588\tWorker: bob\n",
            "Train Epoch: 2 [9600/60032 (16%)]\tLoss: 0.281809\tWorker: bob\n",
            "Train Epoch: 2 [10240/60032 (17%)]\tLoss: 0.181384\tWorker: bob\n",
            "Train Epoch: 2 [10880/60032 (18%)]\tLoss: 0.235889\tWorker: bob\n",
            "Train Epoch: 2 [11520/60032 (19%)]\tLoss: 0.358262\tWorker: bob\n",
            "Train Epoch: 2 [12160/60032 (20%)]\tLoss: 0.378796\tWorker: bob\n",
            "Train Epoch: 2 [12800/60032 (21%)]\tLoss: 0.324240\tWorker: bob\n",
            "Train Epoch: 2 [13440/60032 (22%)]\tLoss: 0.495969\tWorker: bob\n",
            "Train Epoch: 2 [14080/60032 (23%)]\tLoss: 0.328394\tWorker: bob\n",
            "Train Epoch: 2 [14720/60032 (25%)]\tLoss: 0.373076\tWorker: bob\n",
            "Train Epoch: 2 [15360/60032 (26%)]\tLoss: 0.246364\tWorker: bob\n",
            "Train Epoch: 2 [16000/60032 (27%)]\tLoss: 0.346507\tWorker: bob\n",
            "Train Epoch: 2 [16640/60032 (28%)]\tLoss: 0.509071\tWorker: bob\n",
            "Train Epoch: 2 [17280/60032 (29%)]\tLoss: 0.353872\tWorker: bob\n",
            "Train Epoch: 2 [17920/60032 (30%)]\tLoss: 0.356036\tWorker: bob\n",
            "Train Epoch: 2 [18560/60032 (31%)]\tLoss: 0.275170\tWorker: bob\n",
            "Train Epoch: 2 [19200/60032 (32%)]\tLoss: 0.209856\tWorker: bob\n",
            "Train Epoch: 2 [19840/60032 (33%)]\tLoss: 0.211092\tWorker: bob\n",
            "Train Epoch: 2 [20480/60032 (34%)]\tLoss: 0.340585\tWorker: bob\n",
            "Train Epoch: 2 [21120/60032 (35%)]\tLoss: 0.306759\tWorker: bob\n",
            "Train Epoch: 2 [21760/60032 (36%)]\tLoss: 0.232005\tWorker: bob\n",
            "Train Epoch: 2 [22400/60032 (37%)]\tLoss: 0.249320\tWorker: bob\n",
            "Train Epoch: 2 [23040/60032 (38%)]\tLoss: 0.172520\tWorker: bob\n",
            "Train Epoch: 2 [23680/60032 (39%)]\tLoss: 0.485497\tWorker: bob\n",
            "Train Epoch: 2 [24320/60032 (41%)]\tLoss: 0.318679\tWorker: bob\n",
            "Train Epoch: 2 [24960/60032 (42%)]\tLoss: 0.164740\tWorker: bob\n",
            "Train Epoch: 2 [25600/60032 (43%)]\tLoss: 0.197215\tWorker: bob\n",
            "Train Epoch: 2 [26240/60032 (44%)]\tLoss: 0.152968\tWorker: bob\n",
            "Train Epoch: 2 [26880/60032 (45%)]\tLoss: 0.422837\tWorker: bob\n",
            "Train Epoch: 2 [27520/60032 (46%)]\tLoss: 0.485541\tWorker: bob\n",
            "Train Epoch: 2 [28160/60032 (47%)]\tLoss: 0.242662\tWorker: bob\n",
            "Train Epoch: 2 [28800/60032 (48%)]\tLoss: 0.296139\tWorker: bob\n",
            "Train Epoch: 2 [29440/60032 (49%)]\tLoss: 0.267506\tWorker: bob\n",
            "Train Epoch: 2 [30080/60032 (50%)]\tLoss: 1.216200\tWorker: alice\n",
            "Train Epoch: 2 [30720/60032 (51%)]\tLoss: 0.764349\tWorker: alice\n",
            "Train Epoch: 2 [31360/60032 (52%)]\tLoss: 0.663368\tWorker: alice\n",
            "Train Epoch: 2 [32000/60032 (53%)]\tLoss: 0.578253\tWorker: alice\n",
            "Train Epoch: 2 [32640/60032 (54%)]\tLoss: 0.397652\tWorker: alice\n",
            "Train Epoch: 2 [33280/60032 (55%)]\tLoss: 0.240030\tWorker: alice\n",
            "Train Epoch: 2 [33920/60032 (57%)]\tLoss: 0.345788\tWorker: alice\n",
            "Train Epoch: 2 [34560/60032 (58%)]\tLoss: 0.416069\tWorker: alice\n",
            "Train Epoch: 2 [35200/60032 (59%)]\tLoss: 0.222520\tWorker: alice\n",
            "Train Epoch: 2 [35840/60032 (60%)]\tLoss: 0.212798\tWorker: alice\n",
            "Train Epoch: 2 [36480/60032 (61%)]\tLoss: 0.378192\tWorker: alice\n",
            "Train Epoch: 2 [37120/60032 (62%)]\tLoss: 0.709505\tWorker: alice\n",
            "Train Epoch: 2 [37760/60032 (63%)]\tLoss: 0.450195\tWorker: alice\n",
            "Train Epoch: 2 [38400/60032 (64%)]\tLoss: 0.290012\tWorker: alice\n",
            "Train Epoch: 2 [39040/60032 (65%)]\tLoss: 0.477187\tWorker: alice\n",
            "Train Epoch: 2 [39680/60032 (66%)]\tLoss: 0.206979\tWorker: alice\n",
            "Train Epoch: 2 [40320/60032 (67%)]\tLoss: 0.603515\tWorker: alice\n",
            "Train Epoch: 2 [40960/60032 (68%)]\tLoss: 0.376000\tWorker: alice\n",
            "Train Epoch: 2 [41600/60032 (69%)]\tLoss: 0.372577\tWorker: alice\n",
            "Train Epoch: 2 [42240/60032 (70%)]\tLoss: 0.276163\tWorker: alice\n",
            "Train Epoch: 2 [42880/60032 (71%)]\tLoss: 0.335752\tWorker: alice\n",
            "Train Epoch: 2 [43520/60032 (72%)]\tLoss: 0.333160\tWorker: alice\n",
            "Train Epoch: 2 [44160/60032 (74%)]\tLoss: 0.234533\tWorker: alice\n",
            "Train Epoch: 2 [44800/60032 (75%)]\tLoss: 0.254386\tWorker: alice\n",
            "Train Epoch: 2 [45440/60032 (76%)]\tLoss: 0.184998\tWorker: alice\n",
            "Train Epoch: 2 [46080/60032 (77%)]\tLoss: 0.335726\tWorker: alice\n",
            "Train Epoch: 2 [46720/60032 (78%)]\tLoss: 0.344618\tWorker: alice\n",
            "Train Epoch: 2 [47360/60032 (79%)]\tLoss: 0.402970\tWorker: alice\n",
            "Train Epoch: 2 [48000/60032 (80%)]\tLoss: 0.277298\tWorker: alice\n",
            "Train Epoch: 2 [48640/60032 (81%)]\tLoss: 0.265707\tWorker: alice\n",
            "Train Epoch: 2 [49280/60032 (82%)]\tLoss: 0.349070\tWorker: alice\n",
            "Train Epoch: 2 [49920/60032 (83%)]\tLoss: 0.353124\tWorker: alice\n",
            "Train Epoch: 2 [50560/60032 (84%)]\tLoss: 0.315386\tWorker: alice\n",
            "Train Epoch: 2 [51200/60032 (85%)]\tLoss: 0.359977\tWorker: alice\n",
            "Train Epoch: 2 [51840/60032 (86%)]\tLoss: 0.424219\tWorker: alice\n",
            "Train Epoch: 2 [52480/60032 (87%)]\tLoss: 0.331475\tWorker: alice\n",
            "Train Epoch: 2 [53120/60032 (88%)]\tLoss: 0.184187\tWorker: alice\n",
            "Train Epoch: 2 [53760/60032 (90%)]\tLoss: 0.291829\tWorker: alice\n",
            "Train Epoch: 2 [54400/60032 (91%)]\tLoss: 0.310458\tWorker: alice\n",
            "Train Epoch: 2 [55040/60032 (92%)]\tLoss: 0.157509\tWorker: alice\n",
            "Train Epoch: 2 [55680/60032 (93%)]\tLoss: 0.381153\tWorker: alice\n",
            "Train Epoch: 2 [56320/60032 (94%)]\tLoss: 0.429435\tWorker: alice\n",
            "Train Epoch: 2 [56960/60032 (95%)]\tLoss: 0.270188\tWorker: alice\n",
            "Train Epoch: 2 [57600/60032 (96%)]\tLoss: 0.231060\tWorker: alice\n",
            "Train Epoch: 2 [58240/60032 (97%)]\tLoss: 0.189191\tWorker: alice\n",
            "Train Epoch: 2 [58880/60032 (98%)]\tLoss: 0.188028\tWorker: alice\n",
            "Train Epoch: 2 [59520/60032 (99%)]\tLoss: 0.445029\tWorker: alice\n",
            "\n",
            "Test set: Average loss: 0.4314, Accuracy: 9287/10000 (93%)\n",
            "\n",
            "Train Epoch: 3 [0/60032 (0%)]\tLoss: 0.349526\tWorker: bob\n",
            "Train Epoch: 3 [640/60032 (1%)]\tLoss: 0.844814\tWorker: bob\n",
            "Train Epoch: 3 [1280/60032 (2%)]\tLoss: 0.211294\tWorker: bob\n",
            "Train Epoch: 3 [1920/60032 (3%)]\tLoss: 0.362221\tWorker: bob\n",
            "Train Epoch: 3 [2560/60032 (4%)]\tLoss: 0.098572\tWorker: bob\n",
            "Train Epoch: 3 [3200/60032 (5%)]\tLoss: 0.341095\tWorker: bob\n",
            "Train Epoch: 3 [3840/60032 (6%)]\tLoss: 0.151899\tWorker: bob\n",
            "Train Epoch: 3 [4480/60032 (7%)]\tLoss: 0.717090\tWorker: bob\n",
            "Train Epoch: 3 [5120/60032 (9%)]\tLoss: 0.187769\tWorker: bob\n",
            "Train Epoch: 3 [5760/60032 (10%)]\tLoss: 0.268892\tWorker: bob\n",
            "Train Epoch: 3 [6400/60032 (11%)]\tLoss: 0.247988\tWorker: bob\n",
            "Train Epoch: 3 [7040/60032 (12%)]\tLoss: 0.195238\tWorker: bob\n",
            "Train Epoch: 3 [7680/60032 (13%)]\tLoss: 0.208030\tWorker: bob\n",
            "Train Epoch: 3 [8320/60032 (14%)]\tLoss: 0.319254\tWorker: bob\n",
            "Train Epoch: 3 [8960/60032 (15%)]\tLoss: 0.071062\tWorker: bob\n",
            "Train Epoch: 3 [9600/60032 (16%)]\tLoss: 0.164874\tWorker: bob\n",
            "Train Epoch: 3 [10240/60032 (17%)]\tLoss: 0.332922\tWorker: bob\n",
            "Train Epoch: 3 [10880/60032 (18%)]\tLoss: 0.208338\tWorker: bob\n",
            "Train Epoch: 3 [11520/60032 (19%)]\tLoss: 0.219814\tWorker: bob\n",
            "Train Epoch: 3 [12160/60032 (20%)]\tLoss: 0.273856\tWorker: bob\n",
            "Train Epoch: 3 [12800/60032 (21%)]\tLoss: 0.221301\tWorker: bob\n",
            "Train Epoch: 3 [13440/60032 (22%)]\tLoss: 0.315816\tWorker: bob\n",
            "Train Epoch: 3 [14080/60032 (23%)]\tLoss: 0.076318\tWorker: bob\n",
            "Train Epoch: 3 [14720/60032 (25%)]\tLoss: 0.207601\tWorker: bob\n",
            "Train Epoch: 3 [15360/60032 (26%)]\tLoss: 0.204900\tWorker: bob\n",
            "Train Epoch: 3 [16000/60032 (27%)]\tLoss: 0.068659\tWorker: bob\n",
            "Train Epoch: 3 [16640/60032 (28%)]\tLoss: 0.338598\tWorker: bob\n",
            "Train Epoch: 3 [17280/60032 (29%)]\tLoss: 0.569118\tWorker: bob\n",
            "Train Epoch: 3 [17920/60032 (30%)]\tLoss: 0.226505\tWorker: bob\n",
            "Train Epoch: 3 [18560/60032 (31%)]\tLoss: 0.223518\tWorker: bob\n",
            "Train Epoch: 3 [19200/60032 (32%)]\tLoss: 0.222401\tWorker: bob\n",
            "Train Epoch: 3 [19840/60032 (33%)]\tLoss: 0.466442\tWorker: bob\n",
            "Train Epoch: 3 [20480/60032 (34%)]\tLoss: 0.281650\tWorker: bob\n",
            "Train Epoch: 3 [21120/60032 (35%)]\tLoss: 0.119798\tWorker: bob\n",
            "Train Epoch: 3 [21760/60032 (36%)]\tLoss: 0.408961\tWorker: bob\n",
            "Train Epoch: 3 [22400/60032 (37%)]\tLoss: 0.085479\tWorker: bob\n",
            "Train Epoch: 3 [23040/60032 (38%)]\tLoss: 0.151151\tWorker: bob\n",
            "Train Epoch: 3 [23680/60032 (39%)]\tLoss: 0.390186\tWorker: bob\n",
            "Train Epoch: 3 [24320/60032 (41%)]\tLoss: 0.137352\tWorker: bob\n",
            "Train Epoch: 3 [24960/60032 (42%)]\tLoss: 0.178125\tWorker: bob\n",
            "Train Epoch: 3 [25600/60032 (43%)]\tLoss: 0.178775\tWorker: bob\n",
            "Train Epoch: 3 [26240/60032 (44%)]\tLoss: 0.121767\tWorker: bob\n",
            "Train Epoch: 3 [26880/60032 (45%)]\tLoss: 0.141172\tWorker: bob\n",
            "Train Epoch: 3 [27520/60032 (46%)]\tLoss: 0.175533\tWorker: bob\n",
            "Train Epoch: 3 [28160/60032 (47%)]\tLoss: 0.099731\tWorker: bob\n",
            "Train Epoch: 3 [28800/60032 (48%)]\tLoss: 0.285368\tWorker: bob\n",
            "Train Epoch: 3 [29440/60032 (49%)]\tLoss: 0.056352\tWorker: bob\n",
            "Train Epoch: 3 [30080/60032 (50%)]\tLoss: 0.796093\tWorker: alice\n",
            "Train Epoch: 3 [30720/60032 (51%)]\tLoss: 0.756895\tWorker: alice\n",
            "Train Epoch: 3 [31360/60032 (52%)]\tLoss: 0.585063\tWorker: alice\n",
            "Train Epoch: 3 [32000/60032 (53%)]\tLoss: 0.606521\tWorker: alice\n",
            "Train Epoch: 3 [32640/60032 (54%)]\tLoss: 0.292964\tWorker: alice\n",
            "Train Epoch: 3 [33280/60032 (55%)]\tLoss: 0.228685\tWorker: alice\n",
            "Train Epoch: 3 [33920/60032 (57%)]\tLoss: 0.338219\tWorker: alice\n",
            "Train Epoch: 3 [34560/60032 (58%)]\tLoss: 0.061168\tWorker: alice\n",
            "Train Epoch: 3 [35200/60032 (59%)]\tLoss: 0.340476\tWorker: alice\n",
            "Train Epoch: 3 [35840/60032 (60%)]\tLoss: 0.308532\tWorker: alice\n",
            "Train Epoch: 3 [36480/60032 (61%)]\tLoss: 0.206462\tWorker: alice\n",
            "Train Epoch: 3 [37120/60032 (62%)]\tLoss: 0.409172\tWorker: alice\n",
            "Train Epoch: 3 [37760/60032 (63%)]\tLoss: 0.283295\tWorker: alice\n",
            "Train Epoch: 3 [38400/60032 (64%)]\tLoss: 0.153336\tWorker: alice\n",
            "Train Epoch: 3 [39040/60032 (65%)]\tLoss: 0.264468\tWorker: alice\n",
            "Train Epoch: 3 [39680/60032 (66%)]\tLoss: 0.210979\tWorker: alice\n",
            "Train Epoch: 3 [40320/60032 (67%)]\tLoss: 0.105143\tWorker: alice\n",
            "Train Epoch: 3 [40960/60032 (68%)]\tLoss: 0.144292\tWorker: alice\n",
            "Train Epoch: 3 [41600/60032 (69%)]\tLoss: 0.291476\tWorker: alice\n",
            "Train Epoch: 3 [42240/60032 (70%)]\tLoss: 0.206120\tWorker: alice\n",
            "Train Epoch: 3 [42880/60032 (71%)]\tLoss: 0.212496\tWorker: alice\n",
            "Train Epoch: 3 [43520/60032 (72%)]\tLoss: 0.453719\tWorker: alice\n",
            "Train Epoch: 3 [44160/60032 (74%)]\tLoss: 0.101447\tWorker: alice\n",
            "Train Epoch: 3 [44800/60032 (75%)]\tLoss: 0.121562\tWorker: alice\n",
            "Train Epoch: 3 [45440/60032 (76%)]\tLoss: 0.289326\tWorker: alice\n",
            "Train Epoch: 3 [46080/60032 (77%)]\tLoss: 0.284335\tWorker: alice\n",
            "Train Epoch: 3 [46720/60032 (78%)]\tLoss: 0.200511\tWorker: alice\n",
            "Train Epoch: 3 [47360/60032 (79%)]\tLoss: 0.268936\tWorker: alice\n",
            "Train Epoch: 3 [48000/60032 (80%)]\tLoss: 0.058251\tWorker: alice\n",
            "Train Epoch: 3 [48640/60032 (81%)]\tLoss: 0.241676\tWorker: alice\n",
            "Train Epoch: 3 [49280/60032 (82%)]\tLoss: 0.214350\tWorker: alice\n",
            "Train Epoch: 3 [49920/60032 (83%)]\tLoss: 0.400836\tWorker: alice\n",
            "Train Epoch: 3 [50560/60032 (84%)]\tLoss: 0.312143\tWorker: alice\n",
            "Train Epoch: 3 [51200/60032 (85%)]\tLoss: 0.390398\tWorker: alice\n",
            "Train Epoch: 3 [51840/60032 (86%)]\tLoss: 0.225083\tWorker: alice\n",
            "Train Epoch: 3 [52480/60032 (87%)]\tLoss: 0.162494\tWorker: alice\n",
            "Train Epoch: 3 [53120/60032 (88%)]\tLoss: 0.097896\tWorker: alice\n",
            "Train Epoch: 3 [53760/60032 (90%)]\tLoss: 0.147002\tWorker: alice\n",
            "Train Epoch: 3 [54400/60032 (91%)]\tLoss: 0.095615\tWorker: alice\n",
            "Train Epoch: 3 [55040/60032 (92%)]\tLoss: 0.147976\tWorker: alice\n",
            "Train Epoch: 3 [55680/60032 (93%)]\tLoss: 0.107840\tWorker: alice\n",
            "Train Epoch: 3 [56320/60032 (94%)]\tLoss: 0.145470\tWorker: alice\n",
            "Train Epoch: 3 [56960/60032 (95%)]\tLoss: 0.328099\tWorker: alice\n",
            "Train Epoch: 3 [57600/60032 (96%)]\tLoss: 0.155413\tWorker: alice\n",
            "Train Epoch: 3 [58240/60032 (97%)]\tLoss: 0.119109\tWorker: alice\n",
            "Train Epoch: 3 [58880/60032 (98%)]\tLoss: 0.229263\tWorker: alice\n",
            "Train Epoch: 3 [59520/60032 (99%)]\tLoss: 0.102709\tWorker: alice\n",
            "\n",
            "Test set: Average loss: 0.7662, Accuracy: 9443/10000 (94%)\n",
            "\n",
            "Train Epoch: 4 [0/60032 (0%)]\tLoss: 0.430093\tWorker: bob\n",
            "Train Epoch: 4 [640/60032 (1%)]\tLoss: 1.968721\tWorker: bob\n",
            "Train Epoch: 4 [1280/60032 (2%)]\tLoss: 0.358505\tWorker: bob\n",
            "Train Epoch: 4 [1920/60032 (3%)]\tLoss: 0.285969\tWorker: bob\n",
            "Train Epoch: 4 [2560/60032 (4%)]\tLoss: 0.232307\tWorker: bob\n",
            "Train Epoch: 4 [3200/60032 (5%)]\tLoss: 0.259944\tWorker: bob\n",
            "Train Epoch: 4 [3840/60032 (6%)]\tLoss: 0.332414\tWorker: bob\n",
            "Train Epoch: 4 [4480/60032 (7%)]\tLoss: 0.303088\tWorker: bob\n",
            "Train Epoch: 4 [5120/60032 (9%)]\tLoss: 0.431732\tWorker: bob\n",
            "Train Epoch: 4 [5760/60032 (10%)]\tLoss: 0.135392\tWorker: bob\n",
            "Train Epoch: 4 [6400/60032 (11%)]\tLoss: 0.299840\tWorker: bob\n",
            "Train Epoch: 4 [7040/60032 (12%)]\tLoss: 0.355958\tWorker: bob\n",
            "Train Epoch: 4 [7680/60032 (13%)]\tLoss: 0.252264\tWorker: bob\n",
            "Train Epoch: 4 [8320/60032 (14%)]\tLoss: 0.100794\tWorker: bob\n",
            "Train Epoch: 4 [8960/60032 (15%)]\tLoss: 0.295793\tWorker: bob\n",
            "Train Epoch: 4 [9600/60032 (16%)]\tLoss: 0.156738\tWorker: bob\n",
            "Train Epoch: 4 [10240/60032 (17%)]\tLoss: 0.065130\tWorker: bob\n",
            "Train Epoch: 4 [10880/60032 (18%)]\tLoss: 0.110727\tWorker: bob\n",
            "Train Epoch: 4 [11520/60032 (19%)]\tLoss: 0.227577\tWorker: bob\n",
            "Train Epoch: 4 [12160/60032 (20%)]\tLoss: 0.237775\tWorker: bob\n",
            "Train Epoch: 4 [12800/60032 (21%)]\tLoss: 0.333587\tWorker: bob\n",
            "Train Epoch: 4 [13440/60032 (22%)]\tLoss: 0.157609\tWorker: bob\n",
            "Train Epoch: 4 [14080/60032 (23%)]\tLoss: 0.344785\tWorker: bob\n",
            "Train Epoch: 4 [14720/60032 (25%)]\tLoss: 0.095474\tWorker: bob\n",
            "Train Epoch: 4 [15360/60032 (26%)]\tLoss: 0.134103\tWorker: bob\n",
            "Train Epoch: 4 [16000/60032 (27%)]\tLoss: 0.155417\tWorker: bob\n",
            "Train Epoch: 4 [16640/60032 (28%)]\tLoss: 0.101226\tWorker: bob\n",
            "Train Epoch: 4 [17280/60032 (29%)]\tLoss: 0.140616\tWorker: bob\n",
            "Train Epoch: 4 [17920/60032 (30%)]\tLoss: 0.279094\tWorker: bob\n",
            "Train Epoch: 4 [18560/60032 (31%)]\tLoss: 0.244680\tWorker: bob\n",
            "Train Epoch: 4 [19200/60032 (32%)]\tLoss: 0.256607\tWorker: bob\n",
            "Train Epoch: 4 [19840/60032 (33%)]\tLoss: 0.345160\tWorker: bob\n",
            "Train Epoch: 4 [20480/60032 (34%)]\tLoss: 0.112057\tWorker: bob\n",
            "Train Epoch: 4 [21120/60032 (35%)]\tLoss: 0.245044\tWorker: bob\n",
            "Train Epoch: 4 [21760/60032 (36%)]\tLoss: 0.237881\tWorker: bob\n",
            "Train Epoch: 4 [22400/60032 (37%)]\tLoss: 0.103389\tWorker: bob\n",
            "Train Epoch: 4 [23040/60032 (38%)]\tLoss: 0.297109\tWorker: bob\n",
            "Train Epoch: 4 [23680/60032 (39%)]\tLoss: 0.249464\tWorker: bob\n",
            "Train Epoch: 4 [24320/60032 (41%)]\tLoss: 0.227313\tWorker: bob\n",
            "Train Epoch: 4 [24960/60032 (42%)]\tLoss: 0.087461\tWorker: bob\n",
            "Train Epoch: 4 [25600/60032 (43%)]\tLoss: 0.182536\tWorker: bob\n",
            "Train Epoch: 4 [26240/60032 (44%)]\tLoss: 0.124104\tWorker: bob\n",
            "Train Epoch: 4 [26880/60032 (45%)]\tLoss: 0.293813\tWorker: bob\n",
            "Train Epoch: 4 [27520/60032 (46%)]\tLoss: 0.165016\tWorker: bob\n",
            "Train Epoch: 4 [28160/60032 (47%)]\tLoss: 0.242710\tWorker: bob\n",
            "Train Epoch: 4 [28800/60032 (48%)]\tLoss: 0.107330\tWorker: bob\n",
            "Train Epoch: 4 [29440/60032 (49%)]\tLoss: 0.037602\tWorker: bob\n",
            "Train Epoch: 4 [30080/60032 (50%)]\tLoss: 0.407753\tWorker: alice\n",
            "Train Epoch: 4 [30720/60032 (51%)]\tLoss: 0.623961\tWorker: alice\n",
            "Train Epoch: 4 [31360/60032 (52%)]\tLoss: 0.608935\tWorker: alice\n",
            "Train Epoch: 4 [32000/60032 (53%)]\tLoss: 0.239456\tWorker: alice\n",
            "Train Epoch: 4 [32640/60032 (54%)]\tLoss: 0.303826\tWorker: alice\n",
            "Train Epoch: 4 [33280/60032 (55%)]\tLoss: 0.348377\tWorker: alice\n",
            "Train Epoch: 4 [33920/60032 (57%)]\tLoss: 0.333098\tWorker: alice\n",
            "Train Epoch: 4 [34560/60032 (58%)]\tLoss: 0.217190\tWorker: alice\n",
            "Train Epoch: 4 [35200/60032 (59%)]\tLoss: 0.375678\tWorker: alice\n",
            "Train Epoch: 4 [35840/60032 (60%)]\tLoss: 0.215016\tWorker: alice\n",
            "Train Epoch: 4 [36480/60032 (61%)]\tLoss: 0.441948\tWorker: alice\n",
            "Train Epoch: 4 [37120/60032 (62%)]\tLoss: 0.175363\tWorker: alice\n",
            "Train Epoch: 4 [37760/60032 (63%)]\tLoss: 0.214450\tWorker: alice\n",
            "Train Epoch: 4 [38400/60032 (64%)]\tLoss: 0.137317\tWorker: alice\n",
            "Train Epoch: 4 [39040/60032 (65%)]\tLoss: 0.260643\tWorker: alice\n",
            "Train Epoch: 4 [39680/60032 (66%)]\tLoss: 0.213909\tWorker: alice\n",
            "Train Epoch: 4 [40320/60032 (67%)]\tLoss: 0.182606\tWorker: alice\n",
            "Train Epoch: 4 [40960/60032 (68%)]\tLoss: 0.203237\tWorker: alice\n",
            "Train Epoch: 4 [41600/60032 (69%)]\tLoss: 0.228338\tWorker: alice\n",
            "Train Epoch: 4 [42240/60032 (70%)]\tLoss: 0.309284\tWorker: alice\n",
            "Train Epoch: 4 [42880/60032 (71%)]\tLoss: 0.185518\tWorker: alice\n",
            "Train Epoch: 4 [43520/60032 (72%)]\tLoss: 0.137182\tWorker: alice\n",
            "Train Epoch: 4 [44160/60032 (74%)]\tLoss: 0.576996\tWorker: alice\n",
            "Train Epoch: 4 [44800/60032 (75%)]\tLoss: 0.313949\tWorker: alice\n",
            "Train Epoch: 4 [45440/60032 (76%)]\tLoss: 0.142623\tWorker: alice\n",
            "Train Epoch: 4 [46080/60032 (77%)]\tLoss: 0.062380\tWorker: alice\n",
            "Train Epoch: 4 [46720/60032 (78%)]\tLoss: 0.133887\tWorker: alice\n",
            "Train Epoch: 4 [47360/60032 (79%)]\tLoss: 0.253770\tWorker: alice\n",
            "Train Epoch: 4 [48000/60032 (80%)]\tLoss: 0.267149\tWorker: alice\n",
            "Train Epoch: 4 [48640/60032 (81%)]\tLoss: 0.100425\tWorker: alice\n",
            "Train Epoch: 4 [49280/60032 (82%)]\tLoss: 0.224818\tWorker: alice\n",
            "Train Epoch: 4 [49920/60032 (83%)]\tLoss: 0.153209\tWorker: alice\n",
            "Train Epoch: 4 [50560/60032 (84%)]\tLoss: 0.056021\tWorker: alice\n",
            "Train Epoch: 4 [51200/60032 (85%)]\tLoss: 0.140190\tWorker: alice\n",
            "Train Epoch: 4 [51840/60032 (86%)]\tLoss: 0.305914\tWorker: alice\n",
            "Train Epoch: 4 [52480/60032 (87%)]\tLoss: 0.102133\tWorker: alice\n",
            "Train Epoch: 4 [53120/60032 (88%)]\tLoss: 0.213537\tWorker: alice\n",
            "Train Epoch: 4 [53760/60032 (90%)]\tLoss: 0.065776\tWorker: alice\n",
            "Train Epoch: 4 [54400/60032 (91%)]\tLoss: 0.134731\tWorker: alice\n",
            "Train Epoch: 4 [55040/60032 (92%)]\tLoss: 0.464168\tWorker: alice\n",
            "Train Epoch: 4 [55680/60032 (93%)]\tLoss: 0.162889\tWorker: alice\n",
            "Train Epoch: 4 [56320/60032 (94%)]\tLoss: 0.151578\tWorker: alice\n",
            "Train Epoch: 4 [56960/60032 (95%)]\tLoss: 0.180371\tWorker: alice\n",
            "Train Epoch: 4 [57600/60032 (96%)]\tLoss: 0.033480\tWorker: alice\n",
            "Train Epoch: 4 [58240/60032 (97%)]\tLoss: 0.055142\tWorker: alice\n",
            "Train Epoch: 4 [58880/60032 (98%)]\tLoss: 0.185752\tWorker: alice\n",
            "Train Epoch: 4 [59520/60032 (99%)]\tLoss: 0.404718\tWorker: alice\n",
            "\n",
            "Test set: Average loss: 1.7413, Accuracy: 8773/10000 (88%)\n",
            "\n",
            "Train Epoch: 5 [0/60032 (0%)]\tLoss: 3.165017\tWorker: bob\n",
            "Train Epoch: 5 [640/60032 (1%)]\tLoss: 1.833796\tWorker: bob\n",
            "Train Epoch: 5 [1280/60032 (2%)]\tLoss: 0.316018\tWorker: bob\n",
            "Train Epoch: 5 [1920/60032 (3%)]\tLoss: 0.895651\tWorker: bob\n",
            "Train Epoch: 5 [2560/60032 (4%)]\tLoss: 0.439588\tWorker: bob\n",
            "Train Epoch: 5 [3200/60032 (5%)]\tLoss: 0.504812\tWorker: bob\n",
            "Train Epoch: 5 [3840/60032 (6%)]\tLoss: 0.427727\tWorker: bob\n",
            "Train Epoch: 5 [4480/60032 (7%)]\tLoss: 0.585664\tWorker: bob\n",
            "Train Epoch: 5 [5120/60032 (9%)]\tLoss: 0.404399\tWorker: bob\n",
            "Train Epoch: 5 [5760/60032 (10%)]\tLoss: 0.663193\tWorker: bob\n",
            "Train Epoch: 5 [6400/60032 (11%)]\tLoss: 0.134052\tWorker: bob\n",
            "Train Epoch: 5 [7040/60032 (12%)]\tLoss: 0.489383\tWorker: bob\n",
            "Train Epoch: 5 [7680/60032 (13%)]\tLoss: 0.215284\tWorker: bob\n",
            "Train Epoch: 5 [8320/60032 (14%)]\tLoss: 0.233583\tWorker: bob\n",
            "Train Epoch: 5 [8960/60032 (15%)]\tLoss: 0.245565\tWorker: bob\n",
            "Train Epoch: 5 [9600/60032 (16%)]\tLoss: 0.384726\tWorker: bob\n",
            "Train Epoch: 5 [10240/60032 (17%)]\tLoss: 0.098755\tWorker: bob\n",
            "Train Epoch: 5 [10880/60032 (18%)]\tLoss: 0.134311\tWorker: bob\n",
            "Train Epoch: 5 [11520/60032 (19%)]\tLoss: 0.278486\tWorker: bob\n",
            "Train Epoch: 5 [12160/60032 (20%)]\tLoss: 0.200093\tWorker: bob\n",
            "Train Epoch: 5 [12800/60032 (21%)]\tLoss: 0.126753\tWorker: bob\n",
            "Train Epoch: 5 [13440/60032 (22%)]\tLoss: 0.288257\tWorker: bob\n",
            "Train Epoch: 5 [14080/60032 (23%)]\tLoss: 0.077165\tWorker: bob\n",
            "Train Epoch: 5 [14720/60032 (25%)]\tLoss: 0.221641\tWorker: bob\n",
            "Train Epoch: 5 [15360/60032 (26%)]\tLoss: 0.029981\tWorker: bob\n",
            "Train Epoch: 5 [16000/60032 (27%)]\tLoss: 0.665302\tWorker: bob\n",
            "Train Epoch: 5 [16640/60032 (28%)]\tLoss: 0.225277\tWorker: bob\n",
            "Train Epoch: 5 [17280/60032 (29%)]\tLoss: 0.093268\tWorker: bob\n",
            "Train Epoch: 5 [17920/60032 (30%)]\tLoss: 0.575909\tWorker: bob\n",
            "Train Epoch: 5 [18560/60032 (31%)]\tLoss: 0.150042\tWorker: bob\n",
            "Train Epoch: 5 [19200/60032 (32%)]\tLoss: 0.077901\tWorker: bob\n",
            "Train Epoch: 5 [19840/60032 (33%)]\tLoss: 0.513096\tWorker: bob\n",
            "Train Epoch: 5 [20480/60032 (34%)]\tLoss: 0.223895\tWorker: bob\n",
            "Train Epoch: 5 [21120/60032 (35%)]\tLoss: 0.041687\tWorker: bob\n",
            "Train Epoch: 5 [21760/60032 (36%)]\tLoss: 0.267468\tWorker: bob\n",
            "Train Epoch: 5 [22400/60032 (37%)]\tLoss: 0.044782\tWorker: bob\n",
            "Train Epoch: 5 [23040/60032 (38%)]\tLoss: 0.075021\tWorker: bob\n",
            "Train Epoch: 5 [23680/60032 (39%)]\tLoss: 0.758060\tWorker: bob\n",
            "Train Epoch: 5 [24320/60032 (41%)]\tLoss: 0.240426\tWorker: bob\n",
            "Train Epoch: 5 [24960/60032 (42%)]\tLoss: 0.123223\tWorker: bob\n",
            "Train Epoch: 5 [25600/60032 (43%)]\tLoss: 0.207375\tWorker: bob\n",
            "Train Epoch: 5 [26240/60032 (44%)]\tLoss: 0.442089\tWorker: bob\n",
            "Train Epoch: 5 [26880/60032 (45%)]\tLoss: 0.169331\tWorker: bob\n",
            "Train Epoch: 5 [27520/60032 (46%)]\tLoss: 0.008437\tWorker: bob\n",
            "Train Epoch: 5 [28160/60032 (47%)]\tLoss: 0.199373\tWorker: bob\n",
            "Train Epoch: 5 [28800/60032 (48%)]\tLoss: 0.167214\tWorker: bob\n",
            "Train Epoch: 5 [29440/60032 (49%)]\tLoss: 0.195423\tWorker: bob\n",
            "Train Epoch: 5 [30080/60032 (50%)]\tLoss: 3.322317\tWorker: alice\n",
            "Train Epoch: 5 [30720/60032 (51%)]\tLoss: 2.254499\tWorker: alice\n",
            "Train Epoch: 5 [31360/60032 (52%)]\tLoss: 0.859072\tWorker: alice\n",
            "Train Epoch: 5 [32000/60032 (53%)]\tLoss: 0.223861\tWorker: alice\n",
            "Train Epoch: 5 [32640/60032 (54%)]\tLoss: 0.875218\tWorker: alice\n",
            "Train Epoch: 5 [33280/60032 (55%)]\tLoss: 1.125369\tWorker: alice\n",
            "Train Epoch: 5 [33920/60032 (57%)]\tLoss: 0.605973\tWorker: alice\n",
            "Train Epoch: 5 [34560/60032 (58%)]\tLoss: 0.354795\tWorker: alice\n",
            "Train Epoch: 5 [35200/60032 (59%)]\tLoss: 0.461132\tWorker: alice\n",
            "Train Epoch: 5 [35840/60032 (60%)]\tLoss: 0.524486\tWorker: alice\n",
            "Train Epoch: 5 [36480/60032 (61%)]\tLoss: 0.377753\tWorker: alice\n",
            "Train Epoch: 5 [37120/60032 (62%)]\tLoss: 0.289297\tWorker: alice\n",
            "Train Epoch: 5 [37760/60032 (63%)]\tLoss: 0.537721\tWorker: alice\n",
            "Train Epoch: 5 [38400/60032 (64%)]\tLoss: 0.463329\tWorker: alice\n",
            "Train Epoch: 5 [39040/60032 (65%)]\tLoss: 0.067665\tWorker: alice\n",
            "Train Epoch: 5 [39680/60032 (66%)]\tLoss: 0.208680\tWorker: alice\n",
            "Train Epoch: 5 [40320/60032 (67%)]\tLoss: 0.232703\tWorker: alice\n",
            "Train Epoch: 5 [40960/60032 (68%)]\tLoss: 0.282821\tWorker: alice\n",
            "Train Epoch: 5 [41600/60032 (69%)]\tLoss: 0.214048\tWorker: alice\n",
            "Train Epoch: 5 [42240/60032 (70%)]\tLoss: 0.180988\tWorker: alice\n",
            "Train Epoch: 5 [42880/60032 (71%)]\tLoss: 0.203514\tWorker: alice\n",
            "Train Epoch: 5 [43520/60032 (72%)]\tLoss: 0.468159\tWorker: alice\n",
            "Train Epoch: 5 [44160/60032 (74%)]\tLoss: 0.166035\tWorker: alice\n",
            "Train Epoch: 5 [44800/60032 (75%)]\tLoss: 0.265857\tWorker: alice\n",
            "Train Epoch: 5 [45440/60032 (76%)]\tLoss: 0.214275\tWorker: alice\n",
            "Train Epoch: 5 [46080/60032 (77%)]\tLoss: 0.348974\tWorker: alice\n",
            "Train Epoch: 5 [46720/60032 (78%)]\tLoss: 0.316266\tWorker: alice\n",
            "Train Epoch: 5 [47360/60032 (79%)]\tLoss: 0.129119\tWorker: alice\n",
            "Train Epoch: 5 [48000/60032 (80%)]\tLoss: 0.095308\tWorker: alice\n",
            "Train Epoch: 5 [48640/60032 (81%)]\tLoss: 0.046203\tWorker: alice\n",
            "Train Epoch: 5 [49280/60032 (82%)]\tLoss: 0.170770\tWorker: alice\n",
            "Train Epoch: 5 [49920/60032 (83%)]\tLoss: 0.293587\tWorker: alice\n",
            "Train Epoch: 5 [50560/60032 (84%)]\tLoss: 0.362242\tWorker: alice\n",
            "Train Epoch: 5 [51200/60032 (85%)]\tLoss: 0.040276\tWorker: alice\n",
            "Train Epoch: 5 [51840/60032 (86%)]\tLoss: 0.244588\tWorker: alice\n",
            "Train Epoch: 5 [52480/60032 (87%)]\tLoss: 0.263211\tWorker: alice\n",
            "Train Epoch: 5 [53120/60032 (88%)]\tLoss: 0.098889\tWorker: alice\n",
            "Train Epoch: 5 [53760/60032 (90%)]\tLoss: 0.099496\tWorker: alice\n",
            "Train Epoch: 5 [54400/60032 (91%)]\tLoss: 0.194846\tWorker: alice\n",
            "Train Epoch: 5 [55040/60032 (92%)]\tLoss: 0.506840\tWorker: alice\n",
            "Train Epoch: 5 [55680/60032 (93%)]\tLoss: 0.139510\tWorker: alice\n",
            "Train Epoch: 5 [56320/60032 (94%)]\tLoss: 0.163759\tWorker: alice\n",
            "Train Epoch: 5 [56960/60032 (95%)]\tLoss: 0.372176\tWorker: alice\n",
            "Train Epoch: 5 [57600/60032 (96%)]\tLoss: 0.135914\tWorker: alice\n",
            "Train Epoch: 5 [58240/60032 (97%)]\tLoss: 0.143373\tWorker: alice\n",
            "Train Epoch: 5 [58880/60032 (98%)]\tLoss: 0.047868\tWorker: alice\n",
            "Train Epoch: 5 [59520/60032 (99%)]\tLoss: 0.182478\tWorker: alice\n",
            "\n",
            "Test set: Average loss: 1.3212, Accuracy: 9049/10000 (90%)\n",
            "\n",
            "Train Epoch: 6 [0/60032 (0%)]\tLoss: 3.108057\tWorker: bob\n",
            "Train Epoch: 6 [640/60032 (1%)]\tLoss: 3.526091\tWorker: bob\n",
            "Train Epoch: 6 [1280/60032 (2%)]\tLoss: 1.906233\tWorker: bob\n",
            "Train Epoch: 6 [1920/60032 (3%)]\tLoss: 0.924896\tWorker: bob\n",
            "Train Epoch: 6 [2560/60032 (4%)]\tLoss: 1.886769\tWorker: bob\n",
            "Train Epoch: 6 [3200/60032 (5%)]\tLoss: 1.553036\tWorker: bob\n",
            "Train Epoch: 6 [3840/60032 (6%)]\tLoss: 0.993704\tWorker: bob\n",
            "Train Epoch: 6 [4480/60032 (7%)]\tLoss: 0.787719\tWorker: bob\n",
            "Train Epoch: 6 [5120/60032 (9%)]\tLoss: 0.379591\tWorker: bob\n",
            "Train Epoch: 6 [5760/60032 (10%)]\tLoss: 1.267278\tWorker: bob\n",
            "Train Epoch: 6 [6400/60032 (11%)]\tLoss: 0.996699\tWorker: bob\n",
            "Train Epoch: 6 [7040/60032 (12%)]\tLoss: 0.301150\tWorker: bob\n",
            "Train Epoch: 6 [7680/60032 (13%)]\tLoss: 0.491241\tWorker: bob\n",
            "Train Epoch: 6 [8320/60032 (14%)]\tLoss: 0.409038\tWorker: bob\n",
            "Train Epoch: 6 [8960/60032 (15%)]\tLoss: 0.328573\tWorker: bob\n",
            "Train Epoch: 6 [9600/60032 (16%)]\tLoss: 0.267022\tWorker: bob\n",
            "Train Epoch: 6 [10240/60032 (17%)]\tLoss: 0.639326\tWorker: bob\n",
            "Train Epoch: 6 [10880/60032 (18%)]\tLoss: 0.308626\tWorker: bob\n",
            "Train Epoch: 6 [11520/60032 (19%)]\tLoss: 0.515536\tWorker: bob\n",
            "Train Epoch: 6 [12160/60032 (20%)]\tLoss: 0.403262\tWorker: bob\n",
            "Train Epoch: 6 [12800/60032 (21%)]\tLoss: 0.202645\tWorker: bob\n",
            "Train Epoch: 6 [13440/60032 (22%)]\tLoss: 0.229476\tWorker: bob\n",
            "Train Epoch: 6 [14080/60032 (23%)]\tLoss: 0.093959\tWorker: bob\n",
            "Train Epoch: 6 [14720/60032 (25%)]\tLoss: 0.449745\tWorker: bob\n",
            "Train Epoch: 6 [15360/60032 (26%)]\tLoss: 0.086542\tWorker: bob\n",
            "Train Epoch: 6 [16000/60032 (27%)]\tLoss: 0.070727\tWorker: bob\n",
            "Train Epoch: 6 [16640/60032 (28%)]\tLoss: 0.502876\tWorker: bob\n",
            "Train Epoch: 6 [17280/60032 (29%)]\tLoss: 0.135279\tWorker: bob\n",
            "Train Epoch: 6 [17920/60032 (30%)]\tLoss: 0.104777\tWorker: bob\n",
            "Train Epoch: 6 [18560/60032 (31%)]\tLoss: 0.154260\tWorker: bob\n",
            "Train Epoch: 6 [19200/60032 (32%)]\tLoss: 0.201882\tWorker: bob\n",
            "Train Epoch: 6 [19840/60032 (33%)]\tLoss: 0.035872\tWorker: bob\n",
            "Train Epoch: 6 [20480/60032 (34%)]\tLoss: 0.698295\tWorker: bob\n",
            "Train Epoch: 6 [21120/60032 (35%)]\tLoss: 0.038035\tWorker: bob\n",
            "Train Epoch: 6 [21760/60032 (36%)]\tLoss: 0.286369\tWorker: bob\n",
            "Train Epoch: 6 [22400/60032 (37%)]\tLoss: 0.183583\tWorker: bob\n",
            "Train Epoch: 6 [23040/60032 (38%)]\tLoss: 0.210072\tWorker: bob\n",
            "Train Epoch: 6 [23680/60032 (39%)]\tLoss: 0.053238\tWorker: bob\n",
            "Train Epoch: 6 [24320/60032 (41%)]\tLoss: 0.202601\tWorker: bob\n",
            "Train Epoch: 6 [24960/60032 (42%)]\tLoss: 0.112582\tWorker: bob\n",
            "Train Epoch: 6 [25600/60032 (43%)]\tLoss: 0.182233\tWorker: bob\n",
            "Train Epoch: 6 [26240/60032 (44%)]\tLoss: 0.142986\tWorker: bob\n",
            "Train Epoch: 6 [26880/60032 (45%)]\tLoss: 0.355955\tWorker: bob\n",
            "Train Epoch: 6 [27520/60032 (46%)]\tLoss: 0.081313\tWorker: bob\n",
            "Train Epoch: 6 [28160/60032 (47%)]\tLoss: 0.200799\tWorker: bob\n",
            "Train Epoch: 6 [28800/60032 (48%)]\tLoss: 0.591313\tWorker: bob\n",
            "Train Epoch: 6 [29440/60032 (49%)]\tLoss: 0.187966\tWorker: bob\n",
            "Train Epoch: 6 [30080/60032 (50%)]\tLoss: 0.907840\tWorker: alice\n",
            "Train Epoch: 6 [30720/60032 (51%)]\tLoss: 35.473587\tWorker: alice\n",
            "Train Epoch: 6 [31360/60032 (52%)]\tLoss: 0.234566\tWorker: alice\n",
            "Train Epoch: 6 [32000/60032 (53%)]\tLoss: 0.464557\tWorker: alice\n",
            "Train Epoch: 6 [32640/60032 (54%)]\tLoss: 0.763941\tWorker: alice\n",
            "Train Epoch: 6 [33280/60032 (55%)]\tLoss: 0.155327\tWorker: alice\n",
            "Train Epoch: 6 [33920/60032 (57%)]\tLoss: 0.161870\tWorker: alice\n",
            "Train Epoch: 6 [34560/60032 (58%)]\tLoss: 0.305198\tWorker: alice\n",
            "Train Epoch: 6 [35200/60032 (59%)]\tLoss: 0.317422\tWorker: alice\n",
            "Train Epoch: 6 [35840/60032 (60%)]\tLoss: 0.332277\tWorker: alice\n",
            "Train Epoch: 6 [36480/60032 (61%)]\tLoss: 0.226657\tWorker: alice\n",
            "Train Epoch: 6 [37120/60032 (62%)]\tLoss: 0.229520\tWorker: alice\n",
            "Train Epoch: 6 [37760/60032 (63%)]\tLoss: 1.035794\tWorker: alice\n",
            "Train Epoch: 6 [38400/60032 (64%)]\tLoss: 0.485916\tWorker: alice\n",
            "Train Epoch: 6 [39040/60032 (65%)]\tLoss: 0.243279\tWorker: alice\n",
            "Train Epoch: 6 [39680/60032 (66%)]\tLoss: 0.077236\tWorker: alice\n",
            "Train Epoch: 6 [40320/60032 (67%)]\tLoss: 0.323723\tWorker: alice\n",
            "Train Epoch: 6 [40960/60032 (68%)]\tLoss: 0.113484\tWorker: alice\n",
            "Train Epoch: 6 [41600/60032 (69%)]\tLoss: 0.250121\tWorker: alice\n",
            "Train Epoch: 6 [42240/60032 (70%)]\tLoss: 0.102605\tWorker: alice\n",
            "Train Epoch: 6 [42880/60032 (71%)]\tLoss: 0.130777\tWorker: alice\n",
            "Train Epoch: 6 [43520/60032 (72%)]\tLoss: 0.030947\tWorker: alice\n",
            "Train Epoch: 6 [44160/60032 (74%)]\tLoss: 0.156284\tWorker: alice\n",
            "Train Epoch: 6 [44800/60032 (75%)]\tLoss: 0.106852\tWorker: alice\n",
            "Train Epoch: 6 [45440/60032 (76%)]\tLoss: 0.387974\tWorker: alice\n",
            "Train Epoch: 6 [46080/60032 (77%)]\tLoss: 0.156930\tWorker: alice\n",
            "Train Epoch: 6 [46720/60032 (78%)]\tLoss: 0.349785\tWorker: alice\n",
            "Train Epoch: 6 [47360/60032 (79%)]\tLoss: 0.152988\tWorker: alice\n",
            "Train Epoch: 6 [48000/60032 (80%)]\tLoss: 0.258031\tWorker: alice\n",
            "Train Epoch: 6 [48640/60032 (81%)]\tLoss: 0.306171\tWorker: alice\n",
            "Train Epoch: 6 [49280/60032 (82%)]\tLoss: 0.323457\tWorker: alice\n",
            "Train Epoch: 6 [49920/60032 (83%)]\tLoss: 0.109209\tWorker: alice\n",
            "Train Epoch: 6 [50560/60032 (84%)]\tLoss: 0.090901\tWorker: alice\n",
            "Train Epoch: 6 [51200/60032 (85%)]\tLoss: 0.050511\tWorker: alice\n",
            "Train Epoch: 6 [51840/60032 (86%)]\tLoss: 0.146392\tWorker: alice\n",
            "Train Epoch: 6 [52480/60032 (87%)]\tLoss: 0.184658\tWorker: alice\n",
            "Train Epoch: 6 [53120/60032 (88%)]\tLoss: 0.181450\tWorker: alice\n",
            "Train Epoch: 6 [53760/60032 (90%)]\tLoss: 0.034386\tWorker: alice\n",
            "Train Epoch: 6 [54400/60032 (91%)]\tLoss: 0.128181\tWorker: alice\n",
            "Train Epoch: 6 [55040/60032 (92%)]\tLoss: 0.393085\tWorker: alice\n",
            "Train Epoch: 6 [55680/60032 (93%)]\tLoss: 0.033781\tWorker: alice\n",
            "Train Epoch: 6 [56320/60032 (94%)]\tLoss: 0.325892\tWorker: alice\n",
            "Train Epoch: 6 [56960/60032 (95%)]\tLoss: 0.068395\tWorker: alice\n",
            "Train Epoch: 6 [57600/60032 (96%)]\tLoss: 0.142635\tWorker: alice\n",
            "Train Epoch: 6 [58240/60032 (97%)]\tLoss: 0.156093\tWorker: alice\n",
            "Train Epoch: 6 [58880/60032 (98%)]\tLoss: 0.162055\tWorker: alice\n",
            "Train Epoch: 6 [59520/60032 (99%)]\tLoss: 0.407516\tWorker: alice\n",
            "\n",
            "Test set: Average loss: 1.7440, Accuracy: 8824/10000 (88%)\n",
            "\n",
            "Train Epoch: 7 [0/60032 (0%)]\tLoss: 2.001306\tWorker: bob\n",
            "Train Epoch: 7 [640/60032 (1%)]\tLoss: 1.564838\tWorker: bob\n",
            "Train Epoch: 7 [1280/60032 (2%)]\tLoss: 2.164888\tWorker: bob\n",
            "Train Epoch: 7 [1920/60032 (3%)]\tLoss: 1.583126\tWorker: bob\n",
            "Train Epoch: 7 [2560/60032 (4%)]\tLoss: 2.335523\tWorker: bob\n",
            "Train Epoch: 7 [3200/60032 (5%)]\tLoss: 1.515244\tWorker: bob\n",
            "Train Epoch: 7 [3840/60032 (6%)]\tLoss: 0.592594\tWorker: bob\n",
            "Train Epoch: 7 [4480/60032 (7%)]\tLoss: 0.404834\tWorker: bob\n",
            "Train Epoch: 7 [5120/60032 (9%)]\tLoss: 0.819724\tWorker: bob\n",
            "Train Epoch: 7 [5760/60032 (10%)]\tLoss: 0.580811\tWorker: bob\n",
            "Train Epoch: 7 [6400/60032 (11%)]\tLoss: 1.038614\tWorker: bob\n",
            "Train Epoch: 7 [7040/60032 (12%)]\tLoss: 1.003230\tWorker: bob\n",
            "Train Epoch: 7 [7680/60032 (13%)]\tLoss: 0.941863\tWorker: bob\n",
            "Train Epoch: 7 [8320/60032 (14%)]\tLoss: 1.165384\tWorker: bob\n",
            "Train Epoch: 7 [8960/60032 (15%)]\tLoss: 2.231797\tWorker: bob\n",
            "Train Epoch: 7 [9600/60032 (16%)]\tLoss: 0.332776\tWorker: bob\n",
            "Train Epoch: 7 [10240/60032 (17%)]\tLoss: 0.214200\tWorker: bob\n",
            "Train Epoch: 7 [10880/60032 (18%)]\tLoss: 0.702650\tWorker: bob\n",
            "Train Epoch: 7 [11520/60032 (19%)]\tLoss: 0.214181\tWorker: bob\n",
            "Train Epoch: 7 [12160/60032 (20%)]\tLoss: 0.435700\tWorker: bob\n",
            "Train Epoch: 7 [12800/60032 (21%)]\tLoss: 0.688821\tWorker: bob\n",
            "Train Epoch: 7 [13440/60032 (22%)]\tLoss: 0.252164\tWorker: bob\n",
            "Train Epoch: 7 [14080/60032 (23%)]\tLoss: 0.206766\tWorker: bob\n",
            "Train Epoch: 7 [14720/60032 (25%)]\tLoss: 0.237074\tWorker: bob\n",
            "Train Epoch: 7 [15360/60032 (26%)]\tLoss: 0.289239\tWorker: bob\n",
            "Train Epoch: 7 [16000/60032 (27%)]\tLoss: 0.130316\tWorker: bob\n",
            "Train Epoch: 7 [16640/60032 (28%)]\tLoss: 0.760596\tWorker: bob\n",
            "Train Epoch: 7 [17280/60032 (29%)]\tLoss: 0.130180\tWorker: bob\n",
            "Train Epoch: 7 [17920/60032 (30%)]\tLoss: 0.268303\tWorker: bob\n",
            "Train Epoch: 7 [18560/60032 (31%)]\tLoss: 0.178731\tWorker: bob\n",
            "Train Epoch: 7 [19200/60032 (32%)]\tLoss: 0.429311\tWorker: bob\n",
            "Train Epoch: 7 [19840/60032 (33%)]\tLoss: 0.306687\tWorker: bob\n",
            "Train Epoch: 7 [20480/60032 (34%)]\tLoss: 0.188821\tWorker: bob\n",
            "Train Epoch: 7 [21120/60032 (35%)]\tLoss: 0.494714\tWorker: bob\n",
            "Train Epoch: 7 [21760/60032 (36%)]\tLoss: 0.606894\tWorker: bob\n",
            "Train Epoch: 7 [22400/60032 (37%)]\tLoss: 0.352612\tWorker: bob\n",
            "Train Epoch: 7 [23040/60032 (38%)]\tLoss: 0.515081\tWorker: bob\n",
            "Train Epoch: 7 [23680/60032 (39%)]\tLoss: 0.517789\tWorker: bob\n",
            "Train Epoch: 7 [24320/60032 (41%)]\tLoss: 0.346525\tWorker: bob\n",
            "Train Epoch: 7 [24960/60032 (42%)]\tLoss: 0.211851\tWorker: bob\n",
            "Train Epoch: 7 [25600/60032 (43%)]\tLoss: 0.338324\tWorker: bob\n",
            "Train Epoch: 7 [26240/60032 (44%)]\tLoss: 0.455312\tWorker: bob\n",
            "Train Epoch: 7 [26880/60032 (45%)]\tLoss: 0.245366\tWorker: bob\n",
            "Train Epoch: 7 [27520/60032 (46%)]\tLoss: 0.191449\tWorker: bob\n",
            "Train Epoch: 7 [28160/60032 (47%)]\tLoss: 0.118355\tWorker: bob\n",
            "Train Epoch: 7 [28800/60032 (48%)]\tLoss: 0.131286\tWorker: bob\n",
            "Train Epoch: 7 [29440/60032 (49%)]\tLoss: 0.245137\tWorker: bob\n",
            "Train Epoch: 7 [30080/60032 (50%)]\tLoss: 1.669019\tWorker: alice\n",
            "Train Epoch: 7 [30720/60032 (51%)]\tLoss: 0.456290\tWorker: alice\n",
            "Train Epoch: 7 [31360/60032 (52%)]\tLoss: 1.744169\tWorker: alice\n",
            "Train Epoch: 7 [32000/60032 (53%)]\tLoss: 1.765525\tWorker: alice\n",
            "Train Epoch: 7 [32640/60032 (54%)]\tLoss: 0.765662\tWorker: alice\n",
            "Train Epoch: 7 [33280/60032 (55%)]\tLoss: 0.972295\tWorker: alice\n",
            "Train Epoch: 7 [33920/60032 (57%)]\tLoss: 1.102128\tWorker: alice\n",
            "Train Epoch: 7 [34560/60032 (58%)]\tLoss: 0.401595\tWorker: alice\n",
            "Train Epoch: 7 [35200/60032 (59%)]\tLoss: 0.651858\tWorker: alice\n",
            "Train Epoch: 7 [35840/60032 (60%)]\tLoss: 0.905458\tWorker: alice\n",
            "Train Epoch: 7 [36480/60032 (61%)]\tLoss: 2.345361\tWorker: alice\n",
            "Train Epoch: 7 [37120/60032 (62%)]\tLoss: 1.000733\tWorker: alice\n",
            "Train Epoch: 7 [37760/60032 (63%)]\tLoss: 0.161455\tWorker: alice\n",
            "Train Epoch: 7 [38400/60032 (64%)]\tLoss: 0.331302\tWorker: alice\n",
            "Train Epoch: 7 [39040/60032 (65%)]\tLoss: 1.420214\tWorker: alice\n",
            "Train Epoch: 7 [39680/60032 (66%)]\tLoss: 0.270173\tWorker: alice\n",
            "Train Epoch: 7 [40320/60032 (67%)]\tLoss: 2.612005\tWorker: alice\n",
            "Train Epoch: 7 [40960/60032 (68%)]\tLoss: 0.929130\tWorker: alice\n",
            "Train Epoch: 7 [41600/60032 (69%)]\tLoss: 0.279587\tWorker: alice\n",
            "Train Epoch: 7 [42240/60032 (70%)]\tLoss: 1.043594\tWorker: alice\n",
            "Train Epoch: 7 [42880/60032 (71%)]\tLoss: 0.421161\tWorker: alice\n",
            "Train Epoch: 7 [43520/60032 (72%)]\tLoss: 0.164343\tWorker: alice\n",
            "Train Epoch: 7 [44160/60032 (74%)]\tLoss: 0.413391\tWorker: alice\n",
            "Train Epoch: 7 [44800/60032 (75%)]\tLoss: 0.229797\tWorker: alice\n",
            "Train Epoch: 7 [45440/60032 (76%)]\tLoss: 0.582873\tWorker: alice\n",
            "Train Epoch: 7 [46080/60032 (77%)]\tLoss: 0.493077\tWorker: alice\n",
            "Train Epoch: 7 [46720/60032 (78%)]\tLoss: 0.490430\tWorker: alice\n",
            "Train Epoch: 7 [47360/60032 (79%)]\tLoss: 0.173354\tWorker: alice\n",
            "Train Epoch: 7 [48000/60032 (80%)]\tLoss: 1.606663\tWorker: alice\n",
            "Train Epoch: 7 [48640/60032 (81%)]\tLoss: 0.114784\tWorker: alice\n",
            "Train Epoch: 7 [49280/60032 (82%)]\tLoss: 0.651511\tWorker: alice\n",
            "Train Epoch: 7 [49920/60032 (83%)]\tLoss: 0.389025\tWorker: alice\n",
            "Train Epoch: 7 [50560/60032 (84%)]\tLoss: 0.490953\tWorker: alice\n",
            "Train Epoch: 7 [51200/60032 (85%)]\tLoss: 0.239704\tWorker: alice\n",
            "Train Epoch: 7 [51840/60032 (86%)]\tLoss: 0.277617\tWorker: alice\n",
            "Train Epoch: 7 [52480/60032 (87%)]\tLoss: 0.087977\tWorker: alice\n",
            "Train Epoch: 7 [53120/60032 (88%)]\tLoss: 0.133173\tWorker: alice\n",
            "Train Epoch: 7 [53760/60032 (90%)]\tLoss: 0.158028\tWorker: alice\n",
            "Train Epoch: 7 [54400/60032 (91%)]\tLoss: 0.232817\tWorker: alice\n",
            "Train Epoch: 7 [55040/60032 (92%)]\tLoss: 0.266158\tWorker: alice\n",
            "Train Epoch: 7 [55680/60032 (93%)]\tLoss: 0.063275\tWorker: alice\n",
            "Train Epoch: 7 [56320/60032 (94%)]\tLoss: 0.165221\tWorker: alice\n",
            "Train Epoch: 7 [56960/60032 (95%)]\tLoss: 0.293568\tWorker: alice\n",
            "Train Epoch: 7 [57600/60032 (96%)]\tLoss: 0.424425\tWorker: alice\n",
            "Train Epoch: 7 [58240/60032 (97%)]\tLoss: 0.383483\tWorker: alice\n",
            "Train Epoch: 7 [58880/60032 (98%)]\tLoss: 0.051188\tWorker: alice\n",
            "Train Epoch: 7 [59520/60032 (99%)]\tLoss: 0.509480\tWorker: alice\n",
            "\n",
            "Test set: Average loss: 1.6664, Accuracy: 8997/10000 (90%)\n",
            "\n"
          ]
        }
      ]
    }
  ]
}