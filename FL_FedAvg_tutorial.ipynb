{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ej63Q9F5VDWy"
      },
      "source": [
        "Tutorial on Federated Learning with FedAvg using PyTorch and PySyft based on https://learnopencv.com/federated-learning-using-pytorch-and-pysyft/ and https://towardsdatascience.com/federated-learning-a-simple-implementation-of-fedavg-federated-averaging-with-pytorch-90187c9c9577"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "dNwIEBajagKA"
      },
      "source": [
        "## Install PySyft and Torchvision"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "p9Z-tl_jWgeE"
      },
      "outputs": [],
      "source": [
        "!pip install syft==0.2.9"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "FQBYDiJbdQHG"
      },
      "outputs": [],
      "source": [
        "!pip install torchvision==0.2.0"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "x2WDKGKIYdUp"
      },
      "source": [
        "## Simple Testing on PySyft (can be skipped)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "tVNZDzTLVUS8"
      },
      "outputs": [],
      "source": [
        "import torch\n",
        "import syft as sy\n",
        "hook = sy.TorchHook(torch) # add extra functionality to PyTorch"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "MyXfr3LhV91R"
      },
      "outputs": [],
      "source": [
        "# create a machine owned by harmony clinic\n",
        "harmony_clinic = sy.VirtualWorker(hook=hook,id='clinic')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "M75kQXdSXXtC"
      },
      "outputs": [],
      "source": [
        "# we create a Tensor, maybe this is some gene sequence\n",
        "dna = torch.tensor([0,1,2,1,2])\n",
        "\n",
        "# and now I send it, and in turn we get a pointer back that\n",
        "# points to that Tensor\n",
        "dna_ptr = dna.send(harmony_clinic)\n",
        "\n",
        "print(dna_ptr)\n",
        "print(harmony_clinic._objects)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "pmEReTW6YB3d",
        "outputId": "ecdf0b36-68fa-4afe-a937-5786b17ced0b"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "tensor([0, 1, 2, 1, 2])\n",
            "{}\n"
          ]
        }
      ],
      "source": [
        "# get back dna\n",
        "dna = dna_ptr.get()\n",
        "print(dna)\n",
        "\n",
        "# And as you can see... clinic no longer has the tensor dna anymore!!! It has moved back to our machine!\n",
        "print(harmony_clinic._objects)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "2hpF1HRnY1Tv",
        "outputId": "89593387-ea4e-4521-c62e-2b38d87a63fa"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "tensor([2.4000, 6.2000], requires_grad=True)\n",
            "tensor([1., 1.])\n"
          ]
        }
      ],
      "source": [
        "# we create two tensors and send them to clinic\n",
        "train = torch.tensor([2.4, 6.2], requires_grad=True).send(harmony_clinic)\n",
        "label = torch.tensor([2, 6.]).send(harmony_clinic)\n",
        "\n",
        "# we apply some function, in this case a rather simple one, just to show the idea, we use L1 loss\n",
        "loss = (train-label).abs().sum()\n",
        "\n",
        "# Yes, even .backward() works when working with Pointers\n",
        "loss.backward()\n",
        "\n",
        "# now we retreive back the train tensor\n",
        "train = train.get()\n",
        "\n",
        "print(train)\n",
        "\n",
        "# If everything went well, we will see gradients accumulated \n",
        "# in .grad attribute of train\n",
        "print(train.grad)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "AKNKzhn9YVE0"
      },
      "source": [
        "## FL on MNIST dataset"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "id": "eVP2vWh8pzsS"
      },
      "outputs": [],
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "import torch.optim as optim\n",
        "from torchvision import datasets, transforms\n",
        "import logging\n",
        "\n",
        "# import Pysyft to help us to simulate federated learning\n",
        "import syft as sy\n",
        "\n",
        "# hook PyTorch to PySyft, i.e. add extra functionalities to support Federated Learning and other private AI tools\n",
        "hook = sy.TorchHook(torch) \n",
        "\n",
        "# create clients\n",
        "clients = []\n",
        "clients.append(sy.VirtualWorker(hook, id=\"bob\"))\n",
        "clients.append(sy.VirtualWorker(hook, id=\"alice\"))\n",
        "clients.append(sy.VirtualWorker(hook, id=\"untrustful\"))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "id": "fd5-982uY-6L"
      },
      "outputs": [],
      "source": [
        "# define the args\n",
        "args = {\n",
        "    'use_cuda' : True,\n",
        "    'batch_size' : 64,\n",
        "    'test_batch_size' : 1000,\n",
        "    'lr' : 0.01,\n",
        "    'log_interval' : 10,\n",
        "    # with federated learning convergence is faster and less epochs are needed\n",
        "    'epochs' : 7\n",
        "}\n",
        "\n",
        "# check to use GPU or not\n",
        "use_cuda = args['use_cuda'] and torch.cuda.is_available()\n",
        "device = torch.device(\"cuda\" if use_cuda else \"cpu\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "id": "VShbMriNZC6I"
      },
      "outputs": [],
      "source": [
        "# create a simple CNN net\n",
        "class Net(nn.Module):\n",
        "    \n",
        "    def __init__(self):\n",
        "        super(Net, self).__init__()\n",
        "        \n",
        "        self.conv = nn.Sequential(\n",
        "            nn.Conv2d(in_channels = 1, out_channels = 32, kernel_size = 3, stride = 1),\n",
        "            nn.ReLU(),\n",
        "            nn.Conv2d(in_channels=32,out_channels = 64, kernel_size = 3, stride = 1),\n",
        "            nn.ReLU()\n",
        "        )\n",
        "        \n",
        "        self.fc = nn.Sequential(\n",
        "            nn.Linear(in_features=64*12*12, out_features=128),\n",
        "            nn.ReLU(),\n",
        "            nn.Linear(in_features=128, out_features=10),\n",
        "        )\n",
        "\n",
        "        self.dropout = nn.Dropout2d(0.25)\n",
        "    \n",
        "    def forward(self, x):\n",
        "        x = self.conv(x)\n",
        "        x = F.max_pool2d(x,2)\n",
        "        x = x.view(-1, 64*12*12)\n",
        "        x = self.fc(x)\n",
        "        x = F.log_softmax(x, dim=1)\n",
        "        return x"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "_McSsgBEZq1f"
      },
      "outputs": [],
      "source": [
        "# prepare and distribute the data across workers\n",
        "# normally there is no need to distribute data, since it is already at the clients\n",
        "# this is more of a simulation of federated learning\n",
        "federated_train_loader = sy.FederatedDataLoader(\n",
        "    datasets.MNIST('../data', train=True, download=True,\n",
        "                   transform=transforms.Compose([\n",
        "                       transforms.ToTensor(),\n",
        "                       transforms.Normalize((0.1307,), (0.3081,))\n",
        "                   ]))\n",
        "    .federate(tuple(clients)),\n",
        "    batch_size=args['batch_size'], shuffle=True)\n",
        "\n",
        "# test data remains at the central entity\n",
        "test_loader = torch.utils.data.DataLoader(\n",
        "        datasets.MNIST('../data', train=False, transform=transforms.Compose([\n",
        "                           transforms.ToTensor(),\n",
        "                           transforms.Normalize((0.1307,), (0.3081,))\n",
        "                       ])),\n",
        "        batch_size=args['test_batch_size'], shuffle=True)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {
        "id": "bP_p7GrAlXgB"
      },
      "outputs": [],
      "source": [
        "# classic torch code for training except for the federated part\n",
        "def train_locally(args, models, device, train_loader, optimizers, epoch):\n",
        "    for c, m in models.items():\n",
        "        m.train()\n",
        "        # send models to workers\n",
        "        m.send(c)\n",
        "\n",
        "    # iterate over federated data\n",
        "    for batch_idx, (data, target) in enumerate(train_loader):\n",
        "        data, target = data.to(device), target.to(device)\n",
        "\n",
        "        optimizers[data.location].zero_grad()\n",
        "        output = models[data.location](data)\n",
        "        # loss is a ptr to the tensor loss at the remote location\n",
        "        loss = F.nll_loss(output, target)\n",
        "        # call backward() on the loss ptr, that will send the command to call\n",
        "        # backward on the actual loss tensor present on the remote machine\n",
        "        loss.backward()\n",
        "        optimizers[data.location].step()\n",
        "\n",
        "        if batch_idx % args['log_interval'] == 0:\n",
        "\n",
        "            # get back loss, that was created at remote worker\n",
        "            loss = loss.get()\n",
        "\n",
        "            print('Train Epoch: {} [{}/{} ({:.0f}%)]\\tLoss: {:.6f}\\tWorker: {}'.format(\n",
        "                    epoch, \n",
        "                    batch_idx * args['batch_size'], # no of images done\n",
        "                    len(train_loader) * args['batch_size'], # total images left\n",
        "                    100. * batch_idx / len(train_loader),\n",
        "                    loss,\n",
        "                    data.location.id\n",
        "                )\n",
        "            )\n",
        "    \n",
        "    # get back models for aggregation\n",
        "    for m in models.values():\n",
        "        m = m.get()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {
        "id": "cHZpA0PtaKtW"
      },
      "outputs": [],
      "source": [
        "# classic torch code for testing\n",
        "def test(model, device, test_loader):\n",
        "    model.eval()\n",
        "    test_loss = 0\n",
        "    correct = 0\n",
        "    with torch.no_grad():\n",
        "        for data, target in test_loader:\n",
        "            data, target = data.to(device), target.to(device)\n",
        "            output = model(data)\n",
        "\n",
        "            # add losses together\n",
        "            test_loss += F.nll_loss(output, target, reduction='sum').item() \n",
        "\n",
        "            # get the index of the max probability class\n",
        "            pred = output.argmax(dim=1, keepdim=True)  \n",
        "            correct += pred.eq(target.view_as(pred)).sum().item()\n",
        "\n",
        "    test_loss /= len(test_loader.dataset)\n",
        "\n",
        "    print('\\nTest set: Average loss: {:.4f}, Accuracy: {}/{} ({:.0f}%)\\n'.format(\n",
        "        test_loss, correct, len(test_loader.dataset),\n",
        "        100. * correct / len(test_loader.dataset)))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {
        "id": "Wmz8eKE3bYrt"
      },
      "outputs": [],
      "source": [
        "def aggregate(central_model, models, weights):\n",
        "    # firstly compute mean weight values\n",
        "    with torch.no_grad():\n",
        "        for m in models.values():\n",
        "            weights['conv0_mean_weight'] += m.conv[0].weight.data.clone()\n",
        "            weights['conv0_mean_bias'] += m.conv[0].bias.data.clone()\n",
        "            weights['conv2_mean_weight'] += m.conv[2].weight.data.clone()\n",
        "            weights['conv2_mean_bias'] += m.conv[2].bias.data.clone()            \n",
        "            weights['fc0_mean_weight'] += m.fc[0].weight.data.clone()\n",
        "            weights['fc0_mean_bias'] += m.fc[0].bias.data.clone()\n",
        "            weights['fc2_mean_weight'] += m.fc[2].weight.data.clone()\n",
        "            weights['fc2_mean_bias'] += m.fc[2].bias.data.clone()            \n",
        "\n",
        "        weights['conv0_mean_weight'] = weights['conv0_mean_weight']/len(clients)\n",
        "        weights['conv0_mean_bias'] = weights['conv0_mean_bias']/len(clients)\n",
        "        weights['conv2_mean_weight'] = weights['conv2_mean_weight']/len(clients)\n",
        "        weights['conv2_mean_bias'] = weights['conv2_mean_bias']/len(clients)\n",
        "        weights['fc0_mean_weight'] = weights['fc0_mean_weight']/len(clients)\n",
        "        weights['fc0_mean_bias'] = weights['fc0_mean_bias']/len(clients)\n",
        "        weights['fc2_mean_weight'] = weights['fc2_mean_weight']/len(clients)\n",
        "        weights['fc2_mean_bias'] = weights['fc2_mean_bias']/len(clients)\n",
        "\n",
        "        # then copy them to the local models\n",
        "        for m in models.values():\n",
        "            m.conv[0].weight.data = weights['conv0_mean_weight'].data.clone()\n",
        "            m.conv[0].bias.data = weights['conv0_mean_bias'].data.clone()\n",
        "            m.conv[2].weight.data = weights['conv2_mean_weight'].data.clone()\n",
        "            m.conv[2].bias.data = weights['conv2_mean_bias'].data.clone()\n",
        "            m.fc[0].weight.data = weights['fc0_mean_weight'].data.clone()\n",
        "            m.fc[0].bias.data = weights['fc0_mean_bias'].data.clone()\n",
        "            m.fc[2].weight.data = weights['fc2_mean_weight'].data.clone()\n",
        "            m.fc[2].bias.data = weights['fc2_mean_bias'].data.clone()\n",
        "\n",
        "        # and to the central model for the test set\n",
        "        central_model.conv[0].weight.data = weights['conv0_mean_weight'].data.clone()\n",
        "        central_model.conv[0].bias.data = weights['conv0_mean_bias'].data.clone()\n",
        "        central_model.conv[2].weight.data = weights['conv2_mean_weight'].data.clone()\n",
        "        central_model.conv[2].bias.data = weights['conv2_mean_bias'].data.clone()\n",
        "        central_model.fc[0].weight.data = weights['fc0_mean_weight'].data.clone()\n",
        "        central_model.fc[0].bias.data = weights['fc0_mean_bias'].data.clone()\n",
        "        central_model.fc[2].weight.data = weights['fc2_mean_weight'].data.clone()\n",
        "        central_model.fc[2].bias.data = weights['fc2_mean_bias'].data.clone()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "B_jH4dE8FVs1",
        "outputId": "0c170991-fa1e-4e2c-844c-85bde59c4dd1"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Train Epoch: 1 [0/60032 (0%)]\tLoss: 2.282876\tWorker: bob\n",
            "Train Epoch: 1 [640/60032 (1%)]\tLoss: 2.228942\tWorker: bob\n",
            "Train Epoch: 1 [1280/60032 (2%)]\tLoss: 2.086825\tWorker: bob\n",
            "Train Epoch: 1 [1920/60032 (3%)]\tLoss: 1.857036\tWorker: bob\n",
            "Train Epoch: 1 [2560/60032 (4%)]\tLoss: 1.583934\tWorker: bob\n",
            "Train Epoch: 1 [3200/60032 (5%)]\tLoss: 1.012585\tWorker: bob\n",
            "Train Epoch: 1 [3840/60032 (6%)]\tLoss: 0.795606\tWorker: bob\n",
            "Train Epoch: 1 [4480/60032 (7%)]\tLoss: 0.893307\tWorker: bob\n",
            "Train Epoch: 1 [5120/60032 (9%)]\tLoss: 0.673929\tWorker: bob\n",
            "Train Epoch: 1 [5760/60032 (10%)]\tLoss: 0.616753\tWorker: bob\n",
            "Train Epoch: 1 [6400/60032 (11%)]\tLoss: 0.536093\tWorker: bob\n",
            "Train Epoch: 1 [7040/60032 (12%)]\tLoss: 0.607093\tWorker: bob\n",
            "Train Epoch: 1 [7680/60032 (13%)]\tLoss: 0.531591\tWorker: bob\n",
            "Train Epoch: 1 [8320/60032 (14%)]\tLoss: 0.330176\tWorker: bob\n",
            "Train Epoch: 1 [8960/60032 (15%)]\tLoss: 0.411866\tWorker: bob\n",
            "Train Epoch: 1 [9600/60032 (16%)]\tLoss: 0.281410\tWorker: bob\n",
            "Train Epoch: 1 [10240/60032 (17%)]\tLoss: 0.362152\tWorker: bob\n",
            "Train Epoch: 1 [10880/60032 (18%)]\tLoss: 0.530818\tWorker: bob\n",
            "Train Epoch: 1 [11520/60032 (19%)]\tLoss: 0.468367\tWorker: bob\n",
            "Train Epoch: 1 [12160/60032 (20%)]\tLoss: 0.363606\tWorker: bob\n",
            "Train Epoch: 1 [12800/60032 (21%)]\tLoss: 0.422866\tWorker: bob\n",
            "Train Epoch: 1 [13440/60032 (22%)]\tLoss: 0.349353\tWorker: bob\n",
            "Train Epoch: 1 [14080/60032 (23%)]\tLoss: 0.602202\tWorker: bob\n",
            "Train Epoch: 1 [14720/60032 (25%)]\tLoss: 0.365714\tWorker: bob\n",
            "Train Epoch: 1 [15360/60032 (26%)]\tLoss: 0.270766\tWorker: bob\n",
            "Train Epoch: 1 [16000/60032 (27%)]\tLoss: 0.372558\tWorker: bob\n",
            "Train Epoch: 1 [16640/60032 (28%)]\tLoss: 0.151604\tWorker: bob\n",
            "Train Epoch: 1 [17280/60032 (29%)]\tLoss: 0.291660\tWorker: bob\n",
            "Train Epoch: 1 [17920/60032 (30%)]\tLoss: 0.341738\tWorker: bob\n",
            "Train Epoch: 1 [18560/60032 (31%)]\tLoss: 0.270777\tWorker: bob\n",
            "Train Epoch: 1 [19200/60032 (32%)]\tLoss: 0.400880\tWorker: bob\n",
            "Train Epoch: 1 [19840/60032 (33%)]\tLoss: 0.530159\tWorker: bob\n",
            "Train Epoch: 1 [20480/60032 (34%)]\tLoss: 2.224699\tWorker: alice\n",
            "Train Epoch: 1 [21120/60032 (35%)]\tLoss: 2.099291\tWorker: alice\n",
            "Train Epoch: 1 [21760/60032 (36%)]\tLoss: 1.820145\tWorker: alice\n",
            "Train Epoch: 1 [22400/60032 (37%)]\tLoss: 1.505195\tWorker: alice\n",
            "Train Epoch: 1 [23040/60032 (38%)]\tLoss: 1.024664\tWorker: alice\n",
            "Train Epoch: 1 [23680/60032 (39%)]\tLoss: 0.879739\tWorker: alice\n",
            "Train Epoch: 1 [24320/60032 (41%)]\tLoss: 0.912324\tWorker: alice\n",
            "Train Epoch: 1 [24960/60032 (42%)]\tLoss: 0.769197\tWorker: alice\n",
            "Train Epoch: 1 [25600/60032 (43%)]\tLoss: 0.583771\tWorker: alice\n",
            "Train Epoch: 1 [26240/60032 (44%)]\tLoss: 0.561802\tWorker: alice\n",
            "Train Epoch: 1 [26880/60032 (45%)]\tLoss: 0.511783\tWorker: alice\n",
            "Train Epoch: 1 [27520/60032 (46%)]\tLoss: 0.562483\tWorker: alice\n",
            "Train Epoch: 1 [28160/60032 (47%)]\tLoss: 0.441287\tWorker: alice\n",
            "Train Epoch: 1 [28800/60032 (48%)]\tLoss: 0.364860\tWorker: alice\n",
            "Train Epoch: 1 [29440/60032 (49%)]\tLoss: 0.396662\tWorker: alice\n",
            "Train Epoch: 1 [30080/60032 (50%)]\tLoss: 0.345549\tWorker: alice\n",
            "Train Epoch: 1 [30720/60032 (51%)]\tLoss: 0.503631\tWorker: alice\n",
            "Train Epoch: 1 [31360/60032 (52%)]\tLoss: 0.335972\tWorker: alice\n",
            "Train Epoch: 1 [32000/60032 (53%)]\tLoss: 0.350499\tWorker: alice\n",
            "Train Epoch: 1 [32640/60032 (54%)]\tLoss: 0.511283\tWorker: alice\n",
            "Train Epoch: 1 [33280/60032 (55%)]\tLoss: 0.601947\tWorker: alice\n",
            "Train Epoch: 1 [33920/60032 (57%)]\tLoss: 0.362066\tWorker: alice\n",
            "Train Epoch: 1 [34560/60032 (58%)]\tLoss: 0.260290\tWorker: alice\n",
            "Train Epoch: 1 [35200/60032 (59%)]\tLoss: 0.351211\tWorker: alice\n",
            "Train Epoch: 1 [35840/60032 (60%)]\tLoss: 0.328239\tWorker: alice\n",
            "Train Epoch: 1 [36480/60032 (61%)]\tLoss: 0.339280\tWorker: alice\n",
            "Train Epoch: 1 [37120/60032 (62%)]\tLoss: 0.392829\tWorker: alice\n",
            "Train Epoch: 1 [37760/60032 (63%)]\tLoss: 0.257620\tWorker: alice\n",
            "Train Epoch: 1 [38400/60032 (64%)]\tLoss: 0.224514\tWorker: alice\n",
            "Train Epoch: 1 [39040/60032 (65%)]\tLoss: 0.384872\tWorker: alice\n",
            "Train Epoch: 1 [39680/60032 (66%)]\tLoss: 0.359227\tWorker: alice\n",
            "Train Epoch: 1 [40320/60032 (67%)]\tLoss: 2.273342\tWorker: untrustful\n",
            "Train Epoch: 1 [40960/60032 (68%)]\tLoss: 2.130205\tWorker: untrustful\n",
            "Train Epoch: 1 [41600/60032 (69%)]\tLoss: 2.006058\tWorker: untrustful\n",
            "Train Epoch: 1 [42240/60032 (70%)]\tLoss: 1.748865\tWorker: untrustful\n",
            "Train Epoch: 1 [42880/60032 (71%)]\tLoss: 1.480090\tWorker: untrustful\n",
            "Train Epoch: 1 [43520/60032 (72%)]\tLoss: 1.059733\tWorker: untrustful\n",
            "Train Epoch: 1 [44160/60032 (74%)]\tLoss: 0.800840\tWorker: untrustful\n",
            "Train Epoch: 1 [44800/60032 (75%)]\tLoss: 0.783853\tWorker: untrustful\n",
            "Train Epoch: 1 [45440/60032 (76%)]\tLoss: 0.565135\tWorker: untrustful\n",
            "Train Epoch: 1 [46080/60032 (77%)]\tLoss: 0.611441\tWorker: untrustful\n",
            "Train Epoch: 1 [46720/60032 (78%)]\tLoss: 0.588578\tWorker: untrustful\n",
            "Train Epoch: 1 [47360/60032 (79%)]\tLoss: 0.679257\tWorker: untrustful\n",
            "Train Epoch: 1 [48000/60032 (80%)]\tLoss: 0.428811\tWorker: untrustful\n",
            "Train Epoch: 1 [48640/60032 (81%)]\tLoss: 0.366448\tWorker: untrustful\n",
            "Train Epoch: 1 [49280/60032 (82%)]\tLoss: 0.469782\tWorker: untrustful\n",
            "Train Epoch: 1 [49920/60032 (83%)]\tLoss: 0.270319\tWorker: untrustful\n",
            "Train Epoch: 1 [50560/60032 (84%)]\tLoss: 0.662978\tWorker: untrustful\n",
            "Train Epoch: 1 [51200/60032 (85%)]\tLoss: 0.304140\tWorker: untrustful\n",
            "Train Epoch: 1 [51840/60032 (86%)]\tLoss: 0.271147\tWorker: untrustful\n",
            "Train Epoch: 1 [52480/60032 (87%)]\tLoss: 0.260155\tWorker: untrustful\n",
            "Train Epoch: 1 [53120/60032 (88%)]\tLoss: 0.491055\tWorker: untrustful\n",
            "Train Epoch: 1 [53760/60032 (90%)]\tLoss: 0.285368\tWorker: untrustful\n",
            "Train Epoch: 1 [54400/60032 (91%)]\tLoss: 0.255777\tWorker: untrustful\n",
            "Train Epoch: 1 [55040/60032 (92%)]\tLoss: 0.260534\tWorker: untrustful\n",
            "Train Epoch: 1 [55680/60032 (93%)]\tLoss: 0.395284\tWorker: untrustful\n",
            "Train Epoch: 1 [56320/60032 (94%)]\tLoss: 0.499917\tWorker: untrustful\n",
            "Train Epoch: 1 [56960/60032 (95%)]\tLoss: 0.460611\tWorker: untrustful\n",
            "Train Epoch: 1 [57600/60032 (96%)]\tLoss: 0.470268\tWorker: untrustful\n",
            "Train Epoch: 1 [58240/60032 (97%)]\tLoss: 0.215311\tWorker: untrustful\n",
            "Train Epoch: 1 [58880/60032 (98%)]\tLoss: 0.429201\tWorker: untrustful\n",
            "Train Epoch: 1 [59520/60032 (99%)]\tLoss: 0.303540\tWorker: untrustful\n",
            "\n",
            "Test set: Average loss: 1.8811, Accuracy: 5782/10000 (58%)\n",
            "\n",
            "Train Epoch: 2 [0/60032 (0%)]\tLoss: 1.891093\tWorker: bob\n",
            "Train Epoch: 2 [640/60032 (1%)]\tLoss: 1.495978\tWorker: bob\n",
            "Train Epoch: 2 [1280/60032 (2%)]\tLoss: 1.197921\tWorker: bob\n",
            "Train Epoch: 2 [1920/60032 (3%)]\tLoss: 0.806128\tWorker: bob\n",
            "Train Epoch: 2 [2560/60032 (4%)]\tLoss: 0.756721\tWorker: bob\n",
            "Train Epoch: 2 [3200/60032 (5%)]\tLoss: 0.682504\tWorker: bob\n",
            "Train Epoch: 2 [3840/60032 (6%)]\tLoss: 0.551360\tWorker: bob\n",
            "Train Epoch: 2 [4480/60032 (7%)]\tLoss: 0.419084\tWorker: bob\n",
            "Train Epoch: 2 [5120/60032 (9%)]\tLoss: 0.406813\tWorker: bob\n",
            "Train Epoch: 2 [5760/60032 (10%)]\tLoss: 0.548002\tWorker: bob\n",
            "Train Epoch: 2 [6400/60032 (11%)]\tLoss: 0.485401\tWorker: bob\n",
            "Train Epoch: 2 [7040/60032 (12%)]\tLoss: 0.390671\tWorker: bob\n",
            "Train Epoch: 2 [7680/60032 (13%)]\tLoss: 0.442171\tWorker: bob\n",
            "Train Epoch: 2 [8320/60032 (14%)]\tLoss: 0.614659\tWorker: bob\n",
            "Train Epoch: 2 [8960/60032 (15%)]\tLoss: 0.336199\tWorker: bob\n",
            "Train Epoch: 2 [9600/60032 (16%)]\tLoss: 0.601176\tWorker: bob\n",
            "Train Epoch: 2 [10240/60032 (17%)]\tLoss: 0.263232\tWorker: bob\n",
            "Train Epoch: 2 [10880/60032 (18%)]\tLoss: 0.185299\tWorker: bob\n",
            "Train Epoch: 2 [11520/60032 (19%)]\tLoss: 0.337597\tWorker: bob\n",
            "Train Epoch: 2 [12160/60032 (20%)]\tLoss: 0.319332\tWorker: bob\n",
            "Train Epoch: 2 [12800/60032 (21%)]\tLoss: 0.466246\tWorker: bob\n",
            "Train Epoch: 2 [13440/60032 (22%)]\tLoss: 0.292928\tWorker: bob\n",
            "Train Epoch: 2 [14080/60032 (23%)]\tLoss: 0.425749\tWorker: bob\n",
            "Train Epoch: 2 [14720/60032 (25%)]\tLoss: 0.257488\tWorker: bob\n",
            "Train Epoch: 2 [15360/60032 (26%)]\tLoss: 0.412069\tWorker: bob\n",
            "Train Epoch: 2 [16000/60032 (27%)]\tLoss: 0.343930\tWorker: bob\n",
            "Train Epoch: 2 [16640/60032 (28%)]\tLoss: 0.351693\tWorker: bob\n",
            "Train Epoch: 2 [17280/60032 (29%)]\tLoss: 0.385703\tWorker: bob\n",
            "Train Epoch: 2 [17920/60032 (30%)]\tLoss: 0.241036\tWorker: bob\n",
            "Train Epoch: 2 [18560/60032 (31%)]\tLoss: 0.367799\tWorker: bob\n",
            "Train Epoch: 2 [19200/60032 (32%)]\tLoss: 0.297246\tWorker: bob\n",
            "Train Epoch: 2 [19840/60032 (33%)]\tLoss: 0.300627\tWorker: bob\n",
            "Train Epoch: 2 [20480/60032 (34%)]\tLoss: 1.699306\tWorker: alice\n",
            "Train Epoch: 2 [21120/60032 (35%)]\tLoss: 1.333801\tWorker: alice\n",
            "Train Epoch: 2 [21760/60032 (36%)]\tLoss: 1.020742\tWorker: alice\n",
            "Train Epoch: 2 [22400/60032 (37%)]\tLoss: 0.640298\tWorker: alice\n",
            "Train Epoch: 2 [23040/60032 (38%)]\tLoss: 0.589711\tWorker: alice\n",
            "Train Epoch: 2 [23680/60032 (39%)]\tLoss: 0.692334\tWorker: alice\n",
            "Train Epoch: 2 [24320/60032 (41%)]\tLoss: 0.786560\tWorker: alice\n",
            "Train Epoch: 2 [24960/60032 (42%)]\tLoss: 0.397644\tWorker: alice\n",
            "Train Epoch: 2 [25600/60032 (43%)]\tLoss: 0.404344\tWorker: alice\n",
            "Train Epoch: 2 [26240/60032 (44%)]\tLoss: 0.306417\tWorker: alice\n",
            "Train Epoch: 2 [26880/60032 (45%)]\tLoss: 0.427084\tWorker: alice\n",
            "Train Epoch: 2 [27520/60032 (46%)]\tLoss: 0.439389\tWorker: alice\n",
            "Train Epoch: 2 [28160/60032 (47%)]\tLoss: 0.560093\tWorker: alice\n",
            "Train Epoch: 2 [28800/60032 (48%)]\tLoss: 0.363855\tWorker: alice\n",
            "Train Epoch: 2 [29440/60032 (49%)]\tLoss: 0.327586\tWorker: alice\n",
            "Train Epoch: 2 [30080/60032 (50%)]\tLoss: 0.249183\tWorker: alice\n",
            "Train Epoch: 2 [30720/60032 (51%)]\tLoss: 0.480659\tWorker: alice\n",
            "Train Epoch: 2 [31360/60032 (52%)]\tLoss: 0.444302\tWorker: alice\n",
            "Train Epoch: 2 [32000/60032 (53%)]\tLoss: 0.233646\tWorker: alice\n",
            "Train Epoch: 2 [32640/60032 (54%)]\tLoss: 0.573214\tWorker: alice\n",
            "Train Epoch: 2 [33280/60032 (55%)]\tLoss: 0.346495\tWorker: alice\n",
            "Train Epoch: 2 [33920/60032 (57%)]\tLoss: 0.439686\tWorker: alice\n",
            "Train Epoch: 2 [34560/60032 (58%)]\tLoss: 0.446092\tWorker: alice\n",
            "Train Epoch: 2 [35200/60032 (59%)]\tLoss: 0.515895\tWorker: alice\n",
            "Train Epoch: 2 [35840/60032 (60%)]\tLoss: 0.301414\tWorker: alice\n",
            "Train Epoch: 2 [36480/60032 (61%)]\tLoss: 0.587866\tWorker: alice\n",
            "Train Epoch: 2 [37120/60032 (62%)]\tLoss: 0.307704\tWorker: alice\n",
            "Train Epoch: 2 [37760/60032 (63%)]\tLoss: 0.216806\tWorker: alice\n",
            "Train Epoch: 2 [38400/60032 (64%)]\tLoss: 0.467107\tWorker: alice\n",
            "Train Epoch: 2 [39040/60032 (65%)]\tLoss: 0.477268\tWorker: alice\n",
            "Train Epoch: 2 [39680/60032 (66%)]\tLoss: 0.413759\tWorker: alice\n",
            "Train Epoch: 2 [40320/60032 (67%)]\tLoss: 1.791607\tWorker: untrustful\n",
            "Train Epoch: 2 [40960/60032 (68%)]\tLoss: 1.444839\tWorker: untrustful\n",
            "Train Epoch: 2 [41600/60032 (69%)]\tLoss: 1.009732\tWorker: untrustful\n",
            "Train Epoch: 2 [42240/60032 (70%)]\tLoss: 0.670718\tWorker: untrustful\n",
            "Train Epoch: 2 [42880/60032 (71%)]\tLoss: 0.653184\tWorker: untrustful\n",
            "Train Epoch: 2 [43520/60032 (72%)]\tLoss: 0.507955\tWorker: untrustful\n",
            "Train Epoch: 2 [44160/60032 (74%)]\tLoss: 0.711984\tWorker: untrustful\n",
            "Train Epoch: 2 [44800/60032 (75%)]\tLoss: 0.380082\tWorker: untrustful\n",
            "Train Epoch: 2 [45440/60032 (76%)]\tLoss: 0.354672\tWorker: untrustful\n",
            "Train Epoch: 2 [46080/60032 (77%)]\tLoss: 0.332527\tWorker: untrustful\n",
            "Train Epoch: 2 [46720/60032 (78%)]\tLoss: 0.380512\tWorker: untrustful\n",
            "Train Epoch: 2 [47360/60032 (79%)]\tLoss: 0.278044\tWorker: untrustful\n",
            "Train Epoch: 2 [48000/60032 (80%)]\tLoss: 0.526531\tWorker: untrustful\n",
            "Train Epoch: 2 [48640/60032 (81%)]\tLoss: 0.371991\tWorker: untrustful\n",
            "Train Epoch: 2 [49280/60032 (82%)]\tLoss: 0.407695\tWorker: untrustful\n",
            "Train Epoch: 2 [49920/60032 (83%)]\tLoss: 0.409034\tWorker: untrustful\n",
            "Train Epoch: 2 [50560/60032 (84%)]\tLoss: 0.214404\tWorker: untrustful\n",
            "Train Epoch: 2 [51200/60032 (85%)]\tLoss: 0.378182\tWorker: untrustful\n",
            "Train Epoch: 2 [51840/60032 (86%)]\tLoss: 0.224785\tWorker: untrustful\n",
            "Train Epoch: 2 [52480/60032 (87%)]\tLoss: 0.441700\tWorker: untrustful\n",
            "Train Epoch: 2 [53120/60032 (88%)]\tLoss: 0.590325\tWorker: untrustful\n",
            "Train Epoch: 2 [53760/60032 (90%)]\tLoss: 0.500030\tWorker: untrustful\n",
            "Train Epoch: 2 [54400/60032 (91%)]\tLoss: 0.534980\tWorker: untrustful\n",
            "Train Epoch: 2 [55040/60032 (92%)]\tLoss: 0.379235\tWorker: untrustful\n",
            "Train Epoch: 2 [55680/60032 (93%)]\tLoss: 0.390430\tWorker: untrustful\n",
            "Train Epoch: 2 [56320/60032 (94%)]\tLoss: 0.309833\tWorker: untrustful\n",
            "Train Epoch: 2 [56960/60032 (95%)]\tLoss: 0.318687\tWorker: untrustful\n",
            "Train Epoch: 2 [57600/60032 (96%)]\tLoss: 0.493386\tWorker: untrustful\n",
            "Train Epoch: 2 [58240/60032 (97%)]\tLoss: 0.546441\tWorker: untrustful\n",
            "Train Epoch: 2 [58880/60032 (98%)]\tLoss: 0.510273\tWorker: untrustful\n",
            "Train Epoch: 2 [59520/60032 (99%)]\tLoss: 0.283153\tWorker: untrustful\n",
            "\n",
            "Test set: Average loss: 0.4034, Accuracy: 8987/10000 (90%)\n",
            "\n",
            "Train Epoch: 3 [0/60032 (0%)]\tLoss: 0.446093\tWorker: bob\n",
            "Train Epoch: 3 [640/60032 (1%)]\tLoss: 0.370391\tWorker: bob\n",
            "Train Epoch: 3 [1280/60032 (2%)]\tLoss: 0.195002\tWorker: bob\n",
            "Train Epoch: 3 [1920/60032 (3%)]\tLoss: 0.523946\tWorker: bob\n",
            "Train Epoch: 3 [2560/60032 (4%)]\tLoss: 0.157804\tWorker: bob\n",
            "Train Epoch: 3 [3200/60032 (5%)]\tLoss: 0.324349\tWorker: bob\n",
            "Train Epoch: 3 [3840/60032 (6%)]\tLoss: 0.497078\tWorker: bob\n",
            "Train Epoch: 3 [4480/60032 (7%)]\tLoss: 0.356140\tWorker: bob\n",
            "Train Epoch: 3 [5120/60032 (9%)]\tLoss: 0.300518\tWorker: bob\n",
            "Train Epoch: 3 [5760/60032 (10%)]\tLoss: 0.150150\tWorker: bob\n",
            "Train Epoch: 3 [6400/60032 (11%)]\tLoss: 0.213220\tWorker: bob\n",
            "Train Epoch: 3 [7040/60032 (12%)]\tLoss: 0.239386\tWorker: bob\n",
            "Train Epoch: 3 [7680/60032 (13%)]\tLoss: 0.227798\tWorker: bob\n",
            "Train Epoch: 3 [8320/60032 (14%)]\tLoss: 0.196230\tWorker: bob\n",
            "Train Epoch: 3 [8960/60032 (15%)]\tLoss: 0.354144\tWorker: bob\n",
            "Train Epoch: 3 [9600/60032 (16%)]\tLoss: 0.343102\tWorker: bob\n",
            "Train Epoch: 3 [10240/60032 (17%)]\tLoss: 0.418845\tWorker: bob\n",
            "Train Epoch: 3 [10880/60032 (18%)]\tLoss: 0.323127\tWorker: bob\n",
            "Train Epoch: 3 [11520/60032 (19%)]\tLoss: 0.157532\tWorker: bob\n",
            "Train Epoch: 3 [12160/60032 (20%)]\tLoss: 0.224897\tWorker: bob\n",
            "Train Epoch: 3 [12800/60032 (21%)]\tLoss: 0.253031\tWorker: bob\n",
            "Train Epoch: 3 [13440/60032 (22%)]\tLoss: 0.210508\tWorker: bob\n",
            "Train Epoch: 3 [14080/60032 (23%)]\tLoss: 0.188214\tWorker: bob\n",
            "Train Epoch: 3 [14720/60032 (25%)]\tLoss: 0.251394\tWorker: bob\n",
            "Train Epoch: 3 [15360/60032 (26%)]\tLoss: 0.168016\tWorker: bob\n",
            "Train Epoch: 3 [16000/60032 (27%)]\tLoss: 0.505448\tWorker: bob\n",
            "Train Epoch: 3 [16640/60032 (28%)]\tLoss: 0.090611\tWorker: bob\n",
            "Train Epoch: 3 [17280/60032 (29%)]\tLoss: 0.258755\tWorker: bob\n",
            "Train Epoch: 3 [17920/60032 (30%)]\tLoss: 0.381913\tWorker: bob\n",
            "Train Epoch: 3 [18560/60032 (31%)]\tLoss: 0.190261\tWorker: bob\n",
            "Train Epoch: 3 [19200/60032 (32%)]\tLoss: 0.245505\tWorker: bob\n",
            "Train Epoch: 3 [19840/60032 (33%)]\tLoss: 0.190208\tWorker: bob\n",
            "Train Epoch: 3 [20480/60032 (34%)]\tLoss: 0.280474\tWorker: alice\n",
            "Train Epoch: 3 [21120/60032 (35%)]\tLoss: 0.634598\tWorker: alice\n",
            "Train Epoch: 3 [21760/60032 (36%)]\tLoss: 0.353600\tWorker: alice\n",
            "Train Epoch: 3 [22400/60032 (37%)]\tLoss: 0.764528\tWorker: alice\n",
            "Train Epoch: 3 [23040/60032 (38%)]\tLoss: 0.335764\tWorker: alice\n",
            "Train Epoch: 3 [23680/60032 (39%)]\tLoss: 0.269875\tWorker: alice\n",
            "Train Epoch: 3 [24320/60032 (41%)]\tLoss: 0.161617\tWorker: alice\n",
            "Train Epoch: 3 [24960/60032 (42%)]\tLoss: 0.192199\tWorker: alice\n",
            "Train Epoch: 3 [25600/60032 (43%)]\tLoss: 0.236208\tWorker: alice\n",
            "Train Epoch: 3 [26240/60032 (44%)]\tLoss: 0.514002\tWorker: alice\n",
            "Train Epoch: 3 [26880/60032 (45%)]\tLoss: 0.250885\tWorker: alice\n",
            "Train Epoch: 3 [27520/60032 (46%)]\tLoss: 0.287080\tWorker: alice\n",
            "Train Epoch: 3 [28160/60032 (47%)]\tLoss: 0.495839\tWorker: alice\n",
            "Train Epoch: 3 [28800/60032 (48%)]\tLoss: 0.384694\tWorker: alice\n",
            "Train Epoch: 3 [29440/60032 (49%)]\tLoss: 0.280577\tWorker: alice\n",
            "Train Epoch: 3 [30080/60032 (50%)]\tLoss: 0.167044\tWorker: alice\n",
            "Train Epoch: 3 [30720/60032 (51%)]\tLoss: 0.449514\tWorker: alice\n",
            "Train Epoch: 3 [31360/60032 (52%)]\tLoss: 0.302953\tWorker: alice\n",
            "Train Epoch: 3 [32000/60032 (53%)]\tLoss: 0.218468\tWorker: alice\n",
            "Train Epoch: 3 [32640/60032 (54%)]\tLoss: 0.285949\tWorker: alice\n",
            "Train Epoch: 3 [33280/60032 (55%)]\tLoss: 0.305214\tWorker: alice\n",
            "Train Epoch: 3 [33920/60032 (57%)]\tLoss: 0.343128\tWorker: alice\n",
            "Train Epoch: 3 [34560/60032 (58%)]\tLoss: 0.372049\tWorker: alice\n",
            "Train Epoch: 3 [35200/60032 (59%)]\tLoss: 0.389645\tWorker: alice\n",
            "Train Epoch: 3 [35840/60032 (60%)]\tLoss: 0.163710\tWorker: alice\n",
            "Train Epoch: 3 [36480/60032 (61%)]\tLoss: 0.285834\tWorker: alice\n",
            "Train Epoch: 3 [37120/60032 (62%)]\tLoss: 0.239334\tWorker: alice\n",
            "Train Epoch: 3 [37760/60032 (63%)]\tLoss: 0.172806\tWorker: alice\n",
            "Train Epoch: 3 [38400/60032 (64%)]\tLoss: 0.360020\tWorker: alice\n",
            "Train Epoch: 3 [39040/60032 (65%)]\tLoss: 0.172848\tWorker: alice\n",
            "Train Epoch: 3 [39680/60032 (66%)]\tLoss: 0.169419\tWorker: alice\n",
            "Train Epoch: 3 [40320/60032 (67%)]\tLoss: 0.213837\tWorker: untrustful\n",
            "Train Epoch: 3 [40960/60032 (68%)]\tLoss: 0.258641\tWorker: untrustful\n",
            "Train Epoch: 3 [41600/60032 (69%)]\tLoss: 0.210471\tWorker: untrustful\n",
            "Train Epoch: 3 [42240/60032 (70%)]\tLoss: 0.214202\tWorker: untrustful\n",
            "Train Epoch: 3 [42880/60032 (71%)]\tLoss: 0.206374\tWorker: untrustful\n",
            "Train Epoch: 3 [43520/60032 (72%)]\tLoss: 0.287622\tWorker: untrustful\n",
            "Train Epoch: 3 [44160/60032 (74%)]\tLoss: 0.611884\tWorker: untrustful\n",
            "Train Epoch: 3 [44800/60032 (75%)]\tLoss: 0.193481\tWorker: untrustful\n",
            "Train Epoch: 3 [45440/60032 (76%)]\tLoss: 0.328320\tWorker: untrustful\n",
            "Train Epoch: 3 [46080/60032 (77%)]\tLoss: 0.470877\tWorker: untrustful\n",
            "Train Epoch: 3 [46720/60032 (78%)]\tLoss: 0.565626\tWorker: untrustful\n",
            "Train Epoch: 3 [47360/60032 (79%)]\tLoss: 0.276182\tWorker: untrustful\n",
            "Train Epoch: 3 [48000/60032 (80%)]\tLoss: 0.178435\tWorker: untrustful\n",
            "Train Epoch: 3 [48640/60032 (81%)]\tLoss: 0.152894\tWorker: untrustful\n",
            "Train Epoch: 3 [49280/60032 (82%)]\tLoss: 0.305759\tWorker: untrustful\n",
            "Train Epoch: 3 [49920/60032 (83%)]\tLoss: 0.434722\tWorker: untrustful\n",
            "Train Epoch: 3 [50560/60032 (84%)]\tLoss: 0.349703\tWorker: untrustful\n",
            "Train Epoch: 3 [51200/60032 (85%)]\tLoss: 0.472079\tWorker: untrustful\n",
            "Train Epoch: 3 [51840/60032 (86%)]\tLoss: 0.492948\tWorker: untrustful\n",
            "Train Epoch: 3 [52480/60032 (87%)]\tLoss: 0.235112\tWorker: untrustful\n",
            "Train Epoch: 3 [53120/60032 (88%)]\tLoss: 0.358385\tWorker: untrustful\n",
            "Train Epoch: 3 [53760/60032 (90%)]\tLoss: 0.400591\tWorker: untrustful\n",
            "Train Epoch: 3 [54400/60032 (91%)]\tLoss: 0.295823\tWorker: untrustful\n",
            "Train Epoch: 3 [55040/60032 (92%)]\tLoss: 0.252883\tWorker: untrustful\n",
            "Train Epoch: 3 [55680/60032 (93%)]\tLoss: 0.227014\tWorker: untrustful\n",
            "Train Epoch: 3 [56320/60032 (94%)]\tLoss: 0.305205\tWorker: untrustful\n",
            "Train Epoch: 3 [56960/60032 (95%)]\tLoss: 0.261919\tWorker: untrustful\n",
            "Train Epoch: 3 [57600/60032 (96%)]\tLoss: 0.564707\tWorker: untrustful\n",
            "Train Epoch: 3 [58240/60032 (97%)]\tLoss: 0.273615\tWorker: untrustful\n",
            "Train Epoch: 3 [58880/60032 (98%)]\tLoss: 0.107113\tWorker: untrustful\n",
            "Train Epoch: 3 [59520/60032 (99%)]\tLoss: 0.126680\tWorker: untrustful\n",
            "\n",
            "Test set: Average loss: 0.4849, Accuracy: 9319/10000 (93%)\n",
            "\n",
            "Train Epoch: 4 [0/60032 (0%)]\tLoss: 0.386040\tWorker: bob\n",
            "Train Epoch: 4 [640/60032 (1%)]\tLoss: 0.791410\tWorker: bob\n",
            "Train Epoch: 4 [1280/60032 (2%)]\tLoss: 0.125905\tWorker: bob\n",
            "Train Epoch: 4 [1920/60032 (3%)]\tLoss: 0.611441\tWorker: bob\n",
            "Train Epoch: 4 [2560/60032 (4%)]\tLoss: 0.427652\tWorker: bob\n",
            "Train Epoch: 4 [3200/60032 (5%)]\tLoss: 0.271506\tWorker: bob\n",
            "Train Epoch: 4 [3840/60032 (6%)]\tLoss: 0.264739\tWorker: bob\n",
            "Train Epoch: 4 [4480/60032 (7%)]\tLoss: 0.249184\tWorker: bob\n",
            "Train Epoch: 4 [5120/60032 (9%)]\tLoss: 0.271466\tWorker: bob\n",
            "Train Epoch: 4 [5760/60032 (10%)]\tLoss: 0.465515\tWorker: bob\n",
            "Train Epoch: 4 [6400/60032 (11%)]\tLoss: 0.339382\tWorker: bob\n",
            "Train Epoch: 4 [7040/60032 (12%)]\tLoss: 0.126938\tWorker: bob\n",
            "Train Epoch: 4 [7680/60032 (13%)]\tLoss: 0.240800\tWorker: bob\n",
            "Train Epoch: 4 [8320/60032 (14%)]\tLoss: 0.236886\tWorker: bob\n",
            "Train Epoch: 4 [8960/60032 (15%)]\tLoss: 0.250798\tWorker: bob\n",
            "Train Epoch: 4 [9600/60032 (16%)]\tLoss: 0.328074\tWorker: bob\n",
            "Train Epoch: 4 [10240/60032 (17%)]\tLoss: 0.088999\tWorker: bob\n",
            "Train Epoch: 4 [10880/60032 (18%)]\tLoss: 0.114648\tWorker: bob\n",
            "Train Epoch: 4 [11520/60032 (19%)]\tLoss: 0.133757\tWorker: bob\n",
            "Train Epoch: 4 [12160/60032 (20%)]\tLoss: 0.086535\tWorker: bob\n",
            "Train Epoch: 4 [12800/60032 (21%)]\tLoss: 0.301030\tWorker: bob\n",
            "Train Epoch: 4 [13440/60032 (22%)]\tLoss: 0.136392\tWorker: bob\n",
            "Train Epoch: 4 [14080/60032 (23%)]\tLoss: 0.218324\tWorker: bob\n",
            "Train Epoch: 4 [14720/60032 (25%)]\tLoss: 0.184771\tWorker: bob\n",
            "Train Epoch: 4 [15360/60032 (26%)]\tLoss: 0.228333\tWorker: bob\n",
            "Train Epoch: 4 [16000/60032 (27%)]\tLoss: 0.159190\tWorker: bob\n",
            "Train Epoch: 4 [16640/60032 (28%)]\tLoss: 0.182289\tWorker: bob\n",
            "Train Epoch: 4 [17280/60032 (29%)]\tLoss: 0.244565\tWorker: bob\n",
            "Train Epoch: 4 [17920/60032 (30%)]\tLoss: 0.205000\tWorker: bob\n",
            "Train Epoch: 4 [18560/60032 (31%)]\tLoss: 0.324000\tWorker: bob\n",
            "Train Epoch: 4 [19200/60032 (32%)]\tLoss: 0.215313\tWorker: bob\n",
            "Train Epoch: 4 [19840/60032 (33%)]\tLoss: 0.298881\tWorker: bob\n",
            "Train Epoch: 4 [20480/60032 (34%)]\tLoss: 5.609627\tWorker: alice\n",
            "Train Epoch: 4 [21120/60032 (35%)]\tLoss: 0.446306\tWorker: alice\n",
            "Train Epoch: 4 [21760/60032 (36%)]\tLoss: 0.327214\tWorker: alice\n",
            "Train Epoch: 4 [22400/60032 (37%)]\tLoss: 0.208977\tWorker: alice\n",
            "Train Epoch: 4 [23040/60032 (38%)]\tLoss: 0.257533\tWorker: alice\n",
            "Train Epoch: 4 [23680/60032 (39%)]\tLoss: 0.085545\tWorker: alice\n",
            "Train Epoch: 4 [24320/60032 (41%)]\tLoss: 0.213191\tWorker: alice\n",
            "Train Epoch: 4 [24960/60032 (42%)]\tLoss: 0.310436\tWorker: alice\n",
            "Train Epoch: 4 [25600/60032 (43%)]\tLoss: 0.082946\tWorker: alice\n",
            "Train Epoch: 4 [26240/60032 (44%)]\tLoss: 0.172687\tWorker: alice\n",
            "Train Epoch: 4 [26880/60032 (45%)]\tLoss: 0.199488\tWorker: alice\n",
            "Train Epoch: 4 [27520/60032 (46%)]\tLoss: 0.158133\tWorker: alice\n",
            "Train Epoch: 4 [28160/60032 (47%)]\tLoss: 0.285079\tWorker: alice\n",
            "Train Epoch: 4 [28800/60032 (48%)]\tLoss: 0.241408\tWorker: alice\n",
            "Train Epoch: 4 [29440/60032 (49%)]\tLoss: 0.460853\tWorker: alice\n",
            "Train Epoch: 4 [30080/60032 (50%)]\tLoss: 0.477224\tWorker: alice\n",
            "Train Epoch: 4 [30720/60032 (51%)]\tLoss: 0.215425\tWorker: alice\n",
            "Train Epoch: 4 [31360/60032 (52%)]\tLoss: 0.363044\tWorker: alice\n",
            "Train Epoch: 4 [32000/60032 (53%)]\tLoss: 0.217579\tWorker: alice\n",
            "Train Epoch: 4 [32640/60032 (54%)]\tLoss: 0.278891\tWorker: alice\n",
            "Train Epoch: 4 [33280/60032 (55%)]\tLoss: 0.260652\tWorker: alice\n",
            "Train Epoch: 4 [33920/60032 (57%)]\tLoss: 0.214608\tWorker: alice\n",
            "Train Epoch: 4 [34560/60032 (58%)]\tLoss: 0.352634\tWorker: alice\n",
            "Train Epoch: 4 [35200/60032 (59%)]\tLoss: 0.188105\tWorker: alice\n",
            "Train Epoch: 4 [35840/60032 (60%)]\tLoss: 0.099391\tWorker: alice\n",
            "Train Epoch: 4 [36480/60032 (61%)]\tLoss: 0.366215\tWorker: alice\n",
            "Train Epoch: 4 [37120/60032 (62%)]\tLoss: 0.169932\tWorker: alice\n",
            "Train Epoch: 4 [37760/60032 (63%)]\tLoss: 0.187437\tWorker: alice\n",
            "Train Epoch: 4 [38400/60032 (64%)]\tLoss: 0.334505\tWorker: alice\n",
            "Train Epoch: 4 [39040/60032 (65%)]\tLoss: 0.122061\tWorker: alice\n",
            "Train Epoch: 4 [39680/60032 (66%)]\tLoss: 0.162412\tWorker: alice\n",
            "Train Epoch: 4 [40320/60032 (67%)]\tLoss: 1.971392\tWorker: untrustful\n",
            "Train Epoch: 4 [40960/60032 (68%)]\tLoss: 0.182999\tWorker: untrustful\n",
            "Train Epoch: 4 [41600/60032 (69%)]\tLoss: 0.282320\tWorker: untrustful\n",
            "Train Epoch: 4 [42240/60032 (70%)]\tLoss: 0.630792\tWorker: untrustful\n",
            "Train Epoch: 4 [42880/60032 (71%)]\tLoss: 0.130940\tWorker: untrustful\n",
            "Train Epoch: 4 [43520/60032 (72%)]\tLoss: 0.190628\tWorker: untrustful\n",
            "Train Epoch: 4 [44160/60032 (74%)]\tLoss: 0.170495\tWorker: untrustful\n",
            "Train Epoch: 4 [44800/60032 (75%)]\tLoss: 0.112400\tWorker: untrustful\n",
            "Train Epoch: 4 [45440/60032 (76%)]\tLoss: 0.399276\tWorker: untrustful\n",
            "Train Epoch: 4 [46080/60032 (77%)]\tLoss: 0.159732\tWorker: untrustful\n",
            "Train Epoch: 4 [46720/60032 (78%)]\tLoss: 0.284431\tWorker: untrustful\n",
            "Train Epoch: 4 [47360/60032 (79%)]\tLoss: 0.363466\tWorker: untrustful\n",
            "Train Epoch: 4 [48000/60032 (80%)]\tLoss: 0.128109\tWorker: untrustful\n",
            "Train Epoch: 4 [48640/60032 (81%)]\tLoss: 0.452755\tWorker: untrustful\n",
            "Train Epoch: 4 [49280/60032 (82%)]\tLoss: 0.074513\tWorker: untrustful\n",
            "Train Epoch: 4 [49920/60032 (83%)]\tLoss: 0.348463\tWorker: untrustful\n",
            "Train Epoch: 4 [50560/60032 (84%)]\tLoss: 0.346361\tWorker: untrustful\n",
            "Train Epoch: 4 [51200/60032 (85%)]\tLoss: 0.287568\tWorker: untrustful\n",
            "Train Epoch: 4 [51840/60032 (86%)]\tLoss: 0.314345\tWorker: untrustful\n",
            "Train Epoch: 4 [52480/60032 (87%)]\tLoss: 0.120954\tWorker: untrustful\n",
            "Train Epoch: 4 [53120/60032 (88%)]\tLoss: 0.167330\tWorker: untrustful\n",
            "Train Epoch: 4 [53760/60032 (90%)]\tLoss: 0.201448\tWorker: untrustful\n",
            "Train Epoch: 4 [54400/60032 (91%)]\tLoss: 0.325173\tWorker: untrustful\n",
            "Train Epoch: 4 [55040/60032 (92%)]\tLoss: 0.189846\tWorker: untrustful\n",
            "Train Epoch: 4 [55680/60032 (93%)]\tLoss: 0.248906\tWorker: untrustful\n",
            "Train Epoch: 4 [56320/60032 (94%)]\tLoss: 0.138923\tWorker: untrustful\n",
            "Train Epoch: 4 [56960/60032 (95%)]\tLoss: 0.246048\tWorker: untrustful\n",
            "Train Epoch: 4 [57600/60032 (96%)]\tLoss: 0.115448\tWorker: untrustful\n",
            "Train Epoch: 4 [58240/60032 (97%)]\tLoss: 0.257247\tWorker: untrustful\n",
            "Train Epoch: 4 [58880/60032 (98%)]\tLoss: 0.313688\tWorker: untrustful\n",
            "Train Epoch: 4 [59520/60032 (99%)]\tLoss: 0.195339\tWorker: untrustful\n",
            "\n",
            "Test set: Average loss: 0.4645, Accuracy: 9430/10000 (94%)\n",
            "\n",
            "Train Epoch: 5 [0/60032 (0%)]\tLoss: 0.393368\tWorker: bob\n",
            "Train Epoch: 5 [640/60032 (1%)]\tLoss: 0.852509\tWorker: bob\n",
            "Train Epoch: 5 [1280/60032 (2%)]\tLoss: 0.533919\tWorker: bob\n",
            "Train Epoch: 5 [1920/60032 (3%)]\tLoss: 0.478981\tWorker: bob\n",
            "Train Epoch: 5 [2560/60032 (4%)]\tLoss: 0.185848\tWorker: bob\n",
            "Train Epoch: 5 [3200/60032 (5%)]\tLoss: 0.593756\tWorker: bob\n",
            "Train Epoch: 5 [3840/60032 (6%)]\tLoss: 0.244932\tWorker: bob\n",
            "Train Epoch: 5 [4480/60032 (7%)]\tLoss: 0.181679\tWorker: bob\n",
            "Train Epoch: 5 [5120/60032 (9%)]\tLoss: 0.300750\tWorker: bob\n",
            "Train Epoch: 5 [5760/60032 (10%)]\tLoss: 0.540670\tWorker: bob\n",
            "Train Epoch: 5 [6400/60032 (11%)]\tLoss: 0.384147\tWorker: bob\n",
            "Train Epoch: 5 [7040/60032 (12%)]\tLoss: 0.447338\tWorker: bob\n",
            "Train Epoch: 5 [7680/60032 (13%)]\tLoss: 0.419969\tWorker: bob\n",
            "Train Epoch: 5 [8320/60032 (14%)]\tLoss: 0.213803\tWorker: bob\n",
            "Train Epoch: 5 [8960/60032 (15%)]\tLoss: 0.133970\tWorker: bob\n",
            "Train Epoch: 5 [9600/60032 (16%)]\tLoss: 0.308479\tWorker: bob\n",
            "Train Epoch: 5 [10240/60032 (17%)]\tLoss: 0.189285\tWorker: bob\n",
            "Train Epoch: 5 [10880/60032 (18%)]\tLoss: 0.089360\tWorker: bob\n",
            "Train Epoch: 5 [11520/60032 (19%)]\tLoss: 0.335358\tWorker: bob\n",
            "Train Epoch: 5 [12160/60032 (20%)]\tLoss: 0.225663\tWorker: bob\n",
            "Train Epoch: 5 [12800/60032 (21%)]\tLoss: 0.284990\tWorker: bob\n",
            "Train Epoch: 5 [13440/60032 (22%)]\tLoss: 0.139945\tWorker: bob\n",
            "Train Epoch: 5 [14080/60032 (23%)]\tLoss: 0.171490\tWorker: bob\n",
            "Train Epoch: 5 [14720/60032 (25%)]\tLoss: 0.086869\tWorker: bob\n",
            "Train Epoch: 5 [15360/60032 (26%)]\tLoss: 0.067652\tWorker: bob\n",
            "Train Epoch: 5 [16000/60032 (27%)]\tLoss: 0.156265\tWorker: bob\n",
            "Train Epoch: 5 [16640/60032 (28%)]\tLoss: 0.158384\tWorker: bob\n",
            "Train Epoch: 5 [17280/60032 (29%)]\tLoss: 0.216194\tWorker: bob\n",
            "Train Epoch: 5 [17920/60032 (30%)]\tLoss: 0.178753\tWorker: bob\n",
            "Train Epoch: 5 [18560/60032 (31%)]\tLoss: 0.118569\tWorker: bob\n",
            "Train Epoch: 5 [19200/60032 (32%)]\tLoss: 0.171233\tWorker: bob\n",
            "Train Epoch: 5 [19840/60032 (33%)]\tLoss: 0.234970\tWorker: bob\n",
            "Train Epoch: 5 [20480/60032 (34%)]\tLoss: 0.300736\tWorker: alice\n",
            "Train Epoch: 5 [21120/60032 (35%)]\tLoss: 0.339532\tWorker: alice\n",
            "Train Epoch: 5 [21760/60032 (36%)]\tLoss: 0.330331\tWorker: alice\n",
            "Train Epoch: 5 [22400/60032 (37%)]\tLoss: 0.200353\tWorker: alice\n",
            "Train Epoch: 5 [23040/60032 (38%)]\tLoss: 0.467622\tWorker: alice\n",
            "Train Epoch: 5 [23680/60032 (39%)]\tLoss: 0.291410\tWorker: alice\n",
            "Train Epoch: 5 [24320/60032 (41%)]\tLoss: 0.062568\tWorker: alice\n",
            "Train Epoch: 5 [24960/60032 (42%)]\tLoss: 0.320187\tWorker: alice\n",
            "Train Epoch: 5 [25600/60032 (43%)]\tLoss: 0.358027\tWorker: alice\n",
            "Train Epoch: 5 [26240/60032 (44%)]\tLoss: 0.122434\tWorker: alice\n",
            "Train Epoch: 5 [26880/60032 (45%)]\tLoss: 0.187754\tWorker: alice\n",
            "Train Epoch: 5 [27520/60032 (46%)]\tLoss: 0.217549\tWorker: alice\n",
            "Train Epoch: 5 [28160/60032 (47%)]\tLoss: 0.264729\tWorker: alice\n",
            "Train Epoch: 5 [28800/60032 (48%)]\tLoss: 0.306336\tWorker: alice\n",
            "Train Epoch: 5 [29440/60032 (49%)]\tLoss: 0.147726\tWorker: alice\n",
            "Train Epoch: 5 [30080/60032 (50%)]\tLoss: 0.111767\tWorker: alice\n",
            "Train Epoch: 5 [30720/60032 (51%)]\tLoss: 0.320997\tWorker: alice\n",
            "Train Epoch: 5 [31360/60032 (52%)]\tLoss: 0.121217\tWorker: alice\n",
            "Train Epoch: 5 [32000/60032 (53%)]\tLoss: 0.133893\tWorker: alice\n",
            "Train Epoch: 5 [32640/60032 (54%)]\tLoss: 0.249607\tWorker: alice\n",
            "Train Epoch: 5 [33280/60032 (55%)]\tLoss: 0.154268\tWorker: alice\n",
            "Train Epoch: 5 [33920/60032 (57%)]\tLoss: 0.431387\tWorker: alice\n",
            "Train Epoch: 5 [34560/60032 (58%)]\tLoss: 0.071634\tWorker: alice\n",
            "Train Epoch: 5 [35200/60032 (59%)]\tLoss: 0.339750\tWorker: alice\n",
            "Train Epoch: 5 [35840/60032 (60%)]\tLoss: 0.211292\tWorker: alice\n",
            "Train Epoch: 5 [36480/60032 (61%)]\tLoss: 0.179785\tWorker: alice\n",
            "Train Epoch: 5 [37120/60032 (62%)]\tLoss: 0.396902\tWorker: alice\n",
            "Train Epoch: 5 [37760/60032 (63%)]\tLoss: 0.076199\tWorker: alice\n",
            "Train Epoch: 5 [38400/60032 (64%)]\tLoss: 0.220606\tWorker: alice\n",
            "Train Epoch: 5 [39040/60032 (65%)]\tLoss: 0.162389\tWorker: alice\n",
            "Train Epoch: 5 [39680/60032 (66%)]\tLoss: 0.037448\tWorker: alice\n",
            "Train Epoch: 5 [40320/60032 (67%)]\tLoss: 0.913871\tWorker: untrustful\n",
            "Train Epoch: 5 [40960/60032 (68%)]\tLoss: 0.354632\tWorker: untrustful\n",
            "Train Epoch: 5 [41600/60032 (69%)]\tLoss: 0.771140\tWorker: untrustful\n",
            "Train Epoch: 5 [42240/60032 (70%)]\tLoss: 0.127488\tWorker: untrustful\n",
            "Train Epoch: 5 [42880/60032 (71%)]\tLoss: 0.547288\tWorker: untrustful\n",
            "Train Epoch: 5 [43520/60032 (72%)]\tLoss: 0.242493\tWorker: untrustful\n",
            "Train Epoch: 5 [44160/60032 (74%)]\tLoss: 0.272127\tWorker: untrustful\n",
            "Train Epoch: 5 [44800/60032 (75%)]\tLoss: 0.320453\tWorker: untrustful\n",
            "Train Epoch: 5 [45440/60032 (76%)]\tLoss: 0.361387\tWorker: untrustful\n",
            "Train Epoch: 5 [46080/60032 (77%)]\tLoss: 0.366257\tWorker: untrustful\n",
            "Train Epoch: 5 [46720/60032 (78%)]\tLoss: 0.165212\tWorker: untrustful\n",
            "Train Epoch: 5 [47360/60032 (79%)]\tLoss: 0.178689\tWorker: untrustful\n",
            "Train Epoch: 5 [48000/60032 (80%)]\tLoss: 0.434521\tWorker: untrustful\n",
            "Train Epoch: 5 [48640/60032 (81%)]\tLoss: 0.229519\tWorker: untrustful\n",
            "Train Epoch: 5 [49280/60032 (82%)]\tLoss: 0.393954\tWorker: untrustful\n",
            "Train Epoch: 5 [49920/60032 (83%)]\tLoss: 0.299339\tWorker: untrustful\n",
            "Train Epoch: 5 [50560/60032 (84%)]\tLoss: 0.304812\tWorker: untrustful\n",
            "Train Epoch: 5 [51200/60032 (85%)]\tLoss: 0.172824\tWorker: untrustful\n",
            "Train Epoch: 5 [51840/60032 (86%)]\tLoss: 0.312637\tWorker: untrustful\n",
            "Train Epoch: 5 [52480/60032 (87%)]\tLoss: 0.051136\tWorker: untrustful\n",
            "Train Epoch: 5 [53120/60032 (88%)]\tLoss: 0.161750\tWorker: untrustful\n",
            "Train Epoch: 5 [53760/60032 (90%)]\tLoss: 0.281431\tWorker: untrustful\n",
            "Train Epoch: 5 [54400/60032 (91%)]\tLoss: 0.220663\tWorker: untrustful\n",
            "Train Epoch: 5 [55040/60032 (92%)]\tLoss: 0.174963\tWorker: untrustful\n",
            "Train Epoch: 5 [55680/60032 (93%)]\tLoss: 0.190223\tWorker: untrustful\n",
            "Train Epoch: 5 [56320/60032 (94%)]\tLoss: 0.188351\tWorker: untrustful\n",
            "Train Epoch: 5 [56960/60032 (95%)]\tLoss: 0.108770\tWorker: untrustful\n",
            "Train Epoch: 5 [57600/60032 (96%)]\tLoss: 0.213330\tWorker: untrustful\n",
            "Train Epoch: 5 [58240/60032 (97%)]\tLoss: 0.352074\tWorker: untrustful\n",
            "Train Epoch: 5 [58880/60032 (98%)]\tLoss: 0.094853\tWorker: untrustful\n",
            "Train Epoch: 5 [59520/60032 (99%)]\tLoss: 0.065018\tWorker: untrustful\n",
            "\n",
            "Test set: Average loss: 0.3969, Accuracy: 9519/10000 (95%)\n",
            "\n",
            "Train Epoch: 6 [0/60032 (0%)]\tLoss: 0.352466\tWorker: bob\n",
            "Train Epoch: 6 [640/60032 (1%)]\tLoss: 0.873653\tWorker: bob\n",
            "Train Epoch: 6 [1280/60032 (2%)]\tLoss: 0.583523\tWorker: bob\n",
            "Train Epoch: 6 [1920/60032 (3%)]\tLoss: 0.735277\tWorker: bob\n",
            "Train Epoch: 6 [2560/60032 (4%)]\tLoss: 0.769795\tWorker: bob\n",
            "Train Epoch: 6 [3200/60032 (5%)]\tLoss: 0.610666\tWorker: bob\n",
            "Train Epoch: 6 [3840/60032 (6%)]\tLoss: 0.299889\tWorker: bob\n",
            "Train Epoch: 6 [4480/60032 (7%)]\tLoss: 0.265186\tWorker: bob\n",
            "Train Epoch: 6 [5120/60032 (9%)]\tLoss: 0.284538\tWorker: bob\n",
            "Train Epoch: 6 [5760/60032 (10%)]\tLoss: 0.375291\tWorker: bob\n",
            "Train Epoch: 6 [6400/60032 (11%)]\tLoss: 0.277186\tWorker: bob\n",
            "Train Epoch: 6 [7040/60032 (12%)]\tLoss: 0.142944\tWorker: bob\n",
            "Train Epoch: 6 [7680/60032 (13%)]\tLoss: 0.165373\tWorker: bob\n",
            "Train Epoch: 6 [8320/60032 (14%)]\tLoss: 0.191894\tWorker: bob\n",
            "Train Epoch: 6 [8960/60032 (15%)]\tLoss: 0.353899\tWorker: bob\n",
            "Train Epoch: 6 [9600/60032 (16%)]\tLoss: 0.524571\tWorker: bob\n",
            "Train Epoch: 6 [10240/60032 (17%)]\tLoss: 0.406544\tWorker: bob\n",
            "Train Epoch: 6 [10880/60032 (18%)]\tLoss: 0.278569\tWorker: bob\n",
            "Train Epoch: 6 [11520/60032 (19%)]\tLoss: 0.415374\tWorker: bob\n",
            "Train Epoch: 6 [12160/60032 (20%)]\tLoss: 0.591338\tWorker: bob\n",
            "Train Epoch: 6 [12800/60032 (21%)]\tLoss: 0.368702\tWorker: bob\n",
            "Train Epoch: 6 [13440/60032 (22%)]\tLoss: 0.429312\tWorker: bob\n",
            "Train Epoch: 6 [14080/60032 (23%)]\tLoss: 0.062174\tWorker: bob\n",
            "Train Epoch: 6 [14720/60032 (25%)]\tLoss: 0.349565\tWorker: bob\n",
            "Train Epoch: 6 [15360/60032 (26%)]\tLoss: 0.353187\tWorker: bob\n",
            "Train Epoch: 6 [16000/60032 (27%)]\tLoss: 0.157367\tWorker: bob\n",
            "Train Epoch: 6 [16640/60032 (28%)]\tLoss: 0.190311\tWorker: bob\n",
            "Train Epoch: 6 [17280/60032 (29%)]\tLoss: 0.490008\tWorker: bob\n",
            "Train Epoch: 6 [17920/60032 (30%)]\tLoss: 0.051997\tWorker: bob\n",
            "Train Epoch: 6 [18560/60032 (31%)]\tLoss: 0.065188\tWorker: bob\n",
            "Train Epoch: 6 [19200/60032 (32%)]\tLoss: 0.188192\tWorker: bob\n",
            "Train Epoch: 6 [19840/60032 (33%)]\tLoss: 0.131057\tWorker: bob\n",
            "Train Epoch: 6 [20480/60032 (34%)]\tLoss: 0.670109\tWorker: alice\n",
            "Train Epoch: 6 [21120/60032 (35%)]\tLoss: 0.881318\tWorker: alice\n",
            "Train Epoch: 6 [21760/60032 (36%)]\tLoss: 2.223302\tWorker: alice\n",
            "Train Epoch: 6 [22400/60032 (37%)]\tLoss: 0.480901\tWorker: alice\n",
            "Train Epoch: 6 [23040/60032 (38%)]\tLoss: 0.793934\tWorker: alice\n",
            "Train Epoch: 6 [23680/60032 (39%)]\tLoss: 0.299982\tWorker: alice\n",
            "Train Epoch: 6 [24320/60032 (41%)]\tLoss: 0.191215\tWorker: alice\n",
            "Train Epoch: 6 [24960/60032 (42%)]\tLoss: 0.250634\tWorker: alice\n",
            "Train Epoch: 6 [25600/60032 (43%)]\tLoss: 0.153909\tWorker: alice\n",
            "Train Epoch: 6 [26240/60032 (44%)]\tLoss: 0.361837\tWorker: alice\n",
            "Train Epoch: 6 [26880/60032 (45%)]\tLoss: 0.262322\tWorker: alice\n",
            "Train Epoch: 6 [27520/60032 (46%)]\tLoss: 0.420354\tWorker: alice\n",
            "Train Epoch: 6 [28160/60032 (47%)]\tLoss: 0.155155\tWorker: alice\n",
            "Train Epoch: 6 [28800/60032 (48%)]\tLoss: 0.672463\tWorker: alice\n",
            "Train Epoch: 6 [29440/60032 (49%)]\tLoss: 0.252170\tWorker: alice\n",
            "Train Epoch: 6 [30080/60032 (50%)]\tLoss: 0.362919\tWorker: alice\n",
            "Train Epoch: 6 [30720/60032 (51%)]\tLoss: 0.139446\tWorker: alice\n",
            "Train Epoch: 6 [31360/60032 (52%)]\tLoss: 0.427046\tWorker: alice\n",
            "Train Epoch: 6 [32000/60032 (53%)]\tLoss: 0.138871\tWorker: alice\n",
            "Train Epoch: 6 [32640/60032 (54%)]\tLoss: 0.175719\tWorker: alice\n",
            "Train Epoch: 6 [33280/60032 (55%)]\tLoss: 0.157766\tWorker: alice\n",
            "Train Epoch: 6 [33920/60032 (57%)]\tLoss: 0.139728\tWorker: alice\n",
            "Train Epoch: 6 [34560/60032 (58%)]\tLoss: 0.408067\tWorker: alice\n",
            "Train Epoch: 6 [35200/60032 (59%)]\tLoss: 0.530400\tWorker: alice\n",
            "Train Epoch: 6 [35840/60032 (60%)]\tLoss: 0.325847\tWorker: alice\n",
            "Train Epoch: 6 [36480/60032 (61%)]\tLoss: 0.161469\tWorker: alice\n",
            "Train Epoch: 6 [37120/60032 (62%)]\tLoss: 0.568985\tWorker: alice\n",
            "Train Epoch: 6 [37760/60032 (63%)]\tLoss: 0.161658\tWorker: alice\n",
            "Train Epoch: 6 [38400/60032 (64%)]\tLoss: 0.064580\tWorker: alice\n",
            "Train Epoch: 6 [39040/60032 (65%)]\tLoss: 0.018529\tWorker: alice\n",
            "Train Epoch: 6 [39680/60032 (66%)]\tLoss: 0.344556\tWorker: alice\n",
            "Train Epoch: 6 [40320/60032 (67%)]\tLoss: 0.788139\tWorker: untrustful\n",
            "Train Epoch: 6 [40960/60032 (68%)]\tLoss: 1.069047\tWorker: untrustful\n",
            "Train Epoch: 6 [41600/60032 (69%)]\tLoss: 0.500794\tWorker: untrustful\n",
            "Train Epoch: 6 [42240/60032 (70%)]\tLoss: 0.113755\tWorker: untrustful\n",
            "Train Epoch: 6 [42880/60032 (71%)]\tLoss: 0.491340\tWorker: untrustful\n",
            "Train Epoch: 6 [43520/60032 (72%)]\tLoss: 0.550312\tWorker: untrustful\n",
            "Train Epoch: 6 [44160/60032 (74%)]\tLoss: 0.455516\tWorker: untrustful\n",
            "Train Epoch: 6 [44800/60032 (75%)]\tLoss: 0.200377\tWorker: untrustful\n",
            "Train Epoch: 6 [45440/60032 (76%)]\tLoss: 0.499301\tWorker: untrustful\n",
            "Train Epoch: 6 [46080/60032 (77%)]\tLoss: 0.403219\tWorker: untrustful\n",
            "Train Epoch: 6 [46720/60032 (78%)]\tLoss: 0.461729\tWorker: untrustful\n",
            "Train Epoch: 6 [47360/60032 (79%)]\tLoss: 0.323244\tWorker: untrustful\n",
            "Train Epoch: 6 [48000/60032 (80%)]\tLoss: 0.204262\tWorker: untrustful\n",
            "Train Epoch: 6 [48640/60032 (81%)]\tLoss: 0.386737\tWorker: untrustful\n",
            "Train Epoch: 6 [49280/60032 (82%)]\tLoss: 0.241135\tWorker: untrustful\n",
            "Train Epoch: 6 [49920/60032 (83%)]\tLoss: 0.437064\tWorker: untrustful\n",
            "Train Epoch: 6 [50560/60032 (84%)]\tLoss: 0.058266\tWorker: untrustful\n",
            "Train Epoch: 6 [51200/60032 (85%)]\tLoss: 0.168016\tWorker: untrustful\n",
            "Train Epoch: 6 [51840/60032 (86%)]\tLoss: 0.107396\tWorker: untrustful\n",
            "Train Epoch: 6 [52480/60032 (87%)]\tLoss: 0.223397\tWorker: untrustful\n",
            "Train Epoch: 6 [53120/60032 (88%)]\tLoss: 0.355718\tWorker: untrustful\n",
            "Train Epoch: 6 [53760/60032 (90%)]\tLoss: 0.143192\tWorker: untrustful\n",
            "Train Epoch: 6 [54400/60032 (91%)]\tLoss: 0.173724\tWorker: untrustful\n",
            "Train Epoch: 6 [55040/60032 (92%)]\tLoss: 0.351205\tWorker: untrustful\n",
            "Train Epoch: 6 [55680/60032 (93%)]\tLoss: 0.242704\tWorker: untrustful\n",
            "Train Epoch: 6 [56320/60032 (94%)]\tLoss: 0.233940\tWorker: untrustful\n",
            "Train Epoch: 6 [56960/60032 (95%)]\tLoss: 0.128383\tWorker: untrustful\n",
            "Train Epoch: 6 [57600/60032 (96%)]\tLoss: 0.148785\tWorker: untrustful\n",
            "Train Epoch: 6 [58240/60032 (97%)]\tLoss: 0.497808\tWorker: untrustful\n",
            "Train Epoch: 6 [58880/60032 (98%)]\tLoss: 0.328720\tWorker: untrustful\n",
            "Train Epoch: 6 [59520/60032 (99%)]\tLoss: 0.027801\tWorker: untrustful\n",
            "\n",
            "Test set: Average loss: 0.5540, Accuracy: 9344/10000 (93%)\n",
            "\n",
            "Train Epoch: 7 [0/60032 (0%)]\tLoss: 0.272899\tWorker: bob\n",
            "Train Epoch: 7 [640/60032 (1%)]\tLoss: 0.512318\tWorker: bob\n",
            "Train Epoch: 7 [1280/60032 (2%)]\tLoss: 0.655498\tWorker: bob\n",
            "Train Epoch: 7 [1920/60032 (3%)]\tLoss: 1.079610\tWorker: bob\n",
            "Train Epoch: 7 [2560/60032 (4%)]\tLoss: 0.579197\tWorker: bob\n",
            "Train Epoch: 7 [3200/60032 (5%)]\tLoss: 0.765931\tWorker: bob\n",
            "Train Epoch: 7 [3840/60032 (6%)]\tLoss: 0.358244\tWorker: bob\n",
            "Train Epoch: 7 [4480/60032 (7%)]\tLoss: 0.625467\tWorker: bob\n",
            "Train Epoch: 7 [5120/60032 (9%)]\tLoss: 0.287907\tWorker: bob\n",
            "Train Epoch: 7 [5760/60032 (10%)]\tLoss: 0.110235\tWorker: bob\n",
            "Train Epoch: 7 [6400/60032 (11%)]\tLoss: 0.776869\tWorker: bob\n",
            "Train Epoch: 7 [7040/60032 (12%)]\tLoss: 0.549575\tWorker: bob\n",
            "Train Epoch: 7 [7680/60032 (13%)]\tLoss: 0.357258\tWorker: bob\n",
            "Train Epoch: 7 [8320/60032 (14%)]\tLoss: 0.672668\tWorker: bob\n",
            "Train Epoch: 7 [8960/60032 (15%)]\tLoss: 0.479046\tWorker: bob\n",
            "Train Epoch: 7 [9600/60032 (16%)]\tLoss: 0.490233\tWorker: bob\n",
            "Train Epoch: 7 [10240/60032 (17%)]\tLoss: 0.261238\tWorker: bob\n",
            "Train Epoch: 7 [10880/60032 (18%)]\tLoss: 0.578520\tWorker: bob\n",
            "Train Epoch: 7 [11520/60032 (19%)]\tLoss: 0.269169\tWorker: bob\n",
            "Train Epoch: 7 [12160/60032 (20%)]\tLoss: 0.533073\tWorker: bob\n",
            "Train Epoch: 7 [12800/60032 (21%)]\tLoss: 0.968362\tWorker: bob\n",
            "Train Epoch: 7 [13440/60032 (22%)]\tLoss: 0.186264\tWorker: bob\n",
            "Train Epoch: 7 [14080/60032 (23%)]\tLoss: 0.353413\tWorker: bob\n",
            "Train Epoch: 7 [14720/60032 (25%)]\tLoss: 0.606125\tWorker: bob\n",
            "Train Epoch: 7 [15360/60032 (26%)]\tLoss: 0.208654\tWorker: bob\n",
            "Train Epoch: 7 [16000/60032 (27%)]\tLoss: 0.922328\tWorker: bob\n",
            "Train Epoch: 7 [16640/60032 (28%)]\tLoss: 0.098739\tWorker: bob\n",
            "Train Epoch: 7 [17280/60032 (29%)]\tLoss: 0.394453\tWorker: bob\n",
            "Train Epoch: 7 [17920/60032 (30%)]\tLoss: 0.355592\tWorker: bob\n",
            "Train Epoch: 7 [18560/60032 (31%)]\tLoss: 0.194932\tWorker: bob\n",
            "Train Epoch: 7 [19200/60032 (32%)]\tLoss: 0.224786\tWorker: bob\n",
            "Train Epoch: 7 [19840/60032 (33%)]\tLoss: 0.197111\tWorker: bob\n",
            "Train Epoch: 7 [20480/60032 (34%)]\tLoss: 0.738910\tWorker: alice\n",
            "Train Epoch: 7 [21120/60032 (35%)]\tLoss: 6.207553\tWorker: alice\n",
            "Train Epoch: 7 [21760/60032 (36%)]\tLoss: 39.159260\tWorker: alice\n",
            "Train Epoch: 7 [22400/60032 (37%)]\tLoss: 0.485852\tWorker: alice\n",
            "Train Epoch: 7 [23040/60032 (38%)]\tLoss: 0.436391\tWorker: alice\n",
            "Train Epoch: 7 [23680/60032 (39%)]\tLoss: 0.526939\tWorker: alice\n",
            "Train Epoch: 7 [24320/60032 (41%)]\tLoss: 0.502523\tWorker: alice\n",
            "Train Epoch: 7 [24960/60032 (42%)]\tLoss: 1.100151\tWorker: alice\n",
            "Train Epoch: 7 [25600/60032 (43%)]\tLoss: 0.438007\tWorker: alice\n",
            "Train Epoch: 7 [26240/60032 (44%)]\tLoss: 0.542328\tWorker: alice\n",
            "Train Epoch: 7 [26880/60032 (45%)]\tLoss: 0.137408\tWorker: alice\n",
            "Train Epoch: 7 [27520/60032 (46%)]\tLoss: 0.210873\tWorker: alice\n",
            "Train Epoch: 7 [28160/60032 (47%)]\tLoss: 0.337813\tWorker: alice\n",
            "Train Epoch: 7 [28800/60032 (48%)]\tLoss: 0.216883\tWorker: alice\n",
            "Train Epoch: 7 [29440/60032 (49%)]\tLoss: 0.140610\tWorker: alice\n",
            "Train Epoch: 7 [30080/60032 (50%)]\tLoss: 0.651927\tWorker: alice\n",
            "Train Epoch: 7 [30720/60032 (51%)]\tLoss: 0.255301\tWorker: alice\n",
            "Train Epoch: 7 [31360/60032 (52%)]\tLoss: 0.081768\tWorker: alice\n",
            "Train Epoch: 7 [32000/60032 (53%)]\tLoss: 0.126216\tWorker: alice\n",
            "Train Epoch: 7 [32640/60032 (54%)]\tLoss: 0.077151\tWorker: alice\n",
            "Train Epoch: 7 [33280/60032 (55%)]\tLoss: 0.215211\tWorker: alice\n",
            "Train Epoch: 7 [33920/60032 (57%)]\tLoss: 0.072357\tWorker: alice\n",
            "Train Epoch: 7 [34560/60032 (58%)]\tLoss: 0.342570\tWorker: alice\n",
            "Train Epoch: 7 [35200/60032 (59%)]\tLoss: 0.063090\tWorker: alice\n",
            "Train Epoch: 7 [35840/60032 (60%)]\tLoss: 0.584261\tWorker: alice\n",
            "Train Epoch: 7 [36480/60032 (61%)]\tLoss: 0.232849\tWorker: alice\n",
            "Train Epoch: 7 [37120/60032 (62%)]\tLoss: 0.056850\tWorker: alice\n",
            "Train Epoch: 7 [37760/60032 (63%)]\tLoss: 0.065610\tWorker: alice\n",
            "Train Epoch: 7 [38400/60032 (64%)]\tLoss: 0.131219\tWorker: alice\n",
            "Train Epoch: 7 [39040/60032 (65%)]\tLoss: 0.260960\tWorker: alice\n",
            "Train Epoch: 7 [39680/60032 (66%)]\tLoss: 0.229998\tWorker: alice\n",
            "Train Epoch: 7 [40320/60032 (67%)]\tLoss: 1.285336\tWorker: untrustful\n",
            "Train Epoch: 7 [40960/60032 (68%)]\tLoss: 1.968081\tWorker: untrustful\n",
            "Train Epoch: 7 [41600/60032 (69%)]\tLoss: 0.551841\tWorker: untrustful\n",
            "Train Epoch: 7 [42240/60032 (70%)]\tLoss: 0.749368\tWorker: untrustful\n",
            "Train Epoch: 7 [42880/60032 (71%)]\tLoss: 0.832124\tWorker: untrustful\n",
            "Train Epoch: 7 [43520/60032 (72%)]\tLoss: 0.931858\tWorker: untrustful\n",
            "Train Epoch: 7 [44160/60032 (74%)]\tLoss: 0.868411\tWorker: untrustful\n",
            "Train Epoch: 7 [44800/60032 (75%)]\tLoss: 0.872290\tWorker: untrustful\n",
            "Train Epoch: 7 [45440/60032 (76%)]\tLoss: 0.314752\tWorker: untrustful\n",
            "Train Epoch: 7 [46080/60032 (77%)]\tLoss: 0.378764\tWorker: untrustful\n",
            "Train Epoch: 7 [46720/60032 (78%)]\tLoss: 0.232936\tWorker: untrustful\n",
            "Train Epoch: 7 [47360/60032 (79%)]\tLoss: 0.279843\tWorker: untrustful\n",
            "Train Epoch: 7 [48000/60032 (80%)]\tLoss: 0.315704\tWorker: untrustful\n",
            "Train Epoch: 7 [48640/60032 (81%)]\tLoss: 0.246313\tWorker: untrustful\n",
            "Train Epoch: 7 [49280/60032 (82%)]\tLoss: 0.438913\tWorker: untrustful\n",
            "Train Epoch: 7 [49920/60032 (83%)]\tLoss: 0.474732\tWorker: untrustful\n",
            "Train Epoch: 7 [50560/60032 (84%)]\tLoss: 0.202314\tWorker: untrustful\n",
            "Train Epoch: 7 [51200/60032 (85%)]\tLoss: 0.483209\tWorker: untrustful\n",
            "Train Epoch: 7 [51840/60032 (86%)]\tLoss: 0.456295\tWorker: untrustful\n",
            "Train Epoch: 7 [52480/60032 (87%)]\tLoss: 0.258220\tWorker: untrustful\n",
            "Train Epoch: 7 [53120/60032 (88%)]\tLoss: 0.205284\tWorker: untrustful\n",
            "Train Epoch: 7 [53760/60032 (90%)]\tLoss: 0.406504\tWorker: untrustful\n",
            "Train Epoch: 7 [54400/60032 (91%)]\tLoss: 0.221597\tWorker: untrustful\n",
            "Train Epoch: 7 [55040/60032 (92%)]\tLoss: 0.351098\tWorker: untrustful\n",
            "Train Epoch: 7 [55680/60032 (93%)]\tLoss: 1.213180\tWorker: untrustful\n",
            "Train Epoch: 7 [56320/60032 (94%)]\tLoss: 0.115248\tWorker: untrustful\n",
            "Train Epoch: 7 [56960/60032 (95%)]\tLoss: 0.129316\tWorker: untrustful\n",
            "Train Epoch: 7 [57600/60032 (96%)]\tLoss: 0.490884\tWorker: untrustful\n",
            "Train Epoch: 7 [58240/60032 (97%)]\tLoss: 0.314247\tWorker: untrustful\n",
            "Train Epoch: 7 [58880/60032 (98%)]\tLoss: 0.357017\tWorker: untrustful\n",
            "Train Epoch: 7 [59520/60032 (99%)]\tLoss: 0.886102\tWorker: untrustful\n",
            "\n",
            "Test set: Average loss: 0.7585, Accuracy: 8786/10000 (88%)\n",
            "\n"
          ]
        }
      ],
      "source": [
        "# central model\n",
        "central_model = Net().to(device)\n",
        "# optimizer for central model not needed if model is not to be trained\n",
        "#optimizer = optim.SGD(central_model.parameters(), lr=args['lr'])\n",
        "# clients models and optimizers\n",
        "models = {i:Net().to(device) for i in clients}\n",
        "optimizers = {i:optim.SGD(models[i].parameters(), lr=args['lr']) for i in clients}\n",
        "\n",
        "# initialization of dictionary for model aggregation\n",
        "weights = {'conv0_mean_weight' : torch.zeros(size=central_model.conv[0].weight.shape).to(device),\n",
        "           'conv0_mean_bias' : torch.zeros(size=central_model.conv[0].bias.shape).to(device),\n",
        "           'conv2_mean_weight' : torch.zeros(size=central_model.conv[2].weight.shape).to(device),\n",
        "           'conv2_mean_bias' : torch.zeros(size=central_model.conv[2].bias.shape).to(device),\n",
        "           'fc0_mean_weight' : torch.zeros(size=central_model.fc[0].weight.shape).to(device),\n",
        "           'fc0_mean_bias' : torch.zeros(size=central_model.fc[0].bias.shape).to(device),\n",
        "           'fc2_mean_weight' : torch.zeros(size=central_model.fc[2].weight.shape).to(device),\n",
        "           'fc2_mean_bias' : torch.zeros(size=central_model.fc[2].bias.shape).to(device)}\n",
        "\n",
        "logging.info(\"Starting training !!\")\n",
        "\n",
        "for epoch in range(1, args['epochs'] + 1):\n",
        "    train_locally(args, models, device, federated_train_loader, optimizers, epoch)\n",
        "    aggregate(central_model, models, weights)\n",
        "    test(central_model, device, test_loader)"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# now we repeat training, but with one client being untrustful\n",
        "\n",
        "# central model\n",
        "central_model = Net().to(device)\n",
        "# optimizer for central model not needed if model is not to be trained\n",
        "#optimizer = optim.SGD(central_model.parameters(), lr=args['lr'])\n",
        "# clients models and optimizers\n",
        "models = {i:Net().to(device) for i in clients}\n",
        "optimizers = {i:optim.SGD(models[i].parameters(), lr=args['lr']) for i in clients}\n",
        "\n",
        "# initialization of dictionary for model aggregation\n",
        "weights = {'conv0_mean_weight' : torch.zeros(size=central_model.conv[0].weight.shape).to(device),\n",
        "           'conv0_mean_bias' : torch.zeros(size=central_model.conv[0].bias.shape).to(device),\n",
        "           'conv2_mean_weight' : torch.zeros(size=central_model.conv[2].weight.shape).to(device),\n",
        "           'conv2_mean_bias' : torch.zeros(size=central_model.conv[2].bias.shape).to(device),\n",
        "           'fc0_mean_weight' : torch.zeros(size=central_model.fc[0].weight.shape).to(device),\n",
        "           'fc0_mean_bias' : torch.zeros(size=central_model.fc[0].bias.shape).to(device),\n",
        "           'fc2_mean_weight' : torch.zeros(size=central_model.fc[2].weight.shape).to(device),\n",
        "           'fc2_mean_bias' : torch.zeros(size=central_model.fc[2].bias.shape).to(device)}\n",
        "\n",
        "logging.info(\"Starting training !!\")\n",
        "\n",
        "for epoch in range(1, args['epochs'] + 1):\n",
        "    train_locally(args, models, device, federated_train_loader, optimizers, epoch)\n",
        "    # we intentionally change the weights of a client's model to make that client behave as untrustful\n",
        "    models[clients[2]].conv[0].weight.data = models[clients[2]].conv[0].weight.data*10\n",
        "    models[clients[2]].conv[2].weight.data = models[clients[2]].conv[2].weight.data*10\n",
        "    models[clients[2]].fc[0].weight.data = models[clients[2]].fc[0].weight.data*10\n",
        "    models[clients[2]].fc[2].weight.data = models[clients[2]].fc[2].weight.data*10\n",
        "    aggregate(central_model, models, weights)\n",
        "    test(central_model, device, test_loader)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "3Og9LwO8qTSw",
        "outputId": "603d01c9-d1d4-44ef-9ff6-961162730e57"
      },
      "execution_count": 11,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Train Epoch: 1 [0/60032 (0%)]\tLoss: 2.295660\tWorker: bob\n",
            "Train Epoch: 1 [640/60032 (1%)]\tLoss: 2.162782\tWorker: bob\n",
            "Train Epoch: 1 [1280/60032 (2%)]\tLoss: 1.972803\tWorker: bob\n",
            "Train Epoch: 1 [1920/60032 (3%)]\tLoss: 1.656957\tWorker: bob\n",
            "Train Epoch: 1 [2560/60032 (4%)]\tLoss: 1.294741\tWorker: bob\n",
            "Train Epoch: 1 [3200/60032 (5%)]\tLoss: 0.995847\tWorker: bob\n",
            "Train Epoch: 1 [3840/60032 (6%)]\tLoss: 0.958026\tWorker: bob\n",
            "Train Epoch: 1 [4480/60032 (7%)]\tLoss: 0.586278\tWorker: bob\n",
            "Train Epoch: 1 [5120/60032 (9%)]\tLoss: 0.562689\tWorker: bob\n",
            "Train Epoch: 1 [5760/60032 (10%)]\tLoss: 0.673915\tWorker: bob\n",
            "Train Epoch: 1 [6400/60032 (11%)]\tLoss: 0.283507\tWorker: bob\n",
            "Train Epoch: 1 [7040/60032 (12%)]\tLoss: 0.527617\tWorker: bob\n",
            "Train Epoch: 1 [7680/60032 (13%)]\tLoss: 0.477611\tWorker: bob\n",
            "Train Epoch: 1 [8320/60032 (14%)]\tLoss: 0.405786\tWorker: bob\n",
            "Train Epoch: 1 [8960/60032 (15%)]\tLoss: 0.481299\tWorker: bob\n",
            "Train Epoch: 1 [9600/60032 (16%)]\tLoss: 0.316219\tWorker: bob\n",
            "Train Epoch: 1 [10240/60032 (17%)]\tLoss: 0.405584\tWorker: bob\n",
            "Train Epoch: 1 [10880/60032 (18%)]\tLoss: 0.489916\tWorker: bob\n",
            "Train Epoch: 1 [11520/60032 (19%)]\tLoss: 0.366226\tWorker: bob\n",
            "Train Epoch: 1 [12160/60032 (20%)]\tLoss: 0.310034\tWorker: bob\n",
            "Train Epoch: 1 [12800/60032 (21%)]\tLoss: 0.252263\tWorker: bob\n",
            "Train Epoch: 1 [13440/60032 (22%)]\tLoss: 0.299094\tWorker: bob\n",
            "Train Epoch: 1 [14080/60032 (23%)]\tLoss: 0.509307\tWorker: bob\n",
            "Train Epoch: 1 [14720/60032 (25%)]\tLoss: 0.409421\tWorker: bob\n",
            "Train Epoch: 1 [15360/60032 (26%)]\tLoss: 0.419507\tWorker: bob\n",
            "Train Epoch: 1 [16000/60032 (27%)]\tLoss: 0.464015\tWorker: bob\n",
            "Train Epoch: 1 [16640/60032 (28%)]\tLoss: 0.424759\tWorker: bob\n",
            "Train Epoch: 1 [17280/60032 (29%)]\tLoss: 0.468185\tWorker: bob\n",
            "Train Epoch: 1 [17920/60032 (30%)]\tLoss: 0.364432\tWorker: bob\n",
            "Train Epoch: 1 [18560/60032 (31%)]\tLoss: 0.162191\tWorker: bob\n",
            "Train Epoch: 1 [19200/60032 (32%)]\tLoss: 0.188122\tWorker: bob\n",
            "Train Epoch: 1 [19840/60032 (33%)]\tLoss: 0.209775\tWorker: bob\n",
            "Train Epoch: 1 [20480/60032 (34%)]\tLoss: 2.261939\tWorker: alice\n",
            "Train Epoch: 1 [21120/60032 (35%)]\tLoss: 2.155214\tWorker: alice\n",
            "Train Epoch: 1 [21760/60032 (36%)]\tLoss: 2.008727\tWorker: alice\n",
            "Train Epoch: 1 [22400/60032 (37%)]\tLoss: 1.706295\tWorker: alice\n",
            "Train Epoch: 1 [23040/60032 (38%)]\tLoss: 1.434966\tWorker: alice\n",
            "Train Epoch: 1 [23680/60032 (39%)]\tLoss: 1.028990\tWorker: alice\n",
            "Train Epoch: 1 [24320/60032 (41%)]\tLoss: 0.963362\tWorker: alice\n",
            "Train Epoch: 1 [24960/60032 (42%)]\tLoss: 0.689664\tWorker: alice\n",
            "Train Epoch: 1 [25600/60032 (43%)]\tLoss: 0.779878\tWorker: alice\n",
            "Train Epoch: 1 [26240/60032 (44%)]\tLoss: 0.475270\tWorker: alice\n",
            "Train Epoch: 1 [26880/60032 (45%)]\tLoss: 0.514012\tWorker: alice\n",
            "Train Epoch: 1 [27520/60032 (46%)]\tLoss: 0.682315\tWorker: alice\n",
            "Train Epoch: 1 [28160/60032 (47%)]\tLoss: 0.446243\tWorker: alice\n",
            "Train Epoch: 1 [28800/60032 (48%)]\tLoss: 0.406664\tWorker: alice\n",
            "Train Epoch: 1 [29440/60032 (49%)]\tLoss: 0.424938\tWorker: alice\n",
            "Train Epoch: 1 [30080/60032 (50%)]\tLoss: 0.538980\tWorker: alice\n",
            "Train Epoch: 1 [30720/60032 (51%)]\tLoss: 0.492394\tWorker: alice\n",
            "Train Epoch: 1 [31360/60032 (52%)]\tLoss: 0.335839\tWorker: alice\n",
            "Train Epoch: 1 [32000/60032 (53%)]\tLoss: 0.401117\tWorker: alice\n",
            "Train Epoch: 1 [32640/60032 (54%)]\tLoss: 0.541948\tWorker: alice\n",
            "Train Epoch: 1 [33280/60032 (55%)]\tLoss: 0.411871\tWorker: alice\n",
            "Train Epoch: 1 [33920/60032 (57%)]\tLoss: 0.212100\tWorker: alice\n",
            "Train Epoch: 1 [34560/60032 (58%)]\tLoss: 0.359829\tWorker: alice\n",
            "Train Epoch: 1 [35200/60032 (59%)]\tLoss: 0.349057\tWorker: alice\n",
            "Train Epoch: 1 [35840/60032 (60%)]\tLoss: 0.334335\tWorker: alice\n",
            "Train Epoch: 1 [36480/60032 (61%)]\tLoss: 0.428619\tWorker: alice\n",
            "Train Epoch: 1 [37120/60032 (62%)]\tLoss: 0.350515\tWorker: alice\n",
            "Train Epoch: 1 [37760/60032 (63%)]\tLoss: 0.316354\tWorker: alice\n",
            "Train Epoch: 1 [38400/60032 (64%)]\tLoss: 0.472030\tWorker: alice\n",
            "Train Epoch: 1 [39040/60032 (65%)]\tLoss: 0.204700\tWorker: alice\n",
            "Train Epoch: 1 [39680/60032 (66%)]\tLoss: 0.307867\tWorker: alice\n",
            "Train Epoch: 1 [40320/60032 (67%)]\tLoss: 2.260046\tWorker: untrustful\n",
            "Train Epoch: 1 [40960/60032 (68%)]\tLoss: 2.095312\tWorker: untrustful\n",
            "Train Epoch: 1 [41600/60032 (69%)]\tLoss: 1.916938\tWorker: untrustful\n",
            "Train Epoch: 1 [42240/60032 (70%)]\tLoss: 1.591347\tWorker: untrustful\n",
            "Train Epoch: 1 [42880/60032 (71%)]\tLoss: 1.020145\tWorker: untrustful\n",
            "Train Epoch: 1 [43520/60032 (72%)]\tLoss: 0.942847\tWorker: untrustful\n",
            "Train Epoch: 1 [44160/60032 (74%)]\tLoss: 0.734344\tWorker: untrustful\n",
            "Train Epoch: 1 [44800/60032 (75%)]\tLoss: 0.641013\tWorker: untrustful\n",
            "Train Epoch: 1 [45440/60032 (76%)]\tLoss: 0.531318\tWorker: untrustful\n",
            "Train Epoch: 1 [46080/60032 (77%)]\tLoss: 0.593774\tWorker: untrustful\n",
            "Train Epoch: 1 [46720/60032 (78%)]\tLoss: 0.391500\tWorker: untrustful\n",
            "Train Epoch: 1 [47360/60032 (79%)]\tLoss: 0.499403\tWorker: untrustful\n",
            "Train Epoch: 1 [48000/60032 (80%)]\tLoss: 0.450984\tWorker: untrustful\n",
            "Train Epoch: 1 [48640/60032 (81%)]\tLoss: 0.447020\tWorker: untrustful\n",
            "Train Epoch: 1 [49280/60032 (82%)]\tLoss: 0.545290\tWorker: untrustful\n",
            "Train Epoch: 1 [49920/60032 (83%)]\tLoss: 0.523300\tWorker: untrustful\n",
            "Train Epoch: 1 [50560/60032 (84%)]\tLoss: 0.327020\tWorker: untrustful\n",
            "Train Epoch: 1 [51200/60032 (85%)]\tLoss: 0.327735\tWorker: untrustful\n",
            "Train Epoch: 1 [51840/60032 (86%)]\tLoss: 0.337730\tWorker: untrustful\n",
            "Train Epoch: 1 [52480/60032 (87%)]\tLoss: 0.303497\tWorker: untrustful\n",
            "Train Epoch: 1 [53120/60032 (88%)]\tLoss: 0.357569\tWorker: untrustful\n",
            "Train Epoch: 1 [53760/60032 (90%)]\tLoss: 0.312840\tWorker: untrustful\n",
            "Train Epoch: 1 [54400/60032 (91%)]\tLoss: 0.480008\tWorker: untrustful\n",
            "Train Epoch: 1 [55040/60032 (92%)]\tLoss: 0.310803\tWorker: untrustful\n",
            "Train Epoch: 1 [55680/60032 (93%)]\tLoss: 0.525278\tWorker: untrustful\n",
            "Train Epoch: 1 [56320/60032 (94%)]\tLoss: 0.298157\tWorker: untrustful\n",
            "Train Epoch: 1 [56960/60032 (95%)]\tLoss: 0.397013\tWorker: untrustful\n",
            "Train Epoch: 1 [57600/60032 (96%)]\tLoss: 0.317921\tWorker: untrustful\n",
            "Train Epoch: 1 [58240/60032 (97%)]\tLoss: 0.459494\tWorker: untrustful\n",
            "Train Epoch: 1 [58880/60032 (98%)]\tLoss: 0.356877\tWorker: untrustful\n",
            "Train Epoch: 1 [59520/60032 (99%)]\tLoss: 0.178727\tWorker: untrustful\n",
            "\n",
            "Test set: Average loss: 28.0543, Accuracy: 8744/10000 (87%)\n",
            "\n",
            "Train Epoch: 2 [0/60032 (0%)]\tLoss: 23.529799\tWorker: bob\n",
            "Train Epoch: 2 [640/60032 (1%)]\tLoss: 0.850408\tWorker: bob\n",
            "Train Epoch: 2 [1280/60032 (2%)]\tLoss: 0.544677\tWorker: bob\n",
            "Train Epoch: 2 [1920/60032 (3%)]\tLoss: 0.354892\tWorker: bob\n",
            "Train Epoch: 2 [2560/60032 (4%)]\tLoss: 0.220099\tWorker: bob\n",
            "Train Epoch: 2 [3200/60032 (5%)]\tLoss: 0.329027\tWorker: bob\n",
            "Train Epoch: 2 [3840/60032 (6%)]\tLoss: 0.323854\tWorker: bob\n",
            "Train Epoch: 2 [4480/60032 (7%)]\tLoss: 0.187598\tWorker: bob\n",
            "Train Epoch: 2 [5120/60032 (9%)]\tLoss: 0.155905\tWorker: bob\n",
            "Train Epoch: 2 [5760/60032 (10%)]\tLoss: 0.143245\tWorker: bob\n",
            "Train Epoch: 2 [6400/60032 (11%)]\tLoss: 0.171637\tWorker: bob\n",
            "Train Epoch: 2 [7040/60032 (12%)]\tLoss: 0.293721\tWorker: bob\n",
            "Train Epoch: 2 [7680/60032 (13%)]\tLoss: 0.247591\tWorker: bob\n",
            "Train Epoch: 2 [8320/60032 (14%)]\tLoss: 0.114854\tWorker: bob\n",
            "Train Epoch: 2 [8960/60032 (15%)]\tLoss: 0.211105\tWorker: bob\n",
            "Train Epoch: 2 [9600/60032 (16%)]\tLoss: 0.129968\tWorker: bob\n",
            "Train Epoch: 2 [10240/60032 (17%)]\tLoss: 0.189367\tWorker: bob\n",
            "Train Epoch: 2 [10880/60032 (18%)]\tLoss: 0.191633\tWorker: bob\n",
            "Train Epoch: 2 [11520/60032 (19%)]\tLoss: 0.416193\tWorker: bob\n",
            "Train Epoch: 2 [12160/60032 (20%)]\tLoss: 0.067460\tWorker: bob\n",
            "Train Epoch: 2 [12800/60032 (21%)]\tLoss: 0.129623\tWorker: bob\n",
            "Train Epoch: 2 [13440/60032 (22%)]\tLoss: 0.351709\tWorker: bob\n",
            "Train Epoch: 2 [14080/60032 (23%)]\tLoss: 0.173511\tWorker: bob\n",
            "Train Epoch: 2 [14720/60032 (25%)]\tLoss: 0.156659\tWorker: bob\n",
            "Train Epoch: 2 [15360/60032 (26%)]\tLoss: 0.292056\tWorker: bob\n",
            "Train Epoch: 2 [16000/60032 (27%)]\tLoss: 0.111717\tWorker: bob\n",
            "Train Epoch: 2 [16640/60032 (28%)]\tLoss: 0.219505\tWorker: bob\n",
            "Train Epoch: 2 [17280/60032 (29%)]\tLoss: 0.169942\tWorker: bob\n",
            "Train Epoch: 2 [17920/60032 (30%)]\tLoss: 0.231923\tWorker: bob\n",
            "Train Epoch: 2 [18560/60032 (31%)]\tLoss: 0.193164\tWorker: bob\n",
            "Train Epoch: 2 [19200/60032 (32%)]\tLoss: 0.085758\tWorker: bob\n",
            "Train Epoch: 2 [19840/60032 (33%)]\tLoss: 0.189562\tWorker: bob\n",
            "Train Epoch: 2 [20480/60032 (34%)]\tLoss: 1.270433\tWorker: alice\n",
            "Train Epoch: 2 [21120/60032 (35%)]\tLoss: 1.123885\tWorker: alice\n",
            "Train Epoch: 2 [21760/60032 (36%)]\tLoss: 0.776214\tWorker: alice\n",
            "Train Epoch: 2 [22400/60032 (37%)]\tLoss: 0.517997\tWorker: alice\n",
            "Train Epoch: 2 [23040/60032 (38%)]\tLoss: 0.597045\tWorker: alice\n",
            "Train Epoch: 2 [23680/60032 (39%)]\tLoss: 0.380489\tWorker: alice\n",
            "Train Epoch: 2 [24320/60032 (41%)]\tLoss: 0.228796\tWorker: alice\n",
            "Train Epoch: 2 [24960/60032 (42%)]\tLoss: 0.255199\tWorker: alice\n",
            "Train Epoch: 2 [25600/60032 (43%)]\tLoss: 0.395120\tWorker: alice\n",
            "Train Epoch: 2 [26240/60032 (44%)]\tLoss: 0.279282\tWorker: alice\n",
            "Train Epoch: 2 [26880/60032 (45%)]\tLoss: 0.677650\tWorker: alice\n",
            "Train Epoch: 2 [27520/60032 (46%)]\tLoss: 0.225777\tWorker: alice\n",
            "Train Epoch: 2 [28160/60032 (47%)]\tLoss: 0.376495\tWorker: alice\n",
            "Train Epoch: 2 [28800/60032 (48%)]\tLoss: 0.192953\tWorker: alice\n",
            "Train Epoch: 2 [29440/60032 (49%)]\tLoss: 0.188652\tWorker: alice\n",
            "Train Epoch: 2 [30080/60032 (50%)]\tLoss: 0.189997\tWorker: alice\n",
            "Train Epoch: 2 [30720/60032 (51%)]\tLoss: 0.319712\tWorker: alice\n",
            "Train Epoch: 2 [31360/60032 (52%)]\tLoss: 0.203579\tWorker: alice\n",
            "Train Epoch: 2 [32000/60032 (53%)]\tLoss: 0.244809\tWorker: alice\n",
            "Train Epoch: 2 [32640/60032 (54%)]\tLoss: 0.118385\tWorker: alice\n",
            "Train Epoch: 2 [33280/60032 (55%)]\tLoss: 0.143397\tWorker: alice\n",
            "Train Epoch: 2 [33920/60032 (57%)]\tLoss: 0.192547\tWorker: alice\n",
            "Train Epoch: 2 [34560/60032 (58%)]\tLoss: 0.176456\tWorker: alice\n",
            "Train Epoch: 2 [35200/60032 (59%)]\tLoss: 0.170406\tWorker: alice\n",
            "Train Epoch: 2 [35840/60032 (60%)]\tLoss: 0.241692\tWorker: alice\n",
            "Train Epoch: 2 [36480/60032 (61%)]\tLoss: 0.175579\tWorker: alice\n",
            "Train Epoch: 2 [37120/60032 (62%)]\tLoss: 0.198594\tWorker: alice\n",
            "Train Epoch: 2 [37760/60032 (63%)]\tLoss: 0.315294\tWorker: alice\n",
            "Train Epoch: 2 [38400/60032 (64%)]\tLoss: 0.197537\tWorker: alice\n",
            "Train Epoch: 2 [39040/60032 (65%)]\tLoss: 0.088456\tWorker: alice\n",
            "Train Epoch: 2 [39680/60032 (66%)]\tLoss: 0.293851\tWorker: alice\n",
            "Train Epoch: 2 [40320/60032 (67%)]\tLoss: 2.758545\tWorker: untrustful\n",
            "Train Epoch: 2 [40960/60032 (68%)]\tLoss: 1.194613\tWorker: untrustful\n",
            "Train Epoch: 2 [41600/60032 (69%)]\tLoss: 0.772179\tWorker: untrustful\n",
            "Train Epoch: 2 [42240/60032 (70%)]\tLoss: 0.656714\tWorker: untrustful\n",
            "Train Epoch: 2 [42880/60032 (71%)]\tLoss: 0.559185\tWorker: untrustful\n",
            "Train Epoch: 2 [43520/60032 (72%)]\tLoss: 0.431858\tWorker: untrustful\n",
            "Train Epoch: 2 [44160/60032 (74%)]\tLoss: 0.325922\tWorker: untrustful\n",
            "Train Epoch: 2 [44800/60032 (75%)]\tLoss: 0.488123\tWorker: untrustful\n",
            "Train Epoch: 2 [45440/60032 (76%)]\tLoss: 0.335997\tWorker: untrustful\n",
            "Train Epoch: 2 [46080/60032 (77%)]\tLoss: 0.145590\tWorker: untrustful\n",
            "Train Epoch: 2 [46720/60032 (78%)]\tLoss: 0.262386\tWorker: untrustful\n",
            "Train Epoch: 2 [47360/60032 (79%)]\tLoss: 0.118179\tWorker: untrustful\n",
            "Train Epoch: 2 [48000/60032 (80%)]\tLoss: 0.220833\tWorker: untrustful\n",
            "Train Epoch: 2 [48640/60032 (81%)]\tLoss: 0.177731\tWorker: untrustful\n",
            "Train Epoch: 2 [49280/60032 (82%)]\tLoss: 0.149719\tWorker: untrustful\n",
            "Train Epoch: 2 [49920/60032 (83%)]\tLoss: 0.144762\tWorker: untrustful\n",
            "Train Epoch: 2 [50560/60032 (84%)]\tLoss: 0.331802\tWorker: untrustful\n",
            "Train Epoch: 2 [51200/60032 (85%)]\tLoss: 0.296650\tWorker: untrustful\n",
            "Train Epoch: 2 [51840/60032 (86%)]\tLoss: 0.360793\tWorker: untrustful\n",
            "Train Epoch: 2 [52480/60032 (87%)]\tLoss: 0.160005\tWorker: untrustful\n",
            "Train Epoch: 2 [53120/60032 (88%)]\tLoss: 0.230736\tWorker: untrustful\n",
            "Train Epoch: 2 [53760/60032 (90%)]\tLoss: 0.209382\tWorker: untrustful\n",
            "Train Epoch: 2 [54400/60032 (91%)]\tLoss: 0.302163\tWorker: untrustful\n",
            "Train Epoch: 2 [55040/60032 (92%)]\tLoss: 0.255362\tWorker: untrustful\n",
            "Train Epoch: 2 [55680/60032 (93%)]\tLoss: 0.100165\tWorker: untrustful\n",
            "Train Epoch: 2 [56320/60032 (94%)]\tLoss: 0.120923\tWorker: untrustful\n",
            "Train Epoch: 2 [56960/60032 (95%)]\tLoss: 0.248292\tWorker: untrustful\n",
            "Train Epoch: 2 [57600/60032 (96%)]\tLoss: 0.133212\tWorker: untrustful\n",
            "Train Epoch: 2 [58240/60032 (97%)]\tLoss: 0.064939\tWorker: untrustful\n",
            "Train Epoch: 2 [58880/60032 (98%)]\tLoss: 0.091588\tWorker: untrustful\n",
            "Train Epoch: 2 [59520/60032 (99%)]\tLoss: 0.152052\tWorker: untrustful\n",
            "\n",
            "Test set: Average loss: 36.2192, Accuracy: 9336/10000 (93%)\n",
            "\n",
            "Train Epoch: 3 [0/60032 (0%)]\tLoss: 64.689369\tWorker: bob\n",
            "Train Epoch: 3 [640/60032 (1%)]\tLoss: 11577745625821300386449850368.000000\tWorker: bob\n",
            "Train Epoch: 3 [1280/60032 (2%)]\tLoss: nan\tWorker: bob\n",
            "Train Epoch: 3 [1920/60032 (3%)]\tLoss: nan\tWorker: bob\n",
            "Train Epoch: 3 [2560/60032 (4%)]\tLoss: nan\tWorker: bob\n",
            "Train Epoch: 3 [3200/60032 (5%)]\tLoss: nan\tWorker: bob\n",
            "Train Epoch: 3 [3840/60032 (6%)]\tLoss: nan\tWorker: bob\n",
            "Train Epoch: 3 [4480/60032 (7%)]\tLoss: nan\tWorker: bob\n",
            "Train Epoch: 3 [5120/60032 (9%)]\tLoss: nan\tWorker: bob\n",
            "Train Epoch: 3 [5760/60032 (10%)]\tLoss: nan\tWorker: bob\n",
            "Train Epoch: 3 [6400/60032 (11%)]\tLoss: nan\tWorker: bob\n",
            "Train Epoch: 3 [7040/60032 (12%)]\tLoss: nan\tWorker: bob\n",
            "Train Epoch: 3 [7680/60032 (13%)]\tLoss: nan\tWorker: bob\n",
            "Train Epoch: 3 [8320/60032 (14%)]\tLoss: nan\tWorker: bob\n",
            "Train Epoch: 3 [8960/60032 (15%)]\tLoss: nan\tWorker: bob\n",
            "Train Epoch: 3 [9600/60032 (16%)]\tLoss: nan\tWorker: bob\n",
            "Train Epoch: 3 [10240/60032 (17%)]\tLoss: nan\tWorker: bob\n",
            "Train Epoch: 3 [10880/60032 (18%)]\tLoss: nan\tWorker: bob\n",
            "Train Epoch: 3 [11520/60032 (19%)]\tLoss: nan\tWorker: bob\n",
            "Train Epoch: 3 [12160/60032 (20%)]\tLoss: nan\tWorker: bob\n",
            "Train Epoch: 3 [12800/60032 (21%)]\tLoss: nan\tWorker: bob\n",
            "Train Epoch: 3 [13440/60032 (22%)]\tLoss: nan\tWorker: bob\n",
            "Train Epoch: 3 [14080/60032 (23%)]\tLoss: nan\tWorker: bob\n",
            "Train Epoch: 3 [14720/60032 (25%)]\tLoss: nan\tWorker: bob\n",
            "Train Epoch: 3 [15360/60032 (26%)]\tLoss: nan\tWorker: bob\n",
            "Train Epoch: 3 [16000/60032 (27%)]\tLoss: nan\tWorker: bob\n",
            "Train Epoch: 3 [16640/60032 (28%)]\tLoss: nan\tWorker: bob\n",
            "Train Epoch: 3 [17280/60032 (29%)]\tLoss: nan\tWorker: bob\n",
            "Train Epoch: 3 [17920/60032 (30%)]\tLoss: nan\tWorker: bob\n",
            "Train Epoch: 3 [18560/60032 (31%)]\tLoss: nan\tWorker: bob\n",
            "Train Epoch: 3 [19200/60032 (32%)]\tLoss: nan\tWorker: bob\n",
            "Train Epoch: 3 [19840/60032 (33%)]\tLoss: nan\tWorker: bob\n",
            "Train Epoch: 3 [20480/60032 (34%)]\tLoss: 74.195885\tWorker: alice\n",
            "Train Epoch: 3 [21120/60032 (35%)]\tLoss: 10.611707\tWorker: alice\n",
            "Train Epoch: 3 [21760/60032 (36%)]\tLoss: 3.482440\tWorker: alice\n",
            "Train Epoch: 3 [22400/60032 (37%)]\tLoss: 2.949617\tWorker: alice\n",
            "Train Epoch: 3 [23040/60032 (38%)]\tLoss: 3.137356\tWorker: alice\n",
            "Train Epoch: 3 [23680/60032 (39%)]\tLoss: 2.263016\tWorker: alice\n",
            "Train Epoch: 3 [24320/60032 (41%)]\tLoss: 2.297549\tWorker: alice\n",
            "Train Epoch: 3 [24960/60032 (42%)]\tLoss: 2.296943\tWorker: alice\n",
            "Train Epoch: 3 [25600/60032 (43%)]\tLoss: 2.303672\tWorker: alice\n",
            "Train Epoch: 3 [26240/60032 (44%)]\tLoss: 2.297098\tWorker: alice\n",
            "Train Epoch: 3 [26880/60032 (45%)]\tLoss: 2.443782\tWorker: alice\n",
            "Train Epoch: 3 [27520/60032 (46%)]\tLoss: 2.291285\tWorker: alice\n",
            "Train Epoch: 3 [28160/60032 (47%)]\tLoss: 2.372921\tWorker: alice\n",
            "Train Epoch: 3 [28800/60032 (48%)]\tLoss: 2.303422\tWorker: alice\n",
            "Train Epoch: 3 [29440/60032 (49%)]\tLoss: 2.301221\tWorker: alice\n",
            "Train Epoch: 3 [30080/60032 (50%)]\tLoss: 2.291954\tWorker: alice\n",
            "Train Epoch: 3 [30720/60032 (51%)]\tLoss: 2.296422\tWorker: alice\n",
            "Train Epoch: 3 [31360/60032 (52%)]\tLoss: 2.304400\tWorker: alice\n",
            "Train Epoch: 3 [32000/60032 (53%)]\tLoss: 2.313011\tWorker: alice\n",
            "Train Epoch: 3 [32640/60032 (54%)]\tLoss: 2.302429\tWorker: alice\n",
            "Train Epoch: 3 [33280/60032 (55%)]\tLoss: 2.301926\tWorker: alice\n",
            "Train Epoch: 3 [33920/60032 (57%)]\tLoss: 2.291910\tWorker: alice\n",
            "Train Epoch: 3 [34560/60032 (58%)]\tLoss: 2.289623\tWorker: alice\n",
            "Train Epoch: 3 [35200/60032 (59%)]\tLoss: 2.307015\tWorker: alice\n",
            "Train Epoch: 3 [35840/60032 (60%)]\tLoss: 2.306667\tWorker: alice\n",
            "Train Epoch: 3 [36480/60032 (61%)]\tLoss: 2.311524\tWorker: alice\n",
            "Train Epoch: 3 [37120/60032 (62%)]\tLoss: 2.350466\tWorker: alice\n",
            "Train Epoch: 3 [37760/60032 (63%)]\tLoss: 2.301571\tWorker: alice\n",
            "Train Epoch: 3 [38400/60032 (64%)]\tLoss: 2.306172\tWorker: alice\n",
            "Train Epoch: 3 [39040/60032 (65%)]\tLoss: 2.312848\tWorker: alice\n",
            "Train Epoch: 3 [39680/60032 (66%)]\tLoss: 2.308476\tWorker: alice\n",
            "Train Epoch: 3 [40320/60032 (67%)]\tLoss: 238.858948\tWorker: untrustful\n",
            "Train Epoch: 3 [40960/60032 (68%)]\tLoss: 12.691738\tWorker: untrustful\n",
            "Train Epoch: 3 [41600/60032 (69%)]\tLoss: 3.492270\tWorker: untrustful\n",
            "Train Epoch: 3 [42240/60032 (70%)]\tLoss: 4.071511\tWorker: untrustful\n",
            "Train Epoch: 3 [42880/60032 (71%)]\tLoss: 4.515908\tWorker: untrustful\n",
            "Train Epoch: 3 [43520/60032 (72%)]\tLoss: 4.125465\tWorker: untrustful\n",
            "Train Epoch: 3 [44160/60032 (74%)]\tLoss: 5.149926\tWorker: untrustful\n",
            "Train Epoch: 3 [44800/60032 (75%)]\tLoss: 0.821396\tWorker: untrustful\n",
            "Train Epoch: 3 [45440/60032 (76%)]\tLoss: 2.133035\tWorker: untrustful\n",
            "Train Epoch: 3 [46080/60032 (77%)]\tLoss: 2.122526\tWorker: untrustful\n",
            "Train Epoch: 3 [46720/60032 (78%)]\tLoss: 2.421940\tWorker: untrustful\n",
            "Train Epoch: 3 [47360/60032 (79%)]\tLoss: 0.960737\tWorker: untrustful\n",
            "Train Epoch: 3 [48000/60032 (80%)]\tLoss: 1.614326\tWorker: untrustful\n",
            "Train Epoch: 3 [48640/60032 (81%)]\tLoss: 3.216908\tWorker: untrustful\n",
            "Train Epoch: 3 [49280/60032 (82%)]\tLoss: 1.365866\tWorker: untrustful\n",
            "Train Epoch: 3 [49920/60032 (83%)]\tLoss: 1.581887\tWorker: untrustful\n",
            "Train Epoch: 3 [50560/60032 (84%)]\tLoss: 1.724820\tWorker: untrustful\n",
            "Train Epoch: 3 [51200/60032 (85%)]\tLoss: 1.500056\tWorker: untrustful\n",
            "Train Epoch: 3 [51840/60032 (86%)]\tLoss: 1.134191\tWorker: untrustful\n",
            "Train Epoch: 3 [52480/60032 (87%)]\tLoss: 2.435029\tWorker: untrustful\n",
            "Train Epoch: 3 [53120/60032 (88%)]\tLoss: 1.899489\tWorker: untrustful\n",
            "Train Epoch: 3 [53760/60032 (90%)]\tLoss: 0.964316\tWorker: untrustful\n",
            "Train Epoch: 3 [54400/60032 (91%)]\tLoss: 1.827981\tWorker: untrustful\n",
            "Train Epoch: 3 [55040/60032 (92%)]\tLoss: 1.195647\tWorker: untrustful\n",
            "Train Epoch: 3 [55680/60032 (93%)]\tLoss: 2.105313\tWorker: untrustful\n",
            "Train Epoch: 3 [56320/60032 (94%)]\tLoss: 0.623500\tWorker: untrustful\n",
            "Train Epoch: 3 [56960/60032 (95%)]\tLoss: 1.144072\tWorker: untrustful\n",
            "Train Epoch: 3 [57600/60032 (96%)]\tLoss: 0.729050\tWorker: untrustful\n",
            "Train Epoch: 3 [58240/60032 (97%)]\tLoss: 1.022185\tWorker: untrustful\n",
            "Train Epoch: 3 [58880/60032 (98%)]\tLoss: 1.302842\tWorker: untrustful\n",
            "Train Epoch: 3 [59520/60032 (99%)]\tLoss: 0.852830\tWorker: untrustful\n",
            "\n",
            "Test set: Average loss: nan, Accuracy: 980/10000 (10%)\n",
            "\n",
            "Train Epoch: 4 [0/60032 (0%)]\tLoss: nan\tWorker: bob\n",
            "Train Epoch: 4 [640/60032 (1%)]\tLoss: nan\tWorker: bob\n",
            "Train Epoch: 4 [1280/60032 (2%)]\tLoss: nan\tWorker: bob\n",
            "Train Epoch: 4 [1920/60032 (3%)]\tLoss: nan\tWorker: bob\n",
            "Train Epoch: 4 [2560/60032 (4%)]\tLoss: nan\tWorker: bob\n",
            "Train Epoch: 4 [3200/60032 (5%)]\tLoss: nan\tWorker: bob\n",
            "Train Epoch: 4 [3840/60032 (6%)]\tLoss: nan\tWorker: bob\n",
            "Train Epoch: 4 [4480/60032 (7%)]\tLoss: nan\tWorker: bob\n",
            "Train Epoch: 4 [5120/60032 (9%)]\tLoss: nan\tWorker: bob\n",
            "Train Epoch: 4 [5760/60032 (10%)]\tLoss: nan\tWorker: bob\n",
            "Train Epoch: 4 [6400/60032 (11%)]\tLoss: nan\tWorker: bob\n",
            "Train Epoch: 4 [7040/60032 (12%)]\tLoss: nan\tWorker: bob\n",
            "Train Epoch: 4 [7680/60032 (13%)]\tLoss: nan\tWorker: bob\n",
            "Train Epoch: 4 [8320/60032 (14%)]\tLoss: nan\tWorker: bob\n",
            "Train Epoch: 4 [8960/60032 (15%)]\tLoss: nan\tWorker: bob\n",
            "Train Epoch: 4 [9600/60032 (16%)]\tLoss: nan\tWorker: bob\n",
            "Train Epoch: 4 [10240/60032 (17%)]\tLoss: nan\tWorker: bob\n",
            "Train Epoch: 4 [10880/60032 (18%)]\tLoss: nan\tWorker: bob\n",
            "Train Epoch: 4 [11520/60032 (19%)]\tLoss: nan\tWorker: bob\n",
            "Train Epoch: 4 [12160/60032 (20%)]\tLoss: nan\tWorker: bob\n",
            "Train Epoch: 4 [12800/60032 (21%)]\tLoss: nan\tWorker: bob\n",
            "Train Epoch: 4 [13440/60032 (22%)]\tLoss: nan\tWorker: bob\n",
            "Train Epoch: 4 [14080/60032 (23%)]\tLoss: nan\tWorker: bob\n",
            "Train Epoch: 4 [14720/60032 (25%)]\tLoss: nan\tWorker: bob\n",
            "Train Epoch: 4 [15360/60032 (26%)]\tLoss: nan\tWorker: bob\n",
            "Train Epoch: 4 [16000/60032 (27%)]\tLoss: nan\tWorker: bob\n",
            "Train Epoch: 4 [16640/60032 (28%)]\tLoss: nan\tWorker: bob\n",
            "Train Epoch: 4 [17280/60032 (29%)]\tLoss: nan\tWorker: bob\n",
            "Train Epoch: 4 [17920/60032 (30%)]\tLoss: nan\tWorker: bob\n",
            "Train Epoch: 4 [18560/60032 (31%)]\tLoss: nan\tWorker: bob\n",
            "Train Epoch: 4 [19200/60032 (32%)]\tLoss: nan\tWorker: bob\n",
            "Train Epoch: 4 [19840/60032 (33%)]\tLoss: nan\tWorker: bob\n",
            "Train Epoch: 4 [20480/60032 (34%)]\tLoss: nan\tWorker: alice\n",
            "Train Epoch: 4 [21120/60032 (35%)]\tLoss: nan\tWorker: alice\n",
            "Train Epoch: 4 [21760/60032 (36%)]\tLoss: nan\tWorker: alice\n",
            "Train Epoch: 4 [22400/60032 (37%)]\tLoss: nan\tWorker: alice\n",
            "Train Epoch: 4 [23040/60032 (38%)]\tLoss: nan\tWorker: alice\n",
            "Train Epoch: 4 [23680/60032 (39%)]\tLoss: nan\tWorker: alice\n",
            "Train Epoch: 4 [24320/60032 (41%)]\tLoss: nan\tWorker: alice\n",
            "Train Epoch: 4 [24960/60032 (42%)]\tLoss: nan\tWorker: alice\n",
            "Train Epoch: 4 [25600/60032 (43%)]\tLoss: nan\tWorker: alice\n",
            "Train Epoch: 4 [26240/60032 (44%)]\tLoss: nan\tWorker: alice\n",
            "Train Epoch: 4 [26880/60032 (45%)]\tLoss: nan\tWorker: alice\n",
            "Train Epoch: 4 [27520/60032 (46%)]\tLoss: nan\tWorker: alice\n",
            "Train Epoch: 4 [28160/60032 (47%)]\tLoss: nan\tWorker: alice\n",
            "Train Epoch: 4 [28800/60032 (48%)]\tLoss: nan\tWorker: alice\n",
            "Train Epoch: 4 [29440/60032 (49%)]\tLoss: nan\tWorker: alice\n",
            "Train Epoch: 4 [30080/60032 (50%)]\tLoss: nan\tWorker: alice\n",
            "Train Epoch: 4 [30720/60032 (51%)]\tLoss: nan\tWorker: alice\n",
            "Train Epoch: 4 [31360/60032 (52%)]\tLoss: nan\tWorker: alice\n",
            "Train Epoch: 4 [32000/60032 (53%)]\tLoss: nan\tWorker: alice\n",
            "Train Epoch: 4 [32640/60032 (54%)]\tLoss: nan\tWorker: alice\n",
            "Train Epoch: 4 [33280/60032 (55%)]\tLoss: nan\tWorker: alice\n",
            "Train Epoch: 4 [33920/60032 (57%)]\tLoss: nan\tWorker: alice\n",
            "Train Epoch: 4 [34560/60032 (58%)]\tLoss: nan\tWorker: alice\n",
            "Train Epoch: 4 [35200/60032 (59%)]\tLoss: nan\tWorker: alice\n",
            "Train Epoch: 4 [35840/60032 (60%)]\tLoss: nan\tWorker: alice\n",
            "Train Epoch: 4 [36480/60032 (61%)]\tLoss: nan\tWorker: alice\n",
            "Train Epoch: 4 [37120/60032 (62%)]\tLoss: nan\tWorker: alice\n",
            "Train Epoch: 4 [37760/60032 (63%)]\tLoss: nan\tWorker: alice\n",
            "Train Epoch: 4 [38400/60032 (64%)]\tLoss: nan\tWorker: alice\n",
            "Train Epoch: 4 [39040/60032 (65%)]\tLoss: nan\tWorker: alice\n",
            "Train Epoch: 4 [39680/60032 (66%)]\tLoss: nan\tWorker: alice\n",
            "Train Epoch: 4 [40320/60032 (67%)]\tLoss: nan\tWorker: untrustful\n",
            "Train Epoch: 4 [40960/60032 (68%)]\tLoss: nan\tWorker: untrustful\n",
            "Train Epoch: 4 [41600/60032 (69%)]\tLoss: nan\tWorker: untrustful\n",
            "Train Epoch: 4 [42240/60032 (70%)]\tLoss: nan\tWorker: untrustful\n",
            "Train Epoch: 4 [42880/60032 (71%)]\tLoss: nan\tWorker: untrustful\n",
            "Train Epoch: 4 [43520/60032 (72%)]\tLoss: nan\tWorker: untrustful\n",
            "Train Epoch: 4 [44160/60032 (74%)]\tLoss: nan\tWorker: untrustful\n",
            "Train Epoch: 4 [44800/60032 (75%)]\tLoss: nan\tWorker: untrustful\n",
            "Train Epoch: 4 [45440/60032 (76%)]\tLoss: nan\tWorker: untrustful\n",
            "Train Epoch: 4 [46080/60032 (77%)]\tLoss: nan\tWorker: untrustful\n",
            "Train Epoch: 4 [46720/60032 (78%)]\tLoss: nan\tWorker: untrustful\n",
            "Train Epoch: 4 [47360/60032 (79%)]\tLoss: nan\tWorker: untrustful\n",
            "Train Epoch: 4 [48000/60032 (80%)]\tLoss: nan\tWorker: untrustful\n",
            "Train Epoch: 4 [48640/60032 (81%)]\tLoss: nan\tWorker: untrustful\n",
            "Train Epoch: 4 [49280/60032 (82%)]\tLoss: nan\tWorker: untrustful\n",
            "Train Epoch: 4 [49920/60032 (83%)]\tLoss: nan\tWorker: untrustful\n",
            "Train Epoch: 4 [50560/60032 (84%)]\tLoss: nan\tWorker: untrustful\n",
            "Train Epoch: 4 [51200/60032 (85%)]\tLoss: nan\tWorker: untrustful\n",
            "Train Epoch: 4 [51840/60032 (86%)]\tLoss: nan\tWorker: untrustful\n",
            "Train Epoch: 4 [52480/60032 (87%)]\tLoss: nan\tWorker: untrustful\n",
            "Train Epoch: 4 [53120/60032 (88%)]\tLoss: nan\tWorker: untrustful\n",
            "Train Epoch: 4 [53760/60032 (90%)]\tLoss: nan\tWorker: untrustful\n",
            "Train Epoch: 4 [54400/60032 (91%)]\tLoss: nan\tWorker: untrustful\n",
            "Train Epoch: 4 [55040/60032 (92%)]\tLoss: nan\tWorker: untrustful\n",
            "Train Epoch: 4 [55680/60032 (93%)]\tLoss: nan\tWorker: untrustful\n",
            "Train Epoch: 4 [56320/60032 (94%)]\tLoss: nan\tWorker: untrustful\n",
            "Train Epoch: 4 [56960/60032 (95%)]\tLoss: nan\tWorker: untrustful\n",
            "Train Epoch: 4 [57600/60032 (96%)]\tLoss: nan\tWorker: untrustful\n",
            "Train Epoch: 4 [58240/60032 (97%)]\tLoss: nan\tWorker: untrustful\n",
            "Train Epoch: 4 [58880/60032 (98%)]\tLoss: nan\tWorker: untrustful\n",
            "Train Epoch: 4 [59520/60032 (99%)]\tLoss: nan\tWorker: untrustful\n",
            "\n",
            "Test set: Average loss: nan, Accuracy: 980/10000 (10%)\n",
            "\n",
            "Train Epoch: 5 [0/60032 (0%)]\tLoss: nan\tWorker: bob\n",
            "Train Epoch: 5 [640/60032 (1%)]\tLoss: nan\tWorker: bob\n",
            "Train Epoch: 5 [1280/60032 (2%)]\tLoss: nan\tWorker: bob\n",
            "Train Epoch: 5 [1920/60032 (3%)]\tLoss: nan\tWorker: bob\n",
            "Train Epoch: 5 [2560/60032 (4%)]\tLoss: nan\tWorker: bob\n",
            "Train Epoch: 5 [3200/60032 (5%)]\tLoss: nan\tWorker: bob\n",
            "Train Epoch: 5 [3840/60032 (6%)]\tLoss: nan\tWorker: bob\n",
            "Train Epoch: 5 [4480/60032 (7%)]\tLoss: nan\tWorker: bob\n",
            "Train Epoch: 5 [5120/60032 (9%)]\tLoss: nan\tWorker: bob\n",
            "Train Epoch: 5 [5760/60032 (10%)]\tLoss: nan\tWorker: bob\n",
            "Train Epoch: 5 [6400/60032 (11%)]\tLoss: nan\tWorker: bob\n",
            "Train Epoch: 5 [7040/60032 (12%)]\tLoss: nan\tWorker: bob\n",
            "Train Epoch: 5 [7680/60032 (13%)]\tLoss: nan\tWorker: bob\n",
            "Train Epoch: 5 [8320/60032 (14%)]\tLoss: nan\tWorker: bob\n",
            "Train Epoch: 5 [8960/60032 (15%)]\tLoss: nan\tWorker: bob\n",
            "Train Epoch: 5 [9600/60032 (16%)]\tLoss: nan\tWorker: bob\n",
            "Train Epoch: 5 [10240/60032 (17%)]\tLoss: nan\tWorker: bob\n",
            "Train Epoch: 5 [10880/60032 (18%)]\tLoss: nan\tWorker: bob\n",
            "Train Epoch: 5 [11520/60032 (19%)]\tLoss: nan\tWorker: bob\n",
            "Train Epoch: 5 [12160/60032 (20%)]\tLoss: nan\tWorker: bob\n",
            "Train Epoch: 5 [12800/60032 (21%)]\tLoss: nan\tWorker: bob\n",
            "Train Epoch: 5 [13440/60032 (22%)]\tLoss: nan\tWorker: bob\n",
            "Train Epoch: 5 [14080/60032 (23%)]\tLoss: nan\tWorker: bob\n",
            "Train Epoch: 5 [14720/60032 (25%)]\tLoss: nan\tWorker: bob\n",
            "Train Epoch: 5 [15360/60032 (26%)]\tLoss: nan\tWorker: bob\n",
            "Train Epoch: 5 [16000/60032 (27%)]\tLoss: nan\tWorker: bob\n",
            "Train Epoch: 5 [16640/60032 (28%)]\tLoss: nan\tWorker: bob\n",
            "Train Epoch: 5 [17280/60032 (29%)]\tLoss: nan\tWorker: bob\n",
            "Train Epoch: 5 [17920/60032 (30%)]\tLoss: nan\tWorker: bob\n",
            "Train Epoch: 5 [18560/60032 (31%)]\tLoss: nan\tWorker: bob\n",
            "Train Epoch: 5 [19200/60032 (32%)]\tLoss: nan\tWorker: bob\n",
            "Train Epoch: 5 [19840/60032 (33%)]\tLoss: nan\tWorker: bob\n",
            "Train Epoch: 5 [20480/60032 (34%)]\tLoss: nan\tWorker: alice\n",
            "Train Epoch: 5 [21120/60032 (35%)]\tLoss: nan\tWorker: alice\n",
            "Train Epoch: 5 [21760/60032 (36%)]\tLoss: nan\tWorker: alice\n",
            "Train Epoch: 5 [22400/60032 (37%)]\tLoss: nan\tWorker: alice\n",
            "Train Epoch: 5 [23040/60032 (38%)]\tLoss: nan\tWorker: alice\n",
            "Train Epoch: 5 [23680/60032 (39%)]\tLoss: nan\tWorker: alice\n",
            "Train Epoch: 5 [24320/60032 (41%)]\tLoss: nan\tWorker: alice\n",
            "Train Epoch: 5 [24960/60032 (42%)]\tLoss: nan\tWorker: alice\n",
            "Train Epoch: 5 [25600/60032 (43%)]\tLoss: nan\tWorker: alice\n",
            "Train Epoch: 5 [26240/60032 (44%)]\tLoss: nan\tWorker: alice\n",
            "Train Epoch: 5 [26880/60032 (45%)]\tLoss: nan\tWorker: alice\n",
            "Train Epoch: 5 [27520/60032 (46%)]\tLoss: nan\tWorker: alice\n",
            "Train Epoch: 5 [28160/60032 (47%)]\tLoss: nan\tWorker: alice\n",
            "Train Epoch: 5 [28800/60032 (48%)]\tLoss: nan\tWorker: alice\n",
            "Train Epoch: 5 [29440/60032 (49%)]\tLoss: nan\tWorker: alice\n",
            "Train Epoch: 5 [30080/60032 (50%)]\tLoss: nan\tWorker: alice\n",
            "Train Epoch: 5 [30720/60032 (51%)]\tLoss: nan\tWorker: alice\n",
            "Train Epoch: 5 [31360/60032 (52%)]\tLoss: nan\tWorker: alice\n",
            "Train Epoch: 5 [32000/60032 (53%)]\tLoss: nan\tWorker: alice\n",
            "Train Epoch: 5 [32640/60032 (54%)]\tLoss: nan\tWorker: alice\n",
            "Train Epoch: 5 [33280/60032 (55%)]\tLoss: nan\tWorker: alice\n",
            "Train Epoch: 5 [33920/60032 (57%)]\tLoss: nan\tWorker: alice\n",
            "Train Epoch: 5 [34560/60032 (58%)]\tLoss: nan\tWorker: alice\n",
            "Train Epoch: 5 [35200/60032 (59%)]\tLoss: nan\tWorker: alice\n",
            "Train Epoch: 5 [35840/60032 (60%)]\tLoss: nan\tWorker: alice\n",
            "Train Epoch: 5 [36480/60032 (61%)]\tLoss: nan\tWorker: alice\n",
            "Train Epoch: 5 [37120/60032 (62%)]\tLoss: nan\tWorker: alice\n",
            "Train Epoch: 5 [37760/60032 (63%)]\tLoss: nan\tWorker: alice\n",
            "Train Epoch: 5 [38400/60032 (64%)]\tLoss: nan\tWorker: alice\n",
            "Train Epoch: 5 [39040/60032 (65%)]\tLoss: nan\tWorker: alice\n",
            "Train Epoch: 5 [39680/60032 (66%)]\tLoss: nan\tWorker: alice\n",
            "Train Epoch: 5 [40320/60032 (67%)]\tLoss: nan\tWorker: untrustful\n",
            "Train Epoch: 5 [40960/60032 (68%)]\tLoss: nan\tWorker: untrustful\n",
            "Train Epoch: 5 [41600/60032 (69%)]\tLoss: nan\tWorker: untrustful\n",
            "Train Epoch: 5 [42240/60032 (70%)]\tLoss: nan\tWorker: untrustful\n",
            "Train Epoch: 5 [42880/60032 (71%)]\tLoss: nan\tWorker: untrustful\n",
            "Train Epoch: 5 [43520/60032 (72%)]\tLoss: nan\tWorker: untrustful\n",
            "Train Epoch: 5 [44160/60032 (74%)]\tLoss: nan\tWorker: untrustful\n",
            "Train Epoch: 5 [44800/60032 (75%)]\tLoss: nan\tWorker: untrustful\n",
            "Train Epoch: 5 [45440/60032 (76%)]\tLoss: nan\tWorker: untrustful\n",
            "Train Epoch: 5 [46080/60032 (77%)]\tLoss: nan\tWorker: untrustful\n",
            "Train Epoch: 5 [46720/60032 (78%)]\tLoss: nan\tWorker: untrustful\n",
            "Train Epoch: 5 [47360/60032 (79%)]\tLoss: nan\tWorker: untrustful\n",
            "Train Epoch: 5 [48000/60032 (80%)]\tLoss: nan\tWorker: untrustful\n",
            "Train Epoch: 5 [48640/60032 (81%)]\tLoss: nan\tWorker: untrustful\n",
            "Train Epoch: 5 [49280/60032 (82%)]\tLoss: nan\tWorker: untrustful\n",
            "Train Epoch: 5 [49920/60032 (83%)]\tLoss: nan\tWorker: untrustful\n",
            "Train Epoch: 5 [50560/60032 (84%)]\tLoss: nan\tWorker: untrustful\n",
            "Train Epoch: 5 [51200/60032 (85%)]\tLoss: nan\tWorker: untrustful\n",
            "Train Epoch: 5 [51840/60032 (86%)]\tLoss: nan\tWorker: untrustful\n",
            "Train Epoch: 5 [52480/60032 (87%)]\tLoss: nan\tWorker: untrustful\n",
            "Train Epoch: 5 [53120/60032 (88%)]\tLoss: nan\tWorker: untrustful\n",
            "Train Epoch: 5 [53760/60032 (90%)]\tLoss: nan\tWorker: untrustful\n",
            "Train Epoch: 5 [54400/60032 (91%)]\tLoss: nan\tWorker: untrustful\n",
            "Train Epoch: 5 [55040/60032 (92%)]\tLoss: nan\tWorker: untrustful\n",
            "Train Epoch: 5 [55680/60032 (93%)]\tLoss: nan\tWorker: untrustful\n",
            "Train Epoch: 5 [56320/60032 (94%)]\tLoss: nan\tWorker: untrustful\n",
            "Train Epoch: 5 [56960/60032 (95%)]\tLoss: nan\tWorker: untrustful\n",
            "Train Epoch: 5 [57600/60032 (96%)]\tLoss: nan\tWorker: untrustful\n",
            "Train Epoch: 5 [58240/60032 (97%)]\tLoss: nan\tWorker: untrustful\n",
            "Train Epoch: 5 [58880/60032 (98%)]\tLoss: nan\tWorker: untrustful\n",
            "Train Epoch: 5 [59520/60032 (99%)]\tLoss: nan\tWorker: untrustful\n",
            "\n",
            "Test set: Average loss: nan, Accuracy: 980/10000 (10%)\n",
            "\n",
            "Train Epoch: 6 [0/60032 (0%)]\tLoss: nan\tWorker: bob\n",
            "Train Epoch: 6 [640/60032 (1%)]\tLoss: nan\tWorker: bob\n",
            "Train Epoch: 6 [1280/60032 (2%)]\tLoss: nan\tWorker: bob\n",
            "Train Epoch: 6 [1920/60032 (3%)]\tLoss: nan\tWorker: bob\n",
            "Train Epoch: 6 [2560/60032 (4%)]\tLoss: nan\tWorker: bob\n",
            "Train Epoch: 6 [3200/60032 (5%)]\tLoss: nan\tWorker: bob\n",
            "Train Epoch: 6 [3840/60032 (6%)]\tLoss: nan\tWorker: bob\n",
            "Train Epoch: 6 [4480/60032 (7%)]\tLoss: nan\tWorker: bob\n",
            "Train Epoch: 6 [5120/60032 (9%)]\tLoss: nan\tWorker: bob\n",
            "Train Epoch: 6 [5760/60032 (10%)]\tLoss: nan\tWorker: bob\n",
            "Train Epoch: 6 [6400/60032 (11%)]\tLoss: nan\tWorker: bob\n",
            "Train Epoch: 6 [7040/60032 (12%)]\tLoss: nan\tWorker: bob\n",
            "Train Epoch: 6 [7680/60032 (13%)]\tLoss: nan\tWorker: bob\n",
            "Train Epoch: 6 [8320/60032 (14%)]\tLoss: nan\tWorker: bob\n",
            "Train Epoch: 6 [8960/60032 (15%)]\tLoss: nan\tWorker: bob\n",
            "Train Epoch: 6 [9600/60032 (16%)]\tLoss: nan\tWorker: bob\n",
            "Train Epoch: 6 [10240/60032 (17%)]\tLoss: nan\tWorker: bob\n",
            "Train Epoch: 6 [10880/60032 (18%)]\tLoss: nan\tWorker: bob\n",
            "Train Epoch: 6 [11520/60032 (19%)]\tLoss: nan\tWorker: bob\n",
            "Train Epoch: 6 [12160/60032 (20%)]\tLoss: nan\tWorker: bob\n",
            "Train Epoch: 6 [12800/60032 (21%)]\tLoss: nan\tWorker: bob\n",
            "Train Epoch: 6 [13440/60032 (22%)]\tLoss: nan\tWorker: bob\n",
            "Train Epoch: 6 [14080/60032 (23%)]\tLoss: nan\tWorker: bob\n",
            "Train Epoch: 6 [14720/60032 (25%)]\tLoss: nan\tWorker: bob\n",
            "Train Epoch: 6 [15360/60032 (26%)]\tLoss: nan\tWorker: bob\n",
            "Train Epoch: 6 [16000/60032 (27%)]\tLoss: nan\tWorker: bob\n",
            "Train Epoch: 6 [16640/60032 (28%)]\tLoss: nan\tWorker: bob\n",
            "Train Epoch: 6 [17280/60032 (29%)]\tLoss: nan\tWorker: bob\n",
            "Train Epoch: 6 [17920/60032 (30%)]\tLoss: nan\tWorker: bob\n",
            "Train Epoch: 6 [18560/60032 (31%)]\tLoss: nan\tWorker: bob\n",
            "Train Epoch: 6 [19200/60032 (32%)]\tLoss: nan\tWorker: bob\n",
            "Train Epoch: 6 [19840/60032 (33%)]\tLoss: nan\tWorker: bob\n",
            "Train Epoch: 6 [20480/60032 (34%)]\tLoss: nan\tWorker: alice\n",
            "Train Epoch: 6 [21120/60032 (35%)]\tLoss: nan\tWorker: alice\n",
            "Train Epoch: 6 [21760/60032 (36%)]\tLoss: nan\tWorker: alice\n",
            "Train Epoch: 6 [22400/60032 (37%)]\tLoss: nan\tWorker: alice\n",
            "Train Epoch: 6 [23040/60032 (38%)]\tLoss: nan\tWorker: alice\n",
            "Train Epoch: 6 [23680/60032 (39%)]\tLoss: nan\tWorker: alice\n",
            "Train Epoch: 6 [24320/60032 (41%)]\tLoss: nan\tWorker: alice\n",
            "Train Epoch: 6 [24960/60032 (42%)]\tLoss: nan\tWorker: alice\n",
            "Train Epoch: 6 [25600/60032 (43%)]\tLoss: nan\tWorker: alice\n",
            "Train Epoch: 6 [26240/60032 (44%)]\tLoss: nan\tWorker: alice\n",
            "Train Epoch: 6 [26880/60032 (45%)]\tLoss: nan\tWorker: alice\n",
            "Train Epoch: 6 [27520/60032 (46%)]\tLoss: nan\tWorker: alice\n",
            "Train Epoch: 6 [28160/60032 (47%)]\tLoss: nan\tWorker: alice\n",
            "Train Epoch: 6 [28800/60032 (48%)]\tLoss: nan\tWorker: alice\n",
            "Train Epoch: 6 [29440/60032 (49%)]\tLoss: nan\tWorker: alice\n",
            "Train Epoch: 6 [30080/60032 (50%)]\tLoss: nan\tWorker: alice\n",
            "Train Epoch: 6 [30720/60032 (51%)]\tLoss: nan\tWorker: alice\n",
            "Train Epoch: 6 [31360/60032 (52%)]\tLoss: nan\tWorker: alice\n",
            "Train Epoch: 6 [32000/60032 (53%)]\tLoss: nan\tWorker: alice\n",
            "Train Epoch: 6 [32640/60032 (54%)]\tLoss: nan\tWorker: alice\n",
            "Train Epoch: 6 [33280/60032 (55%)]\tLoss: nan\tWorker: alice\n",
            "Train Epoch: 6 [33920/60032 (57%)]\tLoss: nan\tWorker: alice\n",
            "Train Epoch: 6 [34560/60032 (58%)]\tLoss: nan\tWorker: alice\n",
            "Train Epoch: 6 [35200/60032 (59%)]\tLoss: nan\tWorker: alice\n",
            "Train Epoch: 6 [35840/60032 (60%)]\tLoss: nan\tWorker: alice\n",
            "Train Epoch: 6 [36480/60032 (61%)]\tLoss: nan\tWorker: alice\n",
            "Train Epoch: 6 [37120/60032 (62%)]\tLoss: nan\tWorker: alice\n",
            "Train Epoch: 6 [37760/60032 (63%)]\tLoss: nan\tWorker: alice\n",
            "Train Epoch: 6 [38400/60032 (64%)]\tLoss: nan\tWorker: alice\n",
            "Train Epoch: 6 [39040/60032 (65%)]\tLoss: nan\tWorker: alice\n",
            "Train Epoch: 6 [39680/60032 (66%)]\tLoss: nan\tWorker: alice\n",
            "Train Epoch: 6 [40320/60032 (67%)]\tLoss: nan\tWorker: untrustful\n",
            "Train Epoch: 6 [40960/60032 (68%)]\tLoss: nan\tWorker: untrustful\n",
            "Train Epoch: 6 [41600/60032 (69%)]\tLoss: nan\tWorker: untrustful\n",
            "Train Epoch: 6 [42240/60032 (70%)]\tLoss: nan\tWorker: untrustful\n",
            "Train Epoch: 6 [42880/60032 (71%)]\tLoss: nan\tWorker: untrustful\n",
            "Train Epoch: 6 [43520/60032 (72%)]\tLoss: nan\tWorker: untrustful\n",
            "Train Epoch: 6 [44160/60032 (74%)]\tLoss: nan\tWorker: untrustful\n",
            "Train Epoch: 6 [44800/60032 (75%)]\tLoss: nan\tWorker: untrustful\n",
            "Train Epoch: 6 [45440/60032 (76%)]\tLoss: nan\tWorker: untrustful\n",
            "Train Epoch: 6 [46080/60032 (77%)]\tLoss: nan\tWorker: untrustful\n",
            "Train Epoch: 6 [46720/60032 (78%)]\tLoss: nan\tWorker: untrustful\n",
            "Train Epoch: 6 [47360/60032 (79%)]\tLoss: nan\tWorker: untrustful\n",
            "Train Epoch: 6 [48000/60032 (80%)]\tLoss: nan\tWorker: untrustful\n",
            "Train Epoch: 6 [48640/60032 (81%)]\tLoss: nan\tWorker: untrustful\n",
            "Train Epoch: 6 [49280/60032 (82%)]\tLoss: nan\tWorker: untrustful\n",
            "Train Epoch: 6 [49920/60032 (83%)]\tLoss: nan\tWorker: untrustful\n",
            "Train Epoch: 6 [50560/60032 (84%)]\tLoss: nan\tWorker: untrustful\n",
            "Train Epoch: 6 [51200/60032 (85%)]\tLoss: nan\tWorker: untrustful\n",
            "Train Epoch: 6 [51840/60032 (86%)]\tLoss: nan\tWorker: untrustful\n",
            "Train Epoch: 6 [52480/60032 (87%)]\tLoss: nan\tWorker: untrustful\n",
            "Train Epoch: 6 [53120/60032 (88%)]\tLoss: nan\tWorker: untrustful\n",
            "Train Epoch: 6 [53760/60032 (90%)]\tLoss: nan\tWorker: untrustful\n",
            "Train Epoch: 6 [54400/60032 (91%)]\tLoss: nan\tWorker: untrustful\n",
            "Train Epoch: 6 [55040/60032 (92%)]\tLoss: nan\tWorker: untrustful\n",
            "Train Epoch: 6 [55680/60032 (93%)]\tLoss: nan\tWorker: untrustful\n",
            "Train Epoch: 6 [56320/60032 (94%)]\tLoss: nan\tWorker: untrustful\n",
            "Train Epoch: 6 [56960/60032 (95%)]\tLoss: nan\tWorker: untrustful\n",
            "Train Epoch: 6 [57600/60032 (96%)]\tLoss: nan\tWorker: untrustful\n",
            "Train Epoch: 6 [58240/60032 (97%)]\tLoss: nan\tWorker: untrustful\n",
            "Train Epoch: 6 [58880/60032 (98%)]\tLoss: nan\tWorker: untrustful\n",
            "Train Epoch: 6 [59520/60032 (99%)]\tLoss: nan\tWorker: untrustful\n",
            "\n",
            "Test set: Average loss: nan, Accuracy: 980/10000 (10%)\n",
            "\n",
            "Train Epoch: 7 [0/60032 (0%)]\tLoss: nan\tWorker: bob\n",
            "Train Epoch: 7 [640/60032 (1%)]\tLoss: nan\tWorker: bob\n",
            "Train Epoch: 7 [1280/60032 (2%)]\tLoss: nan\tWorker: bob\n",
            "Train Epoch: 7 [1920/60032 (3%)]\tLoss: nan\tWorker: bob\n",
            "Train Epoch: 7 [2560/60032 (4%)]\tLoss: nan\tWorker: bob\n",
            "Train Epoch: 7 [3200/60032 (5%)]\tLoss: nan\tWorker: bob\n",
            "Train Epoch: 7 [3840/60032 (6%)]\tLoss: nan\tWorker: bob\n",
            "Train Epoch: 7 [4480/60032 (7%)]\tLoss: nan\tWorker: bob\n",
            "Train Epoch: 7 [5120/60032 (9%)]\tLoss: nan\tWorker: bob\n",
            "Train Epoch: 7 [5760/60032 (10%)]\tLoss: nan\tWorker: bob\n",
            "Train Epoch: 7 [6400/60032 (11%)]\tLoss: nan\tWorker: bob\n",
            "Train Epoch: 7 [7040/60032 (12%)]\tLoss: nan\tWorker: bob\n",
            "Train Epoch: 7 [7680/60032 (13%)]\tLoss: nan\tWorker: bob\n",
            "Train Epoch: 7 [8320/60032 (14%)]\tLoss: nan\tWorker: bob\n",
            "Train Epoch: 7 [8960/60032 (15%)]\tLoss: nan\tWorker: bob\n",
            "Train Epoch: 7 [9600/60032 (16%)]\tLoss: nan\tWorker: bob\n",
            "Train Epoch: 7 [10240/60032 (17%)]\tLoss: nan\tWorker: bob\n",
            "Train Epoch: 7 [10880/60032 (18%)]\tLoss: nan\tWorker: bob\n",
            "Train Epoch: 7 [11520/60032 (19%)]\tLoss: nan\tWorker: bob\n",
            "Train Epoch: 7 [12160/60032 (20%)]\tLoss: nan\tWorker: bob\n",
            "Train Epoch: 7 [12800/60032 (21%)]\tLoss: nan\tWorker: bob\n",
            "Train Epoch: 7 [13440/60032 (22%)]\tLoss: nan\tWorker: bob\n",
            "Train Epoch: 7 [14080/60032 (23%)]\tLoss: nan\tWorker: bob\n",
            "Train Epoch: 7 [14720/60032 (25%)]\tLoss: nan\tWorker: bob\n",
            "Train Epoch: 7 [15360/60032 (26%)]\tLoss: nan\tWorker: bob\n",
            "Train Epoch: 7 [16000/60032 (27%)]\tLoss: nan\tWorker: bob\n",
            "Train Epoch: 7 [16640/60032 (28%)]\tLoss: nan\tWorker: bob\n",
            "Train Epoch: 7 [17280/60032 (29%)]\tLoss: nan\tWorker: bob\n",
            "Train Epoch: 7 [17920/60032 (30%)]\tLoss: nan\tWorker: bob\n",
            "Train Epoch: 7 [18560/60032 (31%)]\tLoss: nan\tWorker: bob\n",
            "Train Epoch: 7 [19200/60032 (32%)]\tLoss: nan\tWorker: bob\n",
            "Train Epoch: 7 [19840/60032 (33%)]\tLoss: nan\tWorker: bob\n",
            "Train Epoch: 7 [20480/60032 (34%)]\tLoss: nan\tWorker: alice\n",
            "Train Epoch: 7 [21120/60032 (35%)]\tLoss: nan\tWorker: alice\n",
            "Train Epoch: 7 [21760/60032 (36%)]\tLoss: nan\tWorker: alice\n",
            "Train Epoch: 7 [22400/60032 (37%)]\tLoss: nan\tWorker: alice\n",
            "Train Epoch: 7 [23040/60032 (38%)]\tLoss: nan\tWorker: alice\n",
            "Train Epoch: 7 [23680/60032 (39%)]\tLoss: nan\tWorker: alice\n",
            "Train Epoch: 7 [24320/60032 (41%)]\tLoss: nan\tWorker: alice\n",
            "Train Epoch: 7 [24960/60032 (42%)]\tLoss: nan\tWorker: alice\n",
            "Train Epoch: 7 [25600/60032 (43%)]\tLoss: nan\tWorker: alice\n",
            "Train Epoch: 7 [26240/60032 (44%)]\tLoss: nan\tWorker: alice\n",
            "Train Epoch: 7 [26880/60032 (45%)]\tLoss: nan\tWorker: alice\n",
            "Train Epoch: 7 [27520/60032 (46%)]\tLoss: nan\tWorker: alice\n",
            "Train Epoch: 7 [28160/60032 (47%)]\tLoss: nan\tWorker: alice\n",
            "Train Epoch: 7 [28800/60032 (48%)]\tLoss: nan\tWorker: alice\n",
            "Train Epoch: 7 [29440/60032 (49%)]\tLoss: nan\tWorker: alice\n",
            "Train Epoch: 7 [30080/60032 (50%)]\tLoss: nan\tWorker: alice\n",
            "Train Epoch: 7 [30720/60032 (51%)]\tLoss: nan\tWorker: alice\n",
            "Train Epoch: 7 [31360/60032 (52%)]\tLoss: nan\tWorker: alice\n",
            "Train Epoch: 7 [32000/60032 (53%)]\tLoss: nan\tWorker: alice\n",
            "Train Epoch: 7 [32640/60032 (54%)]\tLoss: nan\tWorker: alice\n",
            "Train Epoch: 7 [33280/60032 (55%)]\tLoss: nan\tWorker: alice\n",
            "Train Epoch: 7 [33920/60032 (57%)]\tLoss: nan\tWorker: alice\n",
            "Train Epoch: 7 [34560/60032 (58%)]\tLoss: nan\tWorker: alice\n",
            "Train Epoch: 7 [35200/60032 (59%)]\tLoss: nan\tWorker: alice\n",
            "Train Epoch: 7 [35840/60032 (60%)]\tLoss: nan\tWorker: alice\n",
            "Train Epoch: 7 [36480/60032 (61%)]\tLoss: nan\tWorker: alice\n",
            "Train Epoch: 7 [37120/60032 (62%)]\tLoss: nan\tWorker: alice\n",
            "Train Epoch: 7 [37760/60032 (63%)]\tLoss: nan\tWorker: alice\n",
            "Train Epoch: 7 [38400/60032 (64%)]\tLoss: nan\tWorker: alice\n",
            "Train Epoch: 7 [39040/60032 (65%)]\tLoss: nan\tWorker: alice\n",
            "Train Epoch: 7 [39680/60032 (66%)]\tLoss: nan\tWorker: alice\n",
            "Train Epoch: 7 [40320/60032 (67%)]\tLoss: nan\tWorker: untrustful\n",
            "Train Epoch: 7 [40960/60032 (68%)]\tLoss: nan\tWorker: untrustful\n",
            "Train Epoch: 7 [41600/60032 (69%)]\tLoss: nan\tWorker: untrustful\n",
            "Train Epoch: 7 [42240/60032 (70%)]\tLoss: nan\tWorker: untrustful\n",
            "Train Epoch: 7 [42880/60032 (71%)]\tLoss: nan\tWorker: untrustful\n",
            "Train Epoch: 7 [43520/60032 (72%)]\tLoss: nan\tWorker: untrustful\n",
            "Train Epoch: 7 [44160/60032 (74%)]\tLoss: nan\tWorker: untrustful\n",
            "Train Epoch: 7 [44800/60032 (75%)]\tLoss: nan\tWorker: untrustful\n",
            "Train Epoch: 7 [45440/60032 (76%)]\tLoss: nan\tWorker: untrustful\n",
            "Train Epoch: 7 [46080/60032 (77%)]\tLoss: nan\tWorker: untrustful\n",
            "Train Epoch: 7 [46720/60032 (78%)]\tLoss: nan\tWorker: untrustful\n",
            "Train Epoch: 7 [47360/60032 (79%)]\tLoss: nan\tWorker: untrustful\n",
            "Train Epoch: 7 [48000/60032 (80%)]\tLoss: nan\tWorker: untrustful\n",
            "Train Epoch: 7 [48640/60032 (81%)]\tLoss: nan\tWorker: untrustful\n",
            "Train Epoch: 7 [49280/60032 (82%)]\tLoss: nan\tWorker: untrustful\n",
            "Train Epoch: 7 [49920/60032 (83%)]\tLoss: nan\tWorker: untrustful\n",
            "Train Epoch: 7 [50560/60032 (84%)]\tLoss: nan\tWorker: untrustful\n",
            "Train Epoch: 7 [51200/60032 (85%)]\tLoss: nan\tWorker: untrustful\n",
            "Train Epoch: 7 [51840/60032 (86%)]\tLoss: nan\tWorker: untrustful\n",
            "Train Epoch: 7 [52480/60032 (87%)]\tLoss: nan\tWorker: untrustful\n",
            "Train Epoch: 7 [53120/60032 (88%)]\tLoss: nan\tWorker: untrustful\n",
            "Train Epoch: 7 [53760/60032 (90%)]\tLoss: nan\tWorker: untrustful\n",
            "Train Epoch: 7 [54400/60032 (91%)]\tLoss: nan\tWorker: untrustful\n",
            "Train Epoch: 7 [55040/60032 (92%)]\tLoss: nan\tWorker: untrustful\n",
            "Train Epoch: 7 [55680/60032 (93%)]\tLoss: nan\tWorker: untrustful\n",
            "Train Epoch: 7 [56320/60032 (94%)]\tLoss: nan\tWorker: untrustful\n",
            "Train Epoch: 7 [56960/60032 (95%)]\tLoss: nan\tWorker: untrustful\n",
            "Train Epoch: 7 [57600/60032 (96%)]\tLoss: nan\tWorker: untrustful\n",
            "Train Epoch: 7 [58240/60032 (97%)]\tLoss: nan\tWorker: untrustful\n",
            "Train Epoch: 7 [58880/60032 (98%)]\tLoss: nan\tWorker: untrustful\n",
            "Train Epoch: 7 [59520/60032 (99%)]\tLoss: nan\tWorker: untrustful\n",
            "\n",
            "Test set: Average loss: nan, Accuracy: 980/10000 (10%)\n",
            "\n"
          ]
        }
      ]
    }
  ],
  "metadata": {
    "colab": {
      "collapsed_sections": [
        "x2WDKGKIYdUp"
      ],
      "name": "FL_FedAvg_tutorial.ipynb",
      "toc_visible": true,
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}